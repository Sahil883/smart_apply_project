title,company,location,posted_date,job_link,source,skills,experience,description,salary
Data Engineer/Senior Data Engineer/Lead Data Engineer,ClearNote Health,"San Diego, CA",2025-06-25,https://www.linkedin.com/jobs/view/data-engineer-senior-data-engineer-lead-data-engineer-at-clearnote-health-4257708833?position=1&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=8Q%2FGdIAEQqijkBYOj7Hr%2Bw%3D%3D,LinkedIn,['Sql'],3 years,"Job Title: Data Engineer/Senior Data Engineer/Lead Data EngineerLocation: San Diego, San Mateo, RemoteFLSA Status: ExemptClearNote Health is a precision oncology company developing non-invasive diagnostic tests that detect cancer early—when it’s most treatable. We are seeking a Lead Data Engineer to play a pivotal role in shaping and operating the data infrastructure that powers both our scientific discovery and commercial execution.In this high-impact role, you’ll be responsible for architecting and maintaining the data systems that unify our business, clinical, and scientific data into a trusted and accessible platform. You’ll collaborate cross-functionally with R&D, Lab Operations, G&A, Regulatory, and Software Engineering teams to ensure timely, accurate, and secure access to critical data.This is an outstanding opportunity for an experienced data engineer who thrives in a fast-paced environment, enjoys working across diverse domains, and is passionate about using data to transform patient outcomes.Key ResponsibilitiesPartner with stakeholders across the company to understand their data needs and the systems generating that dataDesign and implement a scalable, sustainable and reliable data operations strategyOwn the architecture and ongoing evolution of our enterprise data warehouseDevelop and maintain data contracts between systems of record and the warehouse to ensure data quality and transparencyContinuously improve the architecture and pipelines for ingesting, transforming, and delivering dataCreate and maintain high-quality documentation for systems, pipelines, and data modelsCollaborate with QA and Regulatory teams to ensure data systems support clinical and regulatory requirementsWork closely with the R&D team to capture, organize and analyze NGS-based laboratory data from R&D and production environmentsCollaborate with BI team to deliver data from scientific analysis to our business intelligence platformQualificationsBachelor's degree in Computer Science, Information Systems, Laboratory Science, or a related field; Master’s degree preferred3+ years of experience in data engineering or data operations rolesStrong proficiency in SQL and PythonDemonstrated experience with data warehousing and transforming raw data into useful models for business and scientific useExcellent written and verbal communication skills, with the ability to collaborate effectively across technical and non-technical teamsExperience working with scientific data, particularly from next-generation sequencing (NGS), is strongly preferredFamiliarity with common bioinformatics data formats is a plusBonus: Familiarity with laboratory workflows and relevant regulatory standards (e.g., FDA, CLIA, ISO)What We ValueAt ClearNote Health, we are driven by our mission and guided by our values:Put Patients FirstRedefining the PossibleTogether We WinCompensationTitle and compensation will be commensurate with skills, experience, and qualifications. The estimated base salary range for this position is $140,000 to $250,000, with potential variation based on depth of experience and location. This role includes stock options and generous benefits.Come join us in addressing large healthcare needs through precision epigenomic medicine!ClearNote Health is an exciting life science company that is reinventing non-invasive molecular diagnostic testing using next generation epigenomic technologies. We are passionate and dedicated to discovering and developing medicines that will make a significant difference in cancer and other epigenomic-driven diseases. Our technologies provide novel insight and quantitation of human health and disease, with our focus on precision medicine applications improving both clinical and health system outcomes. Our company was founded based on pioneering work in the Stanford laboratory of Stephen Quake, with advisors from Stanford and UCSF.We look for extraordinary lifelong learners with a passion and growth mindset for these areas, and for combining biological ingenuity with AI and data analysis. Led by a team with decades of experience bringing products from concept to market, we are an equal opportunity employer and value diversity at our company.We provide generous benefits to all employees including stock options. We are building a world-class company, based in San Diego and San Mateo.Our commitment to Diversity, Equity, Inclusion, and Belonging:We celebrate diversity in perspectives and backgrounds, and this is reflected in our innovation, our mission, and values. Our differences make us unique, help us innovate, and allow us to persevere. We strive to achieve representation and inclusion and redefine the possible in patients living longer lives.Powered by JazzHRFN9USGaHv0Show moreShow less",
Data Engineer III,Fanatics,United States,2025-06-24,https://www.linkedin.com/jobs/view/data-engineer-iii-at-fanatics-4239626016?position=2&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=lp7POUkWj9YPQB4%2FXDsH1Q%3D%3D,LinkedIn,"['Mongodb', 'Machine Learning', 'Python', 'Agile', 'Postgresql', 'Sql', 'Mysql']",Seniority levelMid-Senior level,"Job DescriptionAbout this roleAs a Data Engineer III at Fanatics Betting & Gaming (FBG), you will play a crucial role in the development, deployment, and management of our data architecture. In this role you will partner with ML Engineers, Data Scientists, Data Analysts to build and scale data solutions that address business problems for end users and internal stakeholders. Be responsible for building systems and user experiences for our internal customers that are able to aggregate customer data across many sources, integrate with various 3rd party systems and build personalized experiences that allow us to deliver a great and rewarding experience to our customers. This position is perfect for individuals who are passionate about data, have a knack for problem-solving, and possess a strong foundation in data manipulation and statistical analysis. We are one team; we employ the principles of data-driven development, operational excellence, process calibration, and efficiency to deliver a stable, extensible platform at scale.ResponsibilitiesAssist in the design and construction of scalable data management systems, ensuring they meet business requirements and industry practicesDevelop and implement scalable and efficient data pipeline features /scoring algorithms that work with large-scale data to support analytics, business intelligence, and machine learning models in productionCollaborate with cross functional engineering teams to identify data sources and build services to serve the data into the product.Develop and optimize data pipelines, ensuring efficient and error-free data flow across the organizationMonitor the performance of data processing and storage systems, resolving any failures and ensuring data accuracy and securityDevelop a fundamental understanding of our industry, product, feature set and roadmapProvide technical leadership and mentorship to junior data engineers, promoting a culture of excellence and continuous learningHelp to onboard new team membersStay updated with emerging trends and technologies in data engineering, applying new techniques and tools to improve our data ecosystemMust be open to occasional travel to Fanatics Betting & Gaming offices and other locations for events, meetings, and team-building activitiesRequired Qualifications5+ years of data engineering experience with a proven track record of helping to design and implement large-scale data solutionsPossess a strong foundation in data structures, algorithms, and software engineering principlesDemonstrated skills writing SQL queriesFamiliar with other data languages including R or Python, and tools like Tableau, Microstrategy, Redshift, Databricks, or SnowflakeKnowledgeable about data modeling, ETL development, and data warehousing principlesFamiliar with SQL and NoSQL databases, including MySQL, PostgreSQL, MongoDB, or CassandraSolid analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracyExcellent communication and collaboration skillsIntellectually curious and interested in solving difficult problemsAble to effectively communicate complex technical subjects to a variety of audiences (engineers, technical leaders, marketing and operations teams, non-technical functional leaders)You have an agile mindset; able to embrace new initiatives in a fast-paced environmentThrives in an entrepreneurial, hyper-growth environment where priorities evolve regularly and decisions are made quicklyTeam-first mentality, with a willingness to do what it takes to get the job done and a desire to strive for betterPreferred QualificationsExperience working in a start-up environmentAbout UsFanatics is building a leading global digital sports platform. We ignite the passions of global sports fans and maximize the presence and reach for our hundreds of sports partners globally by offering products and services across Fanatics Commerce, Fanatics Collectibles, and Fanatics Betting & Gaming, allowing sports fans to Buy, Collect, and Bet. Through the Fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its Sportsbook and iGaming platform. Fanatics has an established database of over 100 million global sports fans; a global partner network with approximately 900 sports properties, including major national and international professional sports leagues, players associations, teams, colleges, college conferences and retail partners, 2,500 athletes and celebrities, and 200 exclusive athletes; and over 2,000 retail locations, including its Lids retail stores. Our more than 22,000 employees are committed to relentlessly enhancing the fan experience and delighting sports fans globally.About The TeamLaunched in 2021, Fanatics Betting and Gaming is the online and retail sports betting subsidiary of Fanatics, a global digital sports platform. The Fanatics Sportsbook is available to 95% of the addressable online sports bettor market in the U.S. Fanatics Casino is currently available online in Michigan, New Jersey, Pennsylvania and West Virginia. Fanatics Betting and Gaming operates twenty-two retail sports betting locations, including the only sportsbook inside an NFL stadium at Northwest Stadium. Fanatics Betting and Gaming is headquartered in New York with offices in Denver, Leeds and Dublin.Show moreShow less",
Senior Data Engineer,INSPYR Solutions,"Glendale, CA",2025-07-11,https://www.linkedin.com/jobs/view/senior-data-engineer-at-inspyr-solutions-4265277562?position=3&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=3%2BFlMj6mfXjH7QMs%2BcgtPg%3D%3D,LinkedIn,"['Data Science', 'Python', 'Kubernetes', 'Java', 'Aws', 'Scrum', 'Spark', 'Scala', 'Sql', 'Agile']",Seniority levelMid-Senior level,"Title:Senior Data EngineerIndustry:EntertainmentLocation:Glendale, CADuration:12+ monthsRate Range:$80-$91.25/hrWork Requirements:US Citizen, GC Holders or Authorized to Work in the U.S.Description:As a Senior Data Engineer, you will play a pivotal role in the transformation of data into actionable insights.Collaborate with our dynamic team of technologists to develop cutting-edge data solutions that drive innovation and fuel business growth. Your responsibilities will include managing complex data structures and delivering scalable and efficient data solutions.Your expertise in data engineering will be crucial in optimizing our data-driven decision-making processes.Key Responsibilities:Contribute to maintaining, updating, and expanding existing Core Data platform data pipelinesBuild tools and services to support data discovery, lineage, governance, and privacyCollaborate with other software/data engineers and cross-functional teamsTech stack includes Airflow, Spark, Databricks, Delta Lake, Snowflake, Kubernetes and AWSCollaborate with product managers, architects, and other engineers to drive the success of the Core Data platformContribute to developing and documenting both internal and external standards and best practices for pipeline configurations, naming conventions, and moreEnsure high operational efficiency and quality of the Core Data platform datasets to ensure our solutions meet SLAs and project reliability and accuracy to all our stakeholders (Engineering, Data Science, Operations, and Analytics teams)Be an active participant and advocate of agile/scrum ceremonies to collaborate and improve processes for our teamEngage with and understand our customers, forming relationships that allow us to understand and prioritize both innovative new offerings and incremental platform improvementsMaintain detailed documentation of your work and changes to support data quality and data governance requirementsBasic Qualifications:5+ years of data engineering experience developing large data pipelines ● Proficiency in at least one major programming language (e.g. Python,Java, Scala)Strong SQL skills and ability to create queries to analyze complex datasetsHands-on production environment experience with distributed processing systems such as SparkHands-on production experience with data pipeline orchestration systems such as Airflow for creating and maintaining data pipelinesExperience with at least one major Massively Parallel Processing (MPP) or cloud database technology (Snowflake, Databricks, Big Query).Experience in developing APIs with GraphQLDeep Understanding of AWS or other cloud providers as well as infrastructure as codeFamiliarity with Data Modeling techniques and Data Warehousing standard methodologies and practicesStrong algorithmic problem-solving expertiseAdvance understanding of OLTP vs OLAP environmentsStrong background in at least one of the following: distributed data processing or software engineering of data services, or data modelingFamiliar with Scrum and Agile methodologiesRequired Education:BA/BS Degree Comp Sci/IS or related fieldOur benefits package includes:Comprehensive medical benefitsCompetitive pay401(k) retirement plan...and much more!About INSPYR SolutionsTechnology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients' business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilitiesShow moreShow less",
Data Engineer,Arbital Health,San Francisco Bay Area,2025-06-29,https://www.linkedin.com/jobs/view/data-engineer-at-arbital-health-4259235115?position=4&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=jz3hhZQW00YJNdA%2Bh%2BlbhA%3D%3D,LinkedIn,"['Typescript', 'Machine Learning', 'Python', 'React', 'Aws', 'Tailwind', 'Gcp', 'Sql']",2-4 years,"At Arbital Health, we are transforming the US healthcare system by accelerating the shift from fee-for-service agreements to value-based care agreements. We do this by building a neutral, third-party platform that allows our customers to design, measure, and adjudicate value-based care agreements that maximize resources and help patients get the care they need to thrive. We are looking for highly talented and highly motivated individuals. You will be a roll-up-your-sleeves builder eager to innovate and collaborate to change healthcare.Across all roles, we look for people who embody our values of impact, innovation, collaboration, velocity, and transparency.We’ve built the initial production data pipelines that ingest, enrich, aggregate, and summarize healthcare financial data so it can be easily visualized in our web app by our customers. In addition to sophisticated financial calculations, this code matches patients against a benchmark population so that our clients can understand how their patients are doing in comparison to similar individuals. We're looking for an ambitious data scientist or data engineer to help us enhance and scale this core part of our platform.Is this role right for you?AboutIf you are excited about building a new healthcare data and analytics platform to support Value Based Care ( VBC) that will help reduce the cost of healthcare in the US and the following matches your skills, experience, and interests:Programming in Python ( R is a plus)Development and deployment of a data intensive product on a SaaS platformStatistical or financial calculations, preferably with healthcare dataResponsibilitiesHelp build Arbital's highly scalable data pipelines and warehouses using platforms like AWS and DatabricksAutomate and maintain data workflows for enrichment, aggregation, and quality assuranceEnsure data accuracy, integrity, privacy, security, and compliance through automated quality control proceduresWork on a team with full-stack engineers and actuarial coders to implement end-to-end solutions for contract adjudicationImplement and scale actuarially sound healthcare financial calculationsCollaborate with actuarial science and delivery teams that primarily work in R and ExcelPartner with data scientists to deploy machine learning models in productionContribute to the design, implementation, and overall development of our productsDrive innovation and deliver valuable features for our customersRequirements2 - 4 yrs of professional experience in data-intensive, SaaS platform development projectsDemonstrated ability to scale new platforms into enterprise productsDemonstrated history of being able to quickly understand complex modeling workflows and the big picture driving the need for themAbility to ship extremely high-caliber code and build exceptional productsHigh level of attention to detailAbility to perform under minimal supervision with accountability for specific objectives and work in a rapidly changing, ambiguous start-up environmentPassionate about improving and innovatingOur team works hybrid from the San Francisco Bay Area. We will prioritize candidates who can work two days per week from our San Francisco office. We will also consider highly qualified remote candidates who can travel to San Francisco for in-person collaboration several days per month.Nice To Have'sStartup experience is highly preferredProduct development coding in RExtensive experience with Airflow, Databricks, Python, and AWS or GCPTools we use includeInfrastructure: AWS, GCP, Databricks, Airflow, Sigma, DatadogCore Tools: Python, R, SQL, Next.js , React, TypeScript, Tailwind CSSIDE: Cursor, VS Code, and R StudioVersion Control: GitHubTeam Planning: Jira, Confluence, FigmaThe target salary range for this role is between $150,000-$160,000 +/- depending upon experienceWhy Join Us?We are assembling a team of creative, talented visionaries seeking to build a new technology that will change healthcare. You will be able to learn, build, and scale our team and technology in a collaborative, creative culture that values every team member.We OfferGenerous equity grants of ISO stock optionsWe offer an exceptional benefits package with high employer-paid contributions for health, dental, and vision insurance4% 401(k) matchFlexible PTO, a weeklong winter shutdown, and 10 holidays each yearFlexible hybrid work blending 1-2x week in-office (SF-area team) and collaboration weeks 1-3x per year (all employees). We are open to remote workers outside the SF Bay Area who are willing to travel for in-person collaboration up to 20% time.Quarterly team offsitesThe opportunity to build a critical software platform that accelerates the American healthcare system's transition to value-based careShow moreShow less",
Data & Analytics Engineer,MojoTech,United States,2025-06-24,https://www.linkedin.com/jobs/view/data-analytics-engineer-at-mojotech-4187566335?position=5&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=MPsiABBLi%2F97Basqr%2FBP9w%3D%3D,LinkedIn,"['Docker', 'Ci/Cd', 'Tensorflow', 'Hadoop', 'Git', 'Machine Learning', 'Data Science', 'Kubernetes', 'Aws', 'Pytorch', 'Spark', 'Kafka']",3 years,"OverviewMojoTech, established in 2008, is an innovative digital consulting company that integrates strategy, design, and engineering to create results-driven digital products and experiences for clients nationwide.What We DoWe partner with our clients to discover, define, design, and build tomorrow’s innovations, empowering every company to unlock future market potential, maximize returns on innovation, and drive transformational change. We work with a diverse client base, ranging from rapidly growing startups to established enterprises, helping them accelerate their pace of change and solve their biggest technology challengeRole OverviewAs a Data & Analytics Engineer with MojoTech you will work with our clients to solve complex problems and to deliver high quality solutions as part of a team. Collaborating with product managers, designers, and clients, you will lead discussions to define data requirements and deliver actionable insights and data pipelines to support client analytics needs. Your experience and level of confidence in decision making and the quality of your work ensure commitment to MojoTech's engineering excellence.Things We’re Looking For3+ years of experience in Data Engineering, Data Science, Data WarehousingStrong experience in PythonExperience building and maintaining ETL/ELT pipelines, data warehouses, or real-time analytics systemsBA/BS in Computer Science, Data Science, Engineering, or a related field or equivalent experience in data engineering or analyticsTrack record of developing and optimizing scalable data solutions and larger-scale data initiativesStrong understanding of best practices in data management, including sustainment, governance, and compliance with data quality and security standardsCommitment to continuous learning and sharing knowledge with the teamShould also be able to check off some of this:Experience developing solutions using LLMs (RAG, Agents, etc...)Exposure to big data technologies like Hadoop, Kafka, or Flink for handling large-scale data processing.Experience with data visualization tools such as Tableau, Power BI, or Looker to communicate insights effectively.Knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) or statistical analysis for predictive analytics.Familiarity with containerization and orchestration tools like Docker and Kubernetes in a data engineering context.Understanding of DevOps practices, including CI/CD pipelines and version control systems (e.g., Git).Previous work with real-time data streaming or event-driven architectures.Comfort collaborating with cross-functional teams, including data scientists, analysts, and software engineers.Experience one or more of the following: Palantir, Databricks, Snowflake, Apache Spark, Airflow, AWS Redshift, AWS Glue, Google BigQuery, Google Dataflow, PowerBI, etc...Exposure to leadership or mentorship roles, particularly on a development teamExperience working in an agency, as a software consultant, or tailoring solutions to match client requirements and capabilityStrong communication skills to articulate technical concepts to technical and non-technical stakeholdersExperience mentoring team members or leading small projects is a plusLocated in RI, MA, CT, NY, NJ, NC, SC, FL, CO, CA, UT, OR, PA, TN, OHBenefitsBase salary $90-150k, determined by experience, skills, and locationPerformance based end of year bonusMedical, Dental, FSA401k with 4% matchTrust-based time offCatered lunches when in office5 hours per week dedicated to self-directed learning, innovation projects, or skill developmentDog Friendly OfficesRemote or in office (offices in Boulder, CO and Providence, RI)Paid conference attendance/yearly education stipendCustom workstation6 weeks parental leaveShow moreShow less",
"Data Engineer, Technology",Point72,"New York, United States",2025-06-15,https://www.linkedin.com/jobs/view/data-engineer-technology-at-point72-4251062192?position=6&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=9lnpPaP06Ch4xl86CM0f8w%3D%3D,LinkedIn,"['Ci/Cd', 'Git', 'Python', 'Aws', 'Spark', 'Azure', 'Scala', 'Sql', 'Agile']",Seniority levelMid-Senior level,"A Career with Point72’s Data Engineering Technology TeamAs Point72 reimagines the future of investing, our Technology group is constantly improving our company’s IT infrastructure, positioning us at the forefront of a rapidly evolving technology landscape. We’re a team of experts experimenting, discovering new ways to harness the power of open-source solutions, and embracing enterprise agile methodology. We encourage professional development to ensure you bring innovative ideas to our products while satisfying your own intellectual curiosity.The Data Engineering Technology team provides Point72 with a scalable data engineering capability and set of data services to support the firm’s expanding data needs. We focus on solutions such as cloud computing, data platforms for large scale data processing, data governance, data quality, enterprise reference data, business automation, and high touch service. We have team members across four continents who collaborate to support the firm’s global footprint of investment strategies and teams.What you’ll doDesign, develop, and maintain robust data pipelines and ETL workflows in Databricks to support quantitative research, trading, and risk management.Collaborate closely with data scientists, analysts, and portfolio managers to understand data needs and deliver scalable data infrastructure.Ingest, process, and normalize large volumes of structured and unstructured financial data from a variety of sources.Optimize performance of data pipelines and ensure high availability, reliability, and data quality across all production systems.Implement data governance best practices, including data lineage, cataloging, auditing, and access controls.Support the integration of third-party data vendors and APIs into the broader data ecosystem.Continuously evaluate and implement new tools and technologies to improve data engineering capabilities, with a focus on cloud-native and distributed processing frameworks.What’s Required3–6 years of professional experience in data engineering or a similar role, ideally within a financial services or high-performance computing environment.Expertise in Databricks, including Spark (PySpark or Scala), Delta Lake, and notebook-based development workflows.Proficiency in building scalable, distributed data pipelines in a cloud environment (preferably Azure or AWS).Strong programming skills in Python and SQL.Solid understanding of data architecture principles, data modeling, and data warehousing.Experience with version control (e.g., Git), CI/CD workflows, and modern data orchestration tools (e.g., Airflow, dbt).Demonstrated ability to work collaboratively in a fast-paced, high-stakes environment with both technical and non-technical stakeholders.Bachelor’s or master’s degree in computer science, engineering, or a related technical field.Commitment to the highest ethical standards.We take care of our peopleWe invest in our people, their careers, their health, and their well-being. When you work here, we provide:Fully-paid health care benefitsGenerous parental and family leave policiesMental and physical wellness programsVolunteer opportunitiesNon-profit matching gift programSupport for employee-led affinity groups representing women, minorities and the LGBTQ+ communityTuition assistanceA 401(k) savings program with an employer match and moreAbout Point72Point72 is a leading global alternative investment firm led by Steven A. Cohen. Building on more than 30 years of investing experience, Point72 seeks to deliver superior returns for its investors through fundamental and systematic investing strategies across asset classes and geographies. We aim to attract and retain the industry’s brightest talent by cultivating an investor-led culture and committing to our people’s long-term growth. For more information, visit www.Point72.com/about.The annual base salary range for this role is $180,000-$225,000 (USD) , which does not include discretionary bonus compensation or our comprehensive benefits package. Actual compensation offered to the successful candidate may vary from posted hiring range based upon geographic location, work experience, education, and/or skill level, among other things.Show moreShow less",
Data Engineer,Agility Partners,"Columbus, Ohio Metropolitan Area",2025-06-23,https://www.linkedin.com/jobs/view/data-engineer-at-agility-partners-4255196344?position=7&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=6X73zYRWrw8I8FGHGa2Mcw%3D%3D,LinkedIn,"['Python', 'Aws', 'Scrum', 'Azure', 'Gcp', 'Sql', 'Agile']",3 years,"**OPEN TO CHARLOTTE AND COLUMBUS**As part of one of our premier client’s ongoing data transformation efforts, we’re seeking a Mid-Level Data Engineer to help build and optimize data pipelines for critical business applications. This role is hands-on, focusing on implementing modern data engineering solutions that streamline data flows, enhance internal operations, and improve customer-facing experiences.You'll work with tools like Snowflake, Databricks, dbt, and 5Tran to develop and maintain scalable data pipelines, collaborating closely with senior engineers and business stakeholders.Key Responsibilities:Data Pipeline Development: Build and optimize end-to-end data pipelines using Snowflake, Databricks, dbt, and 5Tran, ensuring data is structured and ready for business applications.Data Transformation & Modeling: Develop dbt models to transform raw data into structured datasets for analytics and operational use.System Optimization: Support the design and implementation of efficient data structures that improve internal data flows and performance.Collaboration & Communication: Work with cross-functional teams, including senior engineers, analysts, and business stakeholders, to ensure data solutions meet business needs.Code Quality & Best Practices: Write clean, efficient SQL and Python code while following best practices for data engineering, testing, and performance optimization.Agile Development: Participate in Scrum ceremonies, contributing to sprint planning, standups, and retrospectives.Skills & Experience:3+ years of experience in data engineering, with hands-on experience in building data pipelines.Strong knowledge of Snowflake and Databricks for data processing and warehousing.Experience with dbt to create transformations and build reusable data models.Familiarity with 5Tran for data integration and pipeline automation.Proficiency in SQL and data modeling, with a focus on optimizing query performance.Experience working with cloud platforms (AWS, GCP, or Azure) and ETL processes.Ability to troubleshoot data pipeline issues and optimize performance.Strong problem-solving skills and ability to work both independently and collaboratively in a team environment.Experience with Agile/Scrum methodologies and a willingness to learn from senior engineers.Preferred:Experience with Python for automation or additional data engineering tasks.Knowledge of business-specific data concepts like unit costs, skews, and menu data is a plus.Previous experience in foodservice or a similar industry is beneficial.This is an exciting opportunity to grow your expertise in modern data engineering while working on impactful projects within a collaborative team environment!**OPEN TO CHARLOTTE AND COLUMBUS**Show moreShow less",
Data Engineer III,IntePros,"Boston, MA",2025-07-10,https://www.linkedin.com/jobs/view/data-engineer-iii-at-intepros-4250395389?position=8&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=BC0m7Xiu3PcBSWFcIAgPAg%3D%3D,LinkedIn,"['Azure', 'Python', 'Sql', 'Aws']",Seniority levelMid-Senior level,"Data Engineer III – Infrastructure AutomationContract: 3+ months | Boston, MA (Hybrid / Remote)IntePros is searching for an experienced Data Engineer III to support a high-growth Infrastructure Automation program for one of our marquee enterprise clients. You’ll design, build, and optimize analytics solutions inside one of the largest, most sophisticated data-warehouse environments in the world—powering core storage, compute, and virtualized services used by millions of end users and developers every day.What You’ll DoOwn end-to-end development of data and BI solutions—data modeling, ETL design, performance tuning, and reporting.Deliver scalable dashboards and ad-hoc datasets in Oracle Business Intelligence Enterprise Edition (OBIEE).Collaborate with product, engineering, and operations stakeholders to translate ambiguous requirements into clear, actionable reports.Ensure data quality and metadata consistency across multiple sources within a rapidly evolving infrastructure landscape.Automate monitoring and troubleshooting of data pipelines to maintain near-real-time insight into infrastructure health.Must-Have Skills & Experience7+ years in data engineering or BI development within large-scale data-warehouse environments.Expert-level OBIEE development—building complex reports, dashboards, and semantic models.Advanced SQL proficiency for querying, optimization, and performance diagnostics on massive datasets.Proven ability to partner directly with business owners, gather requirements, and deliver high-value analytics products.Strong analytical mindset; comfortable investigating data patterns, anomalies, and root causes across diverse systems.Nice-to-HaveExperience integrating OBIEE with cloud data sources (AWS, Azure, or similar).Scripting skills (Python, Shell) for ETL orchestration or infrastructure automation.Exposure to additional BI tools (Tableau, QuickSight) or data-visualization frameworks.Background in infrastructure, networking, or cloud-service telemetry.Why IntePros?You’ll join a trusted consulting partner with a 20-year track record of matching elite technical talent to mission-critical programs. We offer competitive compensation, weekly pay, and dedicated recruiter support throughout the engagement.Show moreShow less",
"Data Engineer, Remote U.S.",Universal Strategic Advisors LLC,United States,2025-07-05,https://www.linkedin.com/jobs/view/data-engineer-remote-u-s-at-universal-strategic-advisors-llc-4191326127?position=9&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=dXQZst5kFdIRus0oDMtWbw%3D%3D,LinkedIn,"['Data Science', 'Python', 'Sql']",Seniority levelMid-Senior level,"Title: Data Engineer – (Remote, U.S.)About The CompanyUniversal Strategic Advisors LLC (US Advisors) is a small business consulting practice specializing in domestic and international government administration, technology implementation & application, law enforcement operations, homeland security solutions, immigration strategy and operations, border security, and compliance. US Advisors brings proven expertise and a trusted insider network to help our clients solve their toughest challenges while managing risk. We acknowledge that change is accelerating. None of us can afford to think just two steps ahead; every decision we make today impacts our ability to safeguard, evolve, grow, and thrive. Our practice is rooted in deep expertise leading organizations through transformation and building operational, analytical, and technological strategies to meet mission demands.Job DescriptionUS Advisors is seeking aData Engineerto support data-driven decision-making through ETL development, automation, and governance. In this role, you will work within a technical environment, utilizing Python and SQL to develop ETL pipelines, connect data to visualization tools, and automate manual reporting processes. You will play a key role in ensuring data quality, security, and best practices, while also researching emerging technologies and providing architectural guidance.Key ResponsibilitiesData Engineering & ETL Development: Develop ETL scripts in Python and SQL within various data environments.Data Governance & Best Practices: Ensure best practices for data tables, fields, and summary statistics. Develop governance techniques, including validation scripts and utility functions.Data Visualization & Automation: Connect data to Qlik, Tableau, Power BI and other dashboards, automating manual processes, and reporting.Technology Research & Innovation: Research new technologies and develop proofs of concept.Client Engagement & Communication: Conduct client demos, present data insights, and interface with customers to gather requirements and provide technical solutions.Required Qualifications:Bachelor’s degree or higher in Data Science, Engineering, Business or related discipline.3 years or more of experience with Data Visualization and Dashboarding (i.e. Power BI, Tableau, Qlik).3 years or more of experience with relational databases and Python/SQL programming and fundamentals.Experience conducting client demos, presenting data insights, and interfacing with customers.Ability to analyze complex data, identify patterns, and apply critical thinking to develop effective solutions.Ability to prioritize, multitask, work against deadlines, and remain detail oriented.Must be a U.S. citizen with the ability to obtain a DHS Public Trust clearance.Additional Information:This position is a W2 full-time role.Work mode is Remote. Candidates local to D.C. are preferred but not required.All candidates must be a U.S. Citizen.Offer is contingent on candidates passing a thorough background check for federal employment. This position requires a background investigation, including a criminal history check, as part of the employment process. Successful completion of this background investigation is a condition of employment. The background investigation will include verification of employment history, education, and other relevant information as determined by the agency.Benefits:Health & Medical Insurance: Eligibility for employer-sponsored health, dental, and discounted vision coverage.401(k) Plan: Eligibility after one full year of employment, with enrollment available at the beginning of each calendar year.Flexible Time Off (FTO).Federal Holidays: Eleven (11) paid Federal holidays, details to be provided during training/orientation.Compensation:$80,000 annuallyEqual Opportunity Employer:US Advisors is committed to fostering a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, or veteran status.Show moreShow less",
Data Engineer,Statt,"Austin, TX",2025-07-08,https://www.linkedin.com/jobs/view/data-engineer-at-statt-4264160265?position=10&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=HmfnqdAo%2B8jVJ02JeowMWw%3D%3D,LinkedIn,"['Docker', 'Ci/Cd', 'Git', 'Machine Learning', 'Python', 'Graphql', 'Aws', 'Pandas', 'Javascript', 'Azure', 'Gcp', 'Sql']",Seniority levelMid-Senior level,"About the CompanyStatt is a global AI and big data SaaS platform focused on surfacing mission-critical insights for the public policy, regulatory affairs, and strategic communications sectors. Based in the Washington, DC and Austin, TX areas, Statt is dedicated to providing solutions that empower large companies, professional services firms, government agencies, and policy organizations to navigate complex legislative, regulatory, and geopolitical landscapes. Statt was co-founded by Steve Glickman, a former senior economic policy advisor in the Obama White House, and Andrew Platt, a former Maryland state representative. For more information, visit www.statt.com.About the RoleWe are seeking a Data Engineer with approximately three to five years of experience to join our growing team. This role is ideal for someone passionate about building reliable, scalable, and efficient data infrastructure that powers analytics, machine learning, and operational decision-making. You will work with a diverse set of data sources—including APIs and web scraping targets—to design and maintain high-quality ETL pipelines that deliver clean, usable datasets to downstream consumers. If you're skilled in Python, know how to scrape, parse, and structure data, and can manage orchestration in the cloud with a security-first mindset, we’d love to talk.ResponsibilitiesData Pipeline Development:Design, implement, and maintain robust ETL pipelines using tools like Airflow or Prefect. Ensure reliability, idempotency, and high data quality across workflows.Web Scraping & API Integration:Develop scalable data ingestion systems by extracting data from third-party APIs (REST, GraphQL, SOAP) and web sources using tools like BeautifulSoup, lxml, Selenium, and Playwright. Handle pagination, JavaScript-heavy pages, rate limiting, retries, and anti-bot protections.Data Storage & Modeling:Manage data storage across formats (e.g., Parquet, JSON, Avro) and systems (e.g., Snowflake, BigQuery, Redshift). Design normalized and denormalized schemas to support both operational and analytical workloads.Cloud Infrastructure & DevOps:Deploy and monitor pipelines using cloud platforms such as AWS, GCP, or Azure. Leverage containerization (Docker) and infrastructure-as-code tools like Terraform or CloudFormation for scalable and repeatable deployment.Workflow Orchestration:Own and monitor workflow scheduling, dependencies, and failure recovery using orchestration frameworks. Proactively identify and resolve bottlenecks and system errors.Security & Compliance:Implement best practices for handling sensitive data, including encryption, API key and credential management, and compliance with data privacy regulations such as GDPR and HIPAA.Version Control & CI/CD:Use Git and CI/CD tools to version control code and deploy pipeline changes safely and efficiently with minimal downtime.Collaboration & Documentation:Work closely with machine learning engineers and software developers to understand data needs. Create thorough documentation and communicate technical concepts clearly to both technical and non-technical stakeholders.QualificationsExperience:Approximately three to five years of professional experience in data engineering or a closely related field.Programming:Strong proficiency in Python (including pandas, requests, BeautifulSoup, lxml) or similar backend languages.Web Data Integration:Hands-on experience with web scraping, API integration, and dealing with complex data ingestion challenges.ETL Expertise:Demonstrated experience in designing end-to-end ETL pipelines, job orchestration, and ensuring data reliability and quality.Data Warehousing:Proficiency in cloud data warehouses (Snowflake, BigQuery, Redshift) and file formats optimized for large-scale analytics.SQL Mastery:Advanced SQL skills for data transformation, aggregation, and schema design (normalized and denormalized).Cloud & DevOps:Practical experience with cloud platforms (AWS, GCP, or Azure), Docker, and infrastructure automation tools.Workflow Tools:Familiarity with orchestration platforms such as Apache Airflow or Prefect.Security & Compliance:Understanding of data protection regulations and secure handling of sensitive data.Version Control:Experience with Git and CI/CD pipelines in a collaborative software development environment.Preferred SkillsBachelor's or Master’s degree in Computer Science, Data Engineering, Information Systems, or a related field.Certifications in relevant cloud services (AWS, GCP, Azure) or data platforms.Experience working in fast-paced startup environments.Pay range and compensation packageCompetitive salaryEquity in line with company stage and roleComprehensive health, dental, and vision insuranceGenerous PTO and flexible work arrangementsOpportunities for professional growth and developmentCollaborative and inclusive work environment with a passionate and talented teamEqual Opportunity StatementStatt is committed to diversity and inclusivity in the workplace.Show moreShow less",
Data Analytics Engineer,Royal Caribbean Group,"Miami, FL",2025-06-27,https://www.linkedin.com/jobs/view/data-analytics-engineer-at-royal-caribbean-group-4245745661?position=11&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=LFArE8n5d9yO1Z3e%2FpgSwQ%3D%3D,LinkedIn,"['Azure', 'Python', 'Sql', 'Go']",Seniority levelEntry level,"Journey with us!Combine your career goals and sense of adventure by joining our incredible team of employees atRoyal Caribbean Group. We are proud to offer a competitive compensation and benefits package, and excellent career development opportunities, each offering unique ways to explore the world.We are proud to be the vacation-industry leader with global brands — including Royal Caribbean International, Celebrity Cruises and Silversea Cruises — the most innovative fleet and private destinations, and the best people. Together, we are dedicated to turning the vacation of a lifetime into a lifetime of vacations for our guests.Royal Caribbean Group’sRevenue Planning & Analysishas an exciting career opportunity for a full timeAnalystEngineer,Datareporting to theSr. Mgr, RM Dev & SystemsThis position will work on-site in Miami, Florida.Position SummaryThe Revenue Management Development and Systems team is responsible for all data, decision tools, reporting and information needs for the Revenue Planning and Revenue Management teams. This position assists in supporting the team’s database, jobs, and reports. Serves as contact person for the department to help explain the tools and data results. Responsibilities include the design, development, and implementation of data processes, reporting, and analytics solutions. This individual can work under minimal supervision. They are responsible for interacting with various departments within Royal Caribbean to find, consolidate, and manipulate data from multiple large data sets; to analyze and understand results; and to create reports.Essential Duties And ResponsibilitiesMeet with business stakeholders to clarify and document reporting requirementsWorks with management & other team members to understand & clarify data analysis and reporting needsMeet with technical stakeholders to perform code reviews and elicit feedbackAssist in developing, implementing, and maintaining business intelligence reporting and user toolsRecommend data architecture and engineering structures necessary to support reportsDevelop new reporting by creating new queries or coding new processes. Aggregates large data sets in SQL, Python, and other analytical tools for analysis. Develops data strategies, specifically around data structures, identifying critical information, as well as the tools used to retrieve and analyze the data. Performs research and analysis on large data sets - data exploration, trending, etc.Continuously search for potential business improvements, revenue opportunity and efficiency gains.Clearly explain tools and results to teams and management. Develop and conduct clear, concise presentations, communicating technical methods and impacts in business terms.Performs other duties as required. This job description in no way states or implies that these are the only duties to be performed by the employee occupying this position. Employees will be required to perform any other job-related duties assigned by their supervisor or management.Qualifications, Knowledge And SkillsBachelor's degree preferably in Mathematics, Statistics, Computer Science, Economics, Analytics, Business, Engineering or BS/BA with combination thereof and related analytic work experience or relevant certifications.2 years of work experience in data analysis, data mining, business case development or other related analytical projects.Equivalent combination of education and experience will be consideredStrong proficiency in query/reporting tools, SQL, Python or other development tools.Understanding of forecasting, data cleansing and transformationsExperience working with databases, including Oracle, Databricks, or SQL ServerStrong quantitative, analytical, and problem-solving skills.Strong quantitative, analytical, and problem-solving skills.Demonstrated ability to develop and implement applications using programming languagesExperience with SQL requiredExperience in supporting and interpreting reporting using business Intelligence tools (Azure, Synapse, and Databricks experience a plus).Ability to handle dynamic workload, balancing short- and long-term projects.Ability to communicate clearly and effectively to business and technical audiences in both spoken and written formats.We know there's a lot to consider.As you go through the application process, our recruiters will be glad to provide guidance, and more relevant details to answer any additional questions. Thank you again for your interest in Royal Caribbean Group. We'll hope to see you onboard soon!It is the policy of the Company to ensure equal employment and promotion opportunity to qualified candidates without discrimination or harassment on the basis of race, color, religion, sex, age, national origin, disability, sexual orientation, sexuality, gender identity or expression, marital status, or any other characteristic protected by law. Royal Caribbean Group and each of its subsidiaries prohibit and will not tolerate discrimination or harassment.Show moreShow less",
Data Engineer (multiple openings) - IHM,Discover,"Houston, TX",2025-07-10,https://www.linkedin.com/jobs/view/data-engineer-multiple-openings-ihm-at-discover-4235906834?position=12&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=d%2BwBRn7iqDSIYbW23h8rxg%3D%3D,LinkedIn,"['Git', 'Aws', 'Java', 'Spark', 'Go', 'Sql', 'Kafka', 'Agile']",3 years,"Discover. A brighter future.With us, you’ll do meaningful work from Day 1. Our collaborative culture is built on three core behaviors: We Play to Win, We Get Better Every Day & We Succeed Together. And we mean it — we want you to grow and make a difference at one of the world's leading digital banking and payments companies. We value what makes you unique so that you have an opportunity to shine.Come build your future, while being the reason millions of people find a brighter financial future with Discover.Job DescriptionEmployer:DFS Corporate Services LLCJob Title:Data Engineer (multiple openings)Job Location:Houston, TXJob Type:Full TimeDuties:Develops and troubleshoots data integration solutions with complex data transformations and provides guidance to other team members. Influences other team members to achieve commitments per guidance from Chapter Leads and actively contributes to agile ceremonies. Telecommuting and/or working from home may be permissible pursuant to company policies.Requirements:Employer will accept a Bachelor’s degree in Computer Science, Computer Engineering, or related field and 3 years of experience in the job offered or in a Data Engineering-related occupation.Position Required Skills:Position requires: Developing real-time and batch data ingestion and stream-analytic solutions utilizing technologies including Kafka, Apache Spark, SQL, Java, and NOSQL; Implementing data lineage, quality checks, and classification; Technologies including Ab Initio, Unix, Kafka, Teradata, Git, MS SQL, Apache Spark and Oracle; Performing prescriptive, predictive and diagnostic analytic solutions utilizing Spark based ETL solutions; Teradata and AWS cloud ETL Framework.Position eligible for incentives under Employee Referral ProgramRate of Pay:The base pay for this position generally ranges between $113,547.00 to $150,200.00. Additional incentives may be provided as part of a market competitive total compensation package. Factors, such as but not limited to, geographical location, relevant experience, education, and skill level may impact the pay for this position. Benefits: We also offer a range of benefits and programs based on eligibility. These benefits include: Paid Parental Leave; Paid Time Off; 401(k) Plan; Medical, Dental, Vision, & Health Savings Account; STD, Life, LTD and AD&D; Recognition Program; Education Assistance; Commuter Benefits; Family Support Programs; Employee Stock Purchase Plan. Learn more atmydiscoverbenefits.com.QUALIFIED APPLICANTS: Please apply directly through our website by clicking on “Apply Now.”Application DeadlineThe application window for this position is anticipated to close on Jul-22-2025. We encourage you to apply as soon as possible. The posting may be available past this date, but it is not guaranteed.BenefitsWe also offer a range of benefits and programs based on eligibility. These benefits include:Paid Parental LeavePaid Time Off401(k) PlanMedical, Dental, Vision, & Health Savings AccountShort and Long Term Disability, Life, and Accidental Death & Dismemberment insurancesRecognition ProgramEducation AssistanceCommuter BenefitsFamily Support ProgramsEmployee Stock Purchase PlanLearn more at mydiscoverbenefits.com.What are you waiting for? Apply today!All Discover employees place our customers at the very center of our work. To deliver on our promises to our customers, each of us contribute every day to a culture that values compliance and risk management.Discover, a division of Capital One, N.A., is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or other legally protected status. (Know Your Rights)Discover complies with federal, state, and local laws applicable to qualified individuals with disabilities and is committed to providing reasonable accommodations. If you require a reasonable accommodation to search for a position, to complete an application, and/or to participate in an interview, please email HireAccommodation@discover.com. Any information you provide regarding your accommodation needs will be kept confidential and will only be used to determine and provide necessary accommodation.At Discover, we are committed to creating an inclusive and equitable workplace through our Fair Chance Hiring practices. Fair Chance Hiring means that we base our hiring decisions on an applicant’s qualifications rather than their criminal record. All our positions are subject to Section 19 of the Federal Deposit Insurance Act. Our applicants go through a background check, and we follow all applicable local laws, including the Los Angeles County Fair Chance Hiring Ordinance (LA County Fair Chance).Positions marked as remote eligible are limited to remote locations within the country in which the position is based.Applicants must be 18 or older at the time of hire.Show moreShow less",
Jr Data Engineer I,"V-Soft Consulting Group, Inc.","Cincinnati, OH",2025-06-25,https://www.linkedin.com/jobs/view/jr-data-engineer-i-at-v-soft-consulting-group-inc-4256196096?position=13&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=XwnW6J4LUXcCbVZ72xQsyg%3D%3D,LinkedIn,"['Mongodb', 'Hadoop', 'Mysql', 'Java', 'Sql', 'Agile']",Seniority levelEntry level,"Primary Location: Cincinnati, OhioV-Soft Consulting is currently hiring for aJr Data Engineer Ifor our premier client inCincinnati, Ohio.Education And Experience »Technical Degree or related work experience.Experience with non-relational & relational databases (SQL, MySQL, NoSQL, Hadoop, MongoDB, etc.).Experience programming and/or architecting a back end language (Java, J2EE, etc.).Must HaveKnowledge, Skills and Abilities »Business Intelligence - Data Engineering.What You’ll DoJob Responsibilities:Design, construct, install, test and maintain data management systems.Build high-performance algorithms, predictive models, and prototypes.Ensure that all systems meet the business/company requirements as well as industry practices.Install/update disaster recovery procedures.Integrate up-and-coming data management and software engineering technologies into existing data structures.Develop set processes for data mining, data modeling, and data production.Create custom software components and analytics applications.Research new uses for existing data.Employ an array of technological languages and tools to connect systems together.Collaborate with members of your team (e.g., data architects, the IT team, data scientists) on the project’s goals.Recommend different ways to constantly improve data reliability and quality.Handle the design and construction of scalable management systems, ensure that all data systems meet company requirements, and also research new uses for data acquisition.Required to know and understand the ins and outs of the industry such as data mining practices, algorithms, and how data can be used.Interested?Qualified candidates should send their resumes tosbandi@vsoftconsulting.comV-Soft Consulting Group is recognized among the top 100 fastest growing staffing companies in North America, V-Soft Consulting Group is headquartered in Louisville, KY with strategic locations in India, Canada and the U.S. V-Soft is known as an agile, innovative technology services company holding several awards and distinctions and has a wide variety of partnerships across diverse technology stacks.As a valued V-Soft Consultant, you’re eligible for full benefits (Medical, Dental, Vision), a 401(k) plan, competitive compensation and more. V-Soft is partnered with numerous Fortune 500 companies, exceptionally positioned to advance your career growth.V-Soft Consulting provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.For more information or to view all our open jobs, please visit www.vsoftconsulting.com or call (844) 425-8425.Show moreShow less",
(USA) Data Engineer III,Walmart,"Denver, CO",2025-07-11,https://www.linkedin.com/jobs/view/usa-data-engineer-iii-at-walmart-4253291461?position=14&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=sBvvTjZ75J0DvT%2FzVFgygg%3D%3D,LinkedIn,"['Data Science', 'Python', 'Aws', 'Spark', 'Scala', 'Sql', 'Kafka']",3 years,"Position Summary...Join Walmart/VIZIO Data Lake Team as a Data Engineer III! This role offers the opportunity to work with cutting-edge technology to drive data-driven decisions and innovation within our company. If you have a passion for big data and cloud-based ecosystems, this is the perfect opportunity to elevate your career.What you'll do...About the Team:The VIZIO Data Lake Team operates at the forefront of Walmart's data initiatives, focusing on building and maintaining high-performance, high-availability data structures. Our team is pivotal in enabling data insights that drive strategic business decisions across various domains. As a Data Engineer III, you will directly impact our efficiency and data governance, ensuring that our data processes align with business goals.What You’ll Do:Extract and transform data from various internal and external sources.Develop and maintain data pipelines and ETL processes.Implement data governance practices and ensure data quality.Design and develop data models, both logical and physical.Lead projects and mentor junior team members.Collaborate with cross-functional teams to support business needs.Stay updated with current data science and analytics trends.What You’ll Bring:BS or MS in Computer Science or a related field.3+ years of experience in data engineering.Proficiency in Python, Pyspark, SQL, and/or Scala.Experience with relational SQL and NoSQL databases.Strong understanding of in-memory processing and data formats (Avro, Parquet, JSON, etc.).Experience with AWS cloud services (EC2, MSK, S3, RDS, SNS, SQS).Knowledge of stream-processing systems (Storm, Spark-Structured-Streaming, Kafka).Familiarity with data pipeline and workflow management tools (Apache Airflow, AWS Data Pipeline).Bonus: Experience with Databricks, Snowflake, and Thoughtspot.At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.‎‎‎You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable.‎For information about PTO, see https://one.walmart.com/notices.‎‎Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.‎Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.‎For Information About Benefits And Eligibility, See One.Walmart.‎The annual salary range for this position is $99,000.00-$198,000.00‎Additional Compensation Includes Annual Or Quarterly Performance Bonuses.‎Additional compensation for certain positions may also include:‎‎Stock‎‎Minimum Qualifications...Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.Option 1: Bachelor’s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years’ experience insoftware engineering or related field. Option 3: Master's degree in Computer Science.Preferred Qualifications...Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.Data engineering, database engineering, business intelligence, or business analytics, Master’s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart’s accessibility standards and guidelines for supporting an inclusive culture.Primary Location...55 Madison St, Denver, CO 80206-5419, United States of AmericaShow moreShow less",
Data Engineer,SmartLight Analytics,"Plano, TX",2025-07-02,https://www.linkedin.com/jobs/view/data-engineer-at-smartlight-analytics-4260550106?position=15&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=6yRQdEKSHaUVaBtQzarWBQ%3D%3D,LinkedIn,"['Azure', 'Sql']",2-4 years,"Job Title:Data EngineerLocation:Remote/HybridDepartment:Information Technology and Product DevelopmentReports To:Sr. Director of EngineeringJob Summary:The Data Engineer is responsible for building and maintaining robust data processing solutions within our healthcare analytics platform. This role focuses on developing T-SQL stored procedures, managing data transformations, and supporting our reporting infrastructure. The ideal candidate brings 2-4 years of hands-on experience with SQL Server and T-SQL development, with a passion for working with healthcare data.We are seeking a talented Data Engineer to join our growing team. The ideal candidate will have solid experience with SQL Server database development and T-SQL programming. While knowledge of healthcare claims processing and reporting tools is beneficial, we provide comprehensive on-the-job training in these areas. This position offers an excellent opportunity to grow your career in healthcare analytics while working with meaningful data that directly impacts healthcare cost reduction.Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.Key Responsibilities:Develop and maintain T-SQL stored procedures for data processing and transformationDesign and optimize SQL Server databases for healthcare analytics workflowsBuild data extraction and transformation processes using raw T-SQLCollaborate with cross-functional teams to understand data requirements and deliver solutionsSupport data quality initiatives and troubleshoot data processing issuesLearn and utilize myDBR reporting tool for creating reports and visualizationsAssist in developing healthcare claims processing workflows and data validationParticipate in code reviews and maintain documentation for data processesMonitor and optimize database performance for large healthcare datasetsSupport ad-hoc data analysis requests from stakeholdersQualifications:Bachelor's degree in Computer Science, Information Systems, or related field2-4 years of experience in data engineering or database developmentStrong proficiency in SQL Server and T-SQL programmingExperience writing complex stored procedures, functions, and triggersSolid understanding of database design principles and optimization techniquesExperience with data transformation and ETL processes using T-SQLStrong analytical and problem-solving skillsExcellent communication and collaboration abilitiesAbility to work independently and manage multiple prioritiesPreferred Qualifications:Experience with healthcare data, medical claims, or prescription claims processingKnowledge of myDBR or similar reporting toolsUnderstanding of healthcare industry regulations and compliance requirementsExperience with performance tuning and query optimizationBackground in data analysis or business intelligenceFamiliarity with Azure SQL Database or cloud platformsKnowledge of data visualization principlesWho Is SmartLight AnalyticsSmartLight Analytics was formed by a group of industry insiders who wanted to make a meaningful impact on the rising cost of healthcare. With this end in mind, SmartLight works for self-funded employers to reduce the wasteful spend in their healthcare plan through our proprietary data analysis. Our process works behind the scenes to save money without interrupting employee benefits or requiring employee behavior changes.Powered by JazzHR5lK1g6pIEDShow moreShow less",
Data Engineer,SpringPoint Technologies,"Tulsa, OK",2025-07-08,https://www.linkedin.com/jobs/view/data-engineer-at-springpoint-technologies-4262872964?position=16&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=g7tkXpjPIp8w4eXSSkxC4g%3D%3D,LinkedIn,"['Html', 'Javascript', 'Css', 'Azure', 'Sql', 'Agile']",6 years,"🌟Job Title:Senior Data Engineer💻Location:Downtown Tulsa (Hybrid) | 🕒Job Type:Full-Time💰Salary:$90-115KAbout Us:We are looking for a Senior Data Engineer to join our Information Technology team. The successful candidate will perform solution development and delivery working both independently and as part of a team. This position will develop ETL’s, Reports, and data pull requests using tools like Informatica, Tableau, SQL Server, and Snowflake environments. This individual will work a hybrid schedule from our downtown Tulsa office.📝Job Summary:The Senior Data Engineer will analyze business requirements and legacy reports to drive new development utilizing 3rd party development tools, SSIS, and SQL Server. The role involves interfacing with business customers and team leads for scope, estimates, requirements, and project deliverables, as well as working with various IT groups for source-to-target mappings and reporting interfaces.💡What You’ll Do:Analyze business requirements and legacy reports to drive new development.Interface and communicate with business customers and team leads for scope, estimates, requirements, and project deliverables.Work with various IT groups for source-to-target mappings and reporting interfaces.Manage schema changes with DBAs.Create and maintain production support documentation.Use project management and SDLC methodologies to achieve on-time results.Participate in an on-call rotation for system support outside of normal working hours.🔍What We Are Looking For:Bachelor’s degree in an IT-related discipline or equivalent experience.6+ years of experience in complete data warehouse lifecycle development.Experience with data migration initiatives and data warehouse concepts.Proficiency in front-end web technologies (HTML, CSS, JavaScript), SQL Server, SSIS, SSRS, Informatica, Tableau, Power BI, Cognos, and other cloud technologies.Strong analytical skills and ability to re-engineer processes and deliver reporting solutions.Hands-on experience with modern reporting tools like Tableau or Power BI, and backend Microsoft SQL Server (T-SQL) development and/or Snowflake.Strong troubleshooting and problem resolution skills.Experience in an Agile environment and knowledge of Azure DevOps is preferred.Show moreShow less",
Data Engineer,Crum & Forster,"Fairfield County, CT",2025-07-08,https://www.linkedin.com/jobs/view/data-engineer-at-crum-forster-4260745702?position=17&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=oDcGBsYgfDg3%2FrlCIfAZ8g%3D%3D,LinkedIn,"['Ci/Cd', 'Git', 'Python', 'Spark', 'Go', 'Azure', 'Sql']",3 years,"Crum & Forster Company OverviewTravel Insured International (TII),a Crum & Forster company is a leading travel insurance provider with more than 30 years in business. As a key component of our Specialty Business Unit, within the Accident & Health division, TII provides travel protection plans to help each individual travel confidently.Travel Insured International is proud to offer products to consumers and to agency partners of all sizes. We're committed to providing dependable coverage, great value, and end-to-end satisfaction for all customers.Job DescriptionC&F Travel Insured International is seeking a skilled Data Engineer to join our Data Architecture team. The ideal candidate will have a strong technical foundation in data engineering practices and will contribute to the development and maintenance of our analytics infrastructure. Working collaboratively with data scientists, analysts, development, and business stakeholders, the Data Engineer will play a key role in developing, optimizing, and supporting scalable data pipelines and integration processes that power analytics, operations, and business decision-making. The role also involves integrating and exploring external data sources, particularly travel related data, to support various analytics initiatives.What You Will DoDesign, build, and maintain reliable ETL/ELT pipelines using Azure Data Factory and related tools.Integrate data from Cosmos DB, SQL databases, Blob Storage, APIs, and other sources.Collaborate with the Data Architect to implement scalable data models and data lake patterns.Monitor pipeline performance, troubleshoot issues, and ensure high data availability and quality.Partner with analysts, developers, and business stakeholders to deliver trusted datasets and self-service capabilities.Contribute to data governance efforts by documenting flows, maintaining data lineage, and enforcing quality checks.Participate in proof-of-concept (POC) work to validate modern tools such as Fabric, Spark, or streaming ingestion.What You Will Bring To C&FBachelor's degree in Computer Science, Engineering, Information Systems, or related field (Master’s a plus)3+ years of experience in a data engineering or related roleProficiency in SQL and Azure Data Factory (or equivalent orchestration tools)Experience working with cloud databases (e.g., Azure SQL, Cosmos DB, Synapse, Big Query)Strong understanding of data integration, performance tuning, and transformation logicFamiliarity with data lake / lakehouse concepts and best practicesVersion control experience (e.g., Git) and scripting in Python or PowerShellFamiliarity with data privacy regulations (e.g., GDPR, CCPA) and data governance frameworksExcellent problem-solving and analytical skillsStrong written and verbal communicationAbility to work independently and collaboratively in a cross-functional teamDetail-oriented with a passion for clean, well-documented dataDesired Skills (Nice To Have)Experience within the insurance industry is highly beneficial.Familiarity with cloud services, particularly Azure offerings and Serverless architectures.Exposure to Microsoft Fabric, data warehouse design, Apache Spark, or event-driven data architectureExperience building data solutions using Blob Storage, Logic Apps, or DatabricksFamiliarity DevOps/DataOps practices and CI/CD for data deploymentsKnowledge of data governance frameworks and toolsWhat C&F Will Bring To YouCompetitive compensation packageGenerous 401K employer matchEmployee Stock Purchase plan with employer matchingGenerous Paid Time OffExcellent benefits that go beyond health, dental & vision. Our programs are focused on your whole family’s wellness, including your physical, mental and financial wellbeingA core C&F tenet is owning your career development, so we provide a wealth of ways for you to keep learning, including tuition reimbursement, industry-related certifications and professional training to keep you progressing on your chosen pathA dynamic, ambitious, fun and exciting work environmentWe believe you do well by doing good and want to encourage a spirit of social and community responsibility, matching donation program, volunteer opportunities, and an employee-driven corporate giving program that lets you participate and support your communityAt C&F you will BELONGIf you require special accommodations, please let us know. We value inclusivity and diversity. We are committed to equal employment opportunity and welcome everyone regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require special accommodations, please let us knowFor California Residents Only: Information collected and processed as part of your career profile and any job applications you choose to submit are subject to our privacy notices and policies, visit https://www.cfins.com/onlineprivacypolicy/ca/noticeatcollection/ for more information.Crum & Forster is committed to ensuring a workplace free from discriminatory pay disparities and complying with applicable pay equity laws. Salary ranges are available for all positions at this location, taking into account roles with a comparable level of responsibility and impact in the relevant labor market and these salary ranges are regularly reviewed and adjusted in accordance with prevailing market conditions. The annualized base pay for the advertised position, located in the specified area, ranges from a minimum of $78,800 to a maximum of $115,500. The actual compensation is determined by various factors, including but not limited to the market pay for the jobs at each level, the responsibilities and skills required for each job, and the employee’s contribution (performance) in that role. To be considered within market range, a salary is at or above the minimum of the range. You may also have the opportunity to participate in discretionary equity (stock) based compensation and/or performance-based variable pay programs.Show moreShow less",
"Data Engineer, Global Live",TikTok,"San Jose, CA",2025-06-18,https://www.linkedin.com/jobs/view/data-engineer-global-live-at-tiktok-4233976899?position=18&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=2o7aaNmgVc71UYE7nnsCDA%3D%3D,LinkedIn,"['Python', 'Sql', 'Express']",Seniority levelMid-Senior level,"ResponsibilitiesThe Data Platform Global Live team is dedicated to empowering the growth of TikTok LIVE business through big data. We support our businesses in achieving their missions by building high quality real-time and offline data warehouses, creating various forms of efficient and data-friendly data assets, and exploring and implementing business oriented data solutions. We provide stable and reliable data capabilities for daily operations, analyses, decision-making of TikTok LIVE features, in addition to robust data support to enhance live performance for streamers.As a data engineer in the Global Live team, you will have the opportunity to build, optimize and grow one of the largest data platforms in the world. You'll have the opportunity to gain hands-on experience on all kinds of systems in the data platform ecosystem. Your work will have a direct and huge impact on the company's core products as well as hundreds of millions of users.As a Data Solutions Consultant, you will be responsible for:- Understanding business problems, designing and implementing easy-to-use data solutions for businesses.- Collaborating with data engineers to understand the data system and provide a smooth data usage experience for data users, including but not limited to data discovery, data usage guidance, and promotion of the data system.- Collaborating with business teams from all over the world to drive user growth and revenue growth.QualificationsMinimum Qualifications:- Good experience in data solutions, such as data consulting, product development, and data tools creation.- Strong problem-solving and analytical skills with the ability to translate business needs into technical solutions and data metrics.- Have an understanding of data warehousing and the construction ideas of the data system.- Experience with coding (SQL, Python, etc.)- Passionate and self-motivated about using data to drive business decisions.About TikTokTikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.​Why Join UsInspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect – and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.​We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an ""Always Day 1"" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.​​Diversity & Inclusion​TikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.​TikTok AccommodationTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://tinyurl.com/RA-request​Job Information【For Pay Transparency】Compensation Description (Annually)The base salary range for this position in the selected city is $145000 - $410000 annually.​Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.​Benefits may vary depending on the nature of employment and the country work location. Employees have day one access to medical, dental, and vision insurance, a 401(k) savings plan with company match, paid parental leave, short-term and long-term disability coverage, life insurance, wellbeing benefits, among others. Employees also receive 10 paid holidays per year, 10 paid sick days per year and 17 days of Paid Personal Time (prorated upon hire with increasing accruals by tenure).​The Company reserves the right to modify or change these benefits programs at any time, with or without notice.​For Los Angeles County (unincorporated) Candidates:​Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state, and local laws including the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act. Our company believes that criminal history may have a direct, adverse and negative relationship on the following job duties, potentially resulting in the withdrawal of the conditional offer of employment:​1. Interacting and occasionally having unsupervised contact with internal/external clients and/or colleagues;​2. Appropriately handling and managing confidential information including proprietary and trade secret information and access to information technology systems; and​3. Exercising sound judgment.​Show moreShow less",
Data Engineer III,McDonald's,"Chicago, IL",2025-07-11,https://www.linkedin.com/jobs/view/data-engineer-iii-at-mcdonald-s-4265353275?position=19&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=1NYRtu8P8qyIVJ%2B4%2FulgOA%3D%3D,LinkedIn,"['Python', 'Aws', 'Spark', 'Gcp', 'Sql', 'Agile']",Seniority levelMid-Senior level,"Company Description:McDonald’s growth strategy, Accelerating the Arches, encompasses all aspects of our business as the leading global omni-channel restaurant brand. As the consumer landscape shifts we are using our competitive advantages to further strengthen our brand. One of our core growth strategies is to Double Down on the 3Ds (Delivery, Digital and Drive Thru). McDonald’s will accelerate technology innovation so 65M+ customers a day will experience a fast, easy experience, whether at one of our 25,000 and growing Drive thrus, through McDelivery, dine-in or takeaway.McDonald’s Global Technology is here to power tomorrow’s feel-good moments.That’s why you’ll find us at the forefront of transformative technology, exploring new and innovative ways to serve our millions of customers and spread happiness one delicious Hot Fudge Sundae-dipped fry at a time. Using AI, robotics and emerging tech, we’re digitizing the Golden Arches. Combine that with our unparalleled global scale, and we’re reshaping all areas of the business, industry and every community that is home to a McDonald’s restaurant. We face complex tech challenges every day. But that’s where our diverse and talented teams come in. They’re made up of the best and brightest from all over the globe, and they thrive in the space where feel-good meets fast-paced.Check out the McDonald’s  Global Technology Technical Blog to learn how technology and our global team are directly enabling the Accelerating the Arches strategy.Department OverviewMcDonald’s Global Technology – Data & Analytics team is looking to hire a Data Engineer who has a deep understanding of Data Product Lifecycle, Standards and Practices. You will be responsible for building scalable and efficient data solutions to support the company's data products and analytics initiatives. As a Data Engineer, you will collaborate with data scientists, analysts, and other cross-functional teams to ensure the availability, reliability, and performance of data systems. Your expertise in cloud computing platforms, technologies and data engineering best practices will play a crucial role in delivering high-quality data products and enabling data-driven decision-making.DutiesResponsibilities:Builds and maintains relevant and reliable data products that support the business needs. Develops and implements new technology solutions as needed to ensure ongoing improvement with data reliability and observability in-view.Participates in new software development engineering. Helps to define business rules that determines the quality of data, assists the product owner in writing test scripts that validates business rules, and performs detailed and rigorous testing to ensure data qualityDevelops a solid understanding of the technical details of data domains, and clearly understands what business problems are being solvedDesigning and developing data pipelines and ETL processes to extract, transform, and load data from various sources into cloud data storage solutions (e.g., S3, Redshift, GCS, BigQuery).Implementing and maintaining scalable data architectures that support efficient data storage, retrieval, and processing.Collaborating with data scientists and analysts to understand data requirements and ensure data accuracy, integrity, and availability.Building and optimizing data integration workflows to connect data from different systems and platforms.Monitoring and troubleshooting data pipelines, identifying and resolving performance issues and bottlenecks.Ensuring data security and compliance with data governance policies and regulations.Managing data infrastructure on cloud platform, including capacity planning, cost optimization, and resource allocation.Staying up to date with emerging data engineering technologies, trends, and best practices, and evaluating their applicability to improve data systems and processes.Documenting data engineering processes, workflows, and solutions for knowledge sharing and future reference.Ability and flexibility to coordinate and work with teams distributed across time zones, as needed. For instance, early morning/late evening hours to coordinate with teams in IndiaQualificationsRequirements:Bachelor's or Master's degree in Computer Science or related engineering field and deep experience with cloud infrastructureStrong experience in data engineering, specifically with AWS & GCP backend tech stack, including but not limited to S3, Redshift, Glue, Lambda, GCS, BigQuery, Cloud Functions, Cloud Run, etc.Proficiency in programming languages commonly used in data engineering, such as Python.Hands-on experience with big data processing frameworks, such as Apache Spark.Hands-on experience with data modeling, ETL/ELT, and data integration techniques.Working knowledge of relational and dimensional data design and modeling in a large multi-platform data environmentSolid understanding of SQL and database concepts.Expert knowledge of quality functions like cleansing, standardization, parsing, de-duplication, mapping, hierarchy management, etc.Expert Knowledge of data, master data and metadata related standards, processes and technologyAbility to drive continuous data management quality (i.e. timeliness, completeness, accuracy) through defined and governed principlesAbility to perform extensive data analysis (comparing multiple datasets) using a variety of toolsDemonstrated experience in data management & data governance capabilitiesFamiliarity with data warehousing principles and best practices.Excellent problem solver - use of data and technology to solve problems or answer complex data related questionsExcellent communication and collaboration skills to work effectively in cross-functional teams.Preferred Requirements:Experience with JIRA and Confluence as part of project workflow and documentation tools is a plusExperience with Agile project management methods and terminology a plusExperience with Prometheus, GrafanaCompensationBonus Eligible:YesLong - Term Incentive:YesBenefits Eligible:YesAdditional Information:Benefits eligible: This position offers health and welfare benefits, a 401(k) plan, adoption assistance program, educational assistance program, flexible ways of working, and time off policies (including sick leave, parental leave, and vacation/PTO). Eligibility requirements apply to some benefits and may depend on job classification and length of employment.Bonus eligible: This position is eligible for a bonus, calculated based on individual and company performance.Long term Incentive eligible: This position is eligible for stock or other equity grants pursuant to McDonald’s long-term incentive plan.McDonald’s is an equal opportunity employer committed to the diversity of our workforce. We promote an inclusive work environment that creates feel-good moments for everyone. McDonald’s provides reasonable accommodations to qualified individuals with disabilities as part of the application or hiring process or to perform the essential functions of their job. If you need assistance accessing or reading this job posting or otherwise feel you need an accommodation during the application or hiring process, please contact mcdhrbenefits@us.mcd.com. Reasonable accommodations will be determined on a case-by-case basis.McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Nothing in this job posting or description should be construed as an offer or guarantee of employment.Show moreShow less",
Data Engineer,Casey's,"Ankeny, IA",2025-06-27,https://www.linkedin.com/jobs/view/data-engineer-at-casey-s-4255854088?position=20&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=GnfFf73Y3fC15gdCZ2%2FxfQ%3D%3D,LinkedIn,"['Ci/Cd', 'Python', 'Aws', 'Azure', 'Gcp', 'Sql']",Seniority levelMid-Senior level,"At Casey's, every event generates data: a pizza delivery, filling up a gas tank, a truck pulling into the lot, pouring a cup of coffee. Our mission is to use that data to generate insights and help solve the organization's biggest challenges. Our team partners with departments across Casey's to make sense of data and provide direction to decision makers. To enable our strategy, we utilize cutting edge technology tools for data collection, transformation, and analysis. Are you driven to wrangle, cleanse, and transform data? Do you want to partner with leaders to make data driven decisions? Does diving in to a wide variety of data to extract insights excite you? We're looking for innovators who can take an idea from the white board to the board room. If that's you, let's talk.The Data Engineer role will be responsible for identifying data sources, data modeling, developing data pipelines, and collaborating with other team members to troubleshoot and maintain the enterprise data platform. This role works closely with the Analytic Solutions team to deliver insights to a wide variety of business units within Casey’s.Key ResponsibilitiesDesign end to end data pipelines using Azure Data Factory, Databricks (Python, SQL)Help maintain enterprise data platformEvaluates source data systems and understands how to extract and model the data for analyticsPerforms thorough testing of data pipelines and data models to ensure data is accurate and pipelines are resilient and self-healingPairs with other data engineers and architects to design, implement, and maintain new data productsEnable business units to leverage our new enterprise data lake by training them on best practicesCollaborates with report developers and data analysts to implement use case specific data models when requiredMaintains documentation on data flows and pipelinesProvide support for the data platform as part of an on-call rotationCasey's is not able to offer employment sponsorship for work authorization in the United States for this role now or in the future.This role is based onsite at our Store Support Center in Ankeny, IA, working onsite 5 days per week.CompensationStarting pay range:‏‏‎ ‎$81,200‏‏‎ ‎-‏‏‎ ‎$111,700. Actual pay may vary based on Casey’s assessment of the candidate's knowledge, skills, abilities (KSAs), related experience, education, and qualifications. Other factors impacting pay include local prevailing wages and internal equity. This position is eligible for an annual cash bonus based on company performance. Our full salary range for this role does extend beyond the hiring range listed, allowing team members the opportunity to continue to grow within the company.RequirementsBachelor’s degree in Computer Science, Data Analytics, Engineering, Information Technology, or equivalent years of related work experienceAt least three (3) years of experience writing complex SQL or Python for data analyticsHistory of implementing dimensional modeling techniques (Kimball, Inman, etc.)Experience with CI/CD, unit and integration testing, automation, and orchestrationComfortable with a variety of data sources including streams, events, APIs, etc.Strong attention to detail, with the ability to remain organized and self-motivatedStrong interpersonal, verbal, and written communication skillsCapable of identifying problems, analyzing, and evaluating information to determine solutionsExperience with cloud platforms is a plus (Azure, AWS, GCP)Experience with Databricks including administration, security, and infrastructure features is a plusShow moreShow less",
Data Engineer,Royal Caribbean Group,"Miami, FL",2025-06-25,https://www.linkedin.com/jobs/view/data-engineer-at-royal-caribbean-group-4257465234?position=21&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=nVLCtq%2FrqrIJZbUyHbvRng%3D%3D,LinkedIn,"['Git', 'Data Science', 'Python', 'Spark', 'Go', 'Azure', 'Sql', 'Kafka', 'Agile']",3 years,"Journey with us!Combine your career goals and sense of adventure by joining our incredible team of employees atRoyal Caribbean Group. We are proud to offer a competitive compensation and benefits package, and excellent career development opportunities, each offering unique ways to explore the world.We are proud to be the vacation-industry leader with global brands — including Royal Caribbean International, Celebrity Cruises and Silversea Cruises — the most innovative fleet and private destinations, and the best people. Together, we are dedicated to turning the vacation of a lifetime into a lifetime of vacations for our guests.Royal Caribbean Group’sData Analytics and AI Teamhas an exciting career opportunity for a full timeData Engineerreporting to theSenior Manager, Data Solutions Engineering.This role will work in-person in Miramar, Florida.Position SummaryThe Data Engineer will be to support the internal organization in implementing and delivering strategies and results. You will be responsible for building, managing, and optimizing reusable enterprise data pipelines effectively and in a timely manner through the development lifecycle to be used by internal consumers, such as data analysts and data scientists. In this role, you will focus on building and enhancing our Revenue Management Automation (RMA) and Revenue 360 data assets. You will use both technical and analytical skills to understand and solve business problems using available resources and current technology stack, while ensuring data governance and data security compliance. You will also mentor junior engineers in finding optimal and efficient solutions for designing, preparing, and storing data for analytical and operational use cases.Essential Duties And ResponsibilitiesCreate and maintain technical design documentation.Conduct requirements gathering, data mapping and designing.Create, build, and maintain complex data pipelines from disparate sources that meet functional / non-functional business requirements.Create, maintain, and refine existing ETL/ELT processes, employing a variety of data integration and data preparation tools.Develop event-based, real-time, and micro-batch data pipelines.Design and implement high-frequency data science model results for operational purposes.Mentor engineers in finding optimal and efficient solutions for designing, preparing, and storing data for analytical and operational use cases.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing pipelines for greater scalability, etc.Work with stakeholders including Product, Data and Business teams to assist with data-related technical issues and support their data needs.Create datasets for: (1) operational reports, key performance indicators/metrics, or other insights into current organizational activities, (2) analytics and data science to provide the ability to uncover the answers to major questions that help organizations make objective decisions and/or gain a competitive edge.Write, debug, and implement complex queries involving multiple tables or databases across platform(s)Collaborate with the Enterprise Architecture team to ensure alignment on data standards and processes.Work with data and analytics experts to strive for greater functionality in data systems.Position requires on-call and off-hours support.Qualifications, Knowledge And SkillsBachelor of Science in Computer Science, Information Technology, Data Science, Analytics or equivalent.3+ years of experience in a data engineering or related role.3+ years of experience with Python, SQL, and Command Line Interfaces.3+ years of experience with streaming technologies (Kafka, Pubsub, Kinesis) and log-based architectures and experience writing batch and stream processing jobs (i.e. Apache Beam, Google Cloud DataFlow, Apache Spark, Apache Storm).3+ years of experience working and creating datasets for a data warehouse.Clear understanding of data modeling patterns.3+ years of experience with ETL/ELT development tools (Azure Data Factory (ADF) preferred).3+ years of experience with code version control using tools, such as Git, and experience with Agile tools (Jira or Azure DevOps preferred).3+ years of cloud experience (Azure preferred).Experienced in using best practices in designing, building and managing data pipelines that require data transformations as well as metadata and workload management.Experienced in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional and new data integration technologies (such as ETL, ELT, data replication, change data captures, message-oriented data movement, API design, stream data integration and data virtualization)Experienced with implementing data quality frameworks.Expert level knowledge with programming languages including Python, SQL, CLI.Expert level knowledge with relational SQL databases such as Oracle and SQL Server.Experience with ERP systems is a plus.Experience with NoSQL databases is a plus.Knowledge of the hospitality industry is a plus.Experience supporting and working with cross-functional teams in a dynamic environment.Proven ability to collaborate with technical peers.Capable of working independently as well as part of a team.Strive to provide orientation and direction to junior engineers requiring their expertise.Experienced with continuous integration and continuous deployment practices.Ability to approach complex problems with creativity and display analytical and problem-solving skills.Display curiosity in understanding the data for the specific area of responsibility.We know there's a lot to consider.As you go through the application process, our recruiters will be glad to provide guidance, and more relevant details to answer any additional questions. Thank you again for your interest in Royal Caribbean Group. We'll hope to see you onboard soon!It is the policy of the Company to ensure equal employment and promotion opportunity to qualified candidates without discrimination or harassment on the basis of race, color, religion, sex, age, national origin, disability, sexual orientation, sexuality, gender identity or expression, marital status, or any other characteristic protected by law. Royal Caribbean Group and each of its subsidiaries prohibit and will not tolerate discrimination or harassment.Show moreShow less",
"Data Engineer, P2 Science, Data, and Insights",Amazon,"Seattle, WA",2025-07-01,https://www.linkedin.com/jobs/view/data-engineer-p2-science-data-and-insights-at-amazon-4217805111?position=22&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=enSRSVhdxB7WsSG6hcY%2BPg%3D%3D,LinkedIn,['Aws'],Seniority levelNot Applicable,"DescriptionAmazon's Pricing & Promotions Science, Data, and Insights organization is seeking a highly analytical Senior Business Intelligence Engineer to help develop/build data solutions, generate automatic insights and leverage anomaly detection tools/technologies which will help to deliver the best prices to our customers. Data analysis is at the core of Amazon’s culture, and your work will have a direct impact on decision making and strategy for our team. You will be gathering customer insights, mining data, making recommendations, and helping senior leaders make key business decisions.Key job responsibilitiesArchitecture design and implementation of next generation data pipelines and BI solutionsManage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc.Build and deliver high quality data architecture and pipelines to support business analyst, data scientists, and customer reporting needs.Interface with other technology teams to extract, transform, and load data from a wide variety of data sourcesContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersAbout The TeamThe Pricing and Promotions Sciences strive to accelerate Customer, Seller, and overall Amazon success through the proactive development of automated intelligent pricing and promotions science systems, and the facilitation of trustworthy data-driven decision making at scale. We look around corners for both long-term and short-term flywheel causal impacts, and think holistically, from sourcing to CX, and all customer journey touch points in betweenBasic Qualifications3+ years of data engineering experienceExperience with data modeling, warehousing and building ETL pipelinesBachelors DegreePreferred QualificationsExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissionsExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company- Amazon.com Services LLCJob ID: A2967569Show moreShow less",
Junior Data Engineer - Remote,MixRank,United States,2025-06-07,https://www.linkedin.com/jobs/view/junior-data-engineer-remote-at-mixrank-4260824342?position=23&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=susy8h2Oo9cHoEjNK%2FqzkQ%3D%3D,LinkedIn,"['Machine Learning', 'Python', 'Data Science', 'Postgresql', 'Linux']",Seniority levelEntry level,"At MixRank, we create B2B SaaS products that enable sales, marketing, finance, and business intelligence teams to accelerate their business with data and insights into their customers. One that provides the most comprehensive database of mobile apps and websites, technographics, companies, and decision makers. It's a platform created with the sole purpose of providing the fastest way for sales reps to build prospect lists, prioritize leads, and contact decision-makers.We're looking for remote engineers to help with data mining, machine learning/data science, data transformation/ETL, data modeling, database scaling, and more.PostgreSQL experience is highly desired (administration, optimization, DDL, etc). Also looking for experience with Python, Linux, Nix, and data mining.Why Join MixRank? Fully-remote, no HQ office. Team of 44 people across 15+ countries. Invested in by Y Combinator, 500 Startups, Mark Cuban. Profitable and growing 50% every year.Please include your updated resume when applying for this role.Thanks, KiranImportant:Only accept job offers from emails ending in @mixrank.com. Offers from any other email addresses are fraudulent. Do not share personal information with unverified senders.Show moreShow less",
Data Engineer,Harbor.ai,"New York, NY",2025-06-27,https://www.linkedin.com/jobs/view/data-engineer-at-harbor-ai-4258479663?position=25&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=AdySOpF96k8Mlml8hWFv6Q%3D%3D,LinkedIn,"['Data Science', 'Python', 'Sql', 'Aws']",2 years,"About Harbor.ai:Harbor.ai is an InsurTech startup aiming to revolutionize the enterprise insurance market. Our technology simplifies the process for insurance brokers to identify optimal coverage for their clients continuously, and this is just the beginning. Our mission is to revolutionize underwriting for better, faster, and smarter processes. As we pursue this goal, we are implementing a data-driven business model that fundamentally reshapes how insurance products are sold. Established in 2018, Harbor.ai is Venture-backed and based in New York.This role is 100% on-site at our corporate Headquarters in New York City (SoHo).About the Role:We are seeking a skilled Data Engineer to join our growing team and help build the foundation of our data infrastructure. In this role, you will focus on implementing and maintaining robust ETL/ELT pipelines, ensuring data quality, and supporting our analytics and reporting needs across the organization.Your primary responsibilities will include:Building and maintaining scalable data pipelines to integrate data from various sourcesImplementing data cleaning, transformation, and validation processesWorking with SQL databases and cloud data warehousing solutionsCreating efficient data models to support analytics use casesSetting up and maintaining Airflow workflows for process automationCollaborating with analytics and product teams to deliver data solutionsImplementing data quality monitoring and alertingRequired QualificationsBachelor's degree in Computer Science, Data Science, or related field2+ years of experience in data engineeringStrong Python and SQL expertiseExperience with ETL/ELT processes and data pipeline developmentHands-on experience with Airflow or similar workflow orchestration toolsFamiliarity with AWS data services (S3, Redshift, RDS, etc.)Knowledge of data quality best practicesPreferred QualificationsExperience with dbt or similar transformation toolsKnowledge of insurance industry data structures and standards (e.g., ACORD)Previous startup experienceExperience with real-time data processingNote:AI-generated answers are strictly prohibited and will not be tolerated, both within this application and throughout our interview pipeline.Please note that any emails from recruiters will be printed, then ceremoniously disposed of.Working withHarbor.aiHarbor is committed to providing reasonable support (called accommodations) in our recruiting processes for candidates with disabilities, long term conditions, mental health conditions, or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need support, please reach out to Careers@harborai.netIndividual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus, equity or sales incentives, if applicable.In addition to the base salary, Harbor offers PTO, sick time, medical, dental and vision benefits for the employee, as well as a 401k with an employer match.Harbor is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local law. We may use your information to maintain the safety and security of Harbor, its employees, and others as required or permitted by law. Additionally, Harbor.ai participates in the E-Verify program in certain locations, as required by law.Harbor is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at Careers@harborai.netShow moreShow less",
Data Engineer,Well,"New York, NY",2025-06-25,https://www.linkedin.com/jobs/view/data-engineer-at-well-4228503975?position=26&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=y9TmeNILHe0byMJI4W%2B5RQ%3D%3D,LinkedIn,"['Microservices', 'Python', 'Node.Js', 'Aws', 'Gcp', 'Sql']",Seniority levelMid-Senior level,"Company:The mission of Well (https://www.well.co/) is to transform healthcare through our unique impact on our members' health and happiness. We do this through our differentiated consumer experience and world-class data and analytics engine that drive engagement and behavior change. Our product — a consumer health engagement platform — integrates concierge services, behavioral health, telemedicine, care management and wellness services to drive sustained engagement, lower costs and improve the health of members. In addition to our product, we know our team makes us unique. We're a highly diverse and engaged organization whose employees are passionate about the mission of the company and whose management is passionate about the employees. We promote an employee- and member-centric culture with generous benefits, which you can learn more about here: https://www.well.co/careers.Position Title:Data EngineerReporting To:Manager, Data EngineeringLocation:Chapel Hill, NC; Minneapolis, MN; Boston, MA (Newton); New York, NY; Remote in approved locationsSalary:This role can be hired for at either the Data Engineer 1 ($115,000-$135,000) or Data Engineer 2 ($125,000-$145,000), depending on qualifications, plus bonus potential and benefitsPosition Summary:Well is seeking an experienced and self-motivated data engineer to help build the next generation of our data pipelines and data tooling. You will be creating new services, interfaces, data pipelines and cloud infrastructure to support the transfer, ingestion, monitoring and integration of large amounts of data: hundreds of files, millions of events, and trillions of rows in our data lake. We are in the process of rebuilding our data infrastructure for scale which drives mission critical AI features for our products. Come join us, it's a pivotal time to be at Well and you'll make a real difference.Responsibilities:Contribute to the design and implementation of a robust ETL process, managing file ingestion from external parties to GCP BigQuery with a focus on scalability and reliability.Configure, maintain, and extend core data platform and internal data administration tools.Monitor ETL processes with internal and external partners, and respond to events and/or data processing errors.Collaborate with a team of experienced software engineers, DevOps engineers and data scientists to create and deliver a seamless, tailored experience for data team stakeholders.Achieve operational excellence in software development by participating in and leading code reviews, quality assurance, and production monitoringTake personal responsibility for keeping all Well systems and data, including sensitive member data, secure and safe, according to Well data and security policies and HIPAA guidelinesPreferred Qualifications:Experience in delivery of software utilizing Node.js, Python, SQL, and Terraform3+ years of software development experience using cloud providers (AWS and/or GCP)Good understanding of general engineering principles, trade-offs, and systems designRobust coding abilities and demonstrated ability to resolve complex technical challengesExperience with test driven development, an understanding of microservices and big data patterns with insight into choices and tradeoffs involvedDegree and/or background in a related field such as Computer Science, Engineering or a quantitative discipline with strong demonstrated practical experienceWillingness to learn new patterns, languages, technologies and methodologies, aligning both with our Engineering culture and continuously evolving technology environmentExperience working with healthcare data such as insurance claims, electronic health records, and wearables a plusAdditional Job InformationWell is on a mission to redefine the healthcare experience. This is an opportunity to re-shape healthcare for America. We are developing solutions to improve the quality and affordability of healthcare. We welcome team members who are passionate about that mission. We embrace diversity and are committed to building an inclusive team.Well is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. We seek diversity and encourage individuals from underrepresented groups to apply.Show moreShow less",
Associate Data Engineer,Tecovas,"Austin, TX",2025-05-31,https://www.linkedin.com/jobs/view/associate-data-engineer-at-tecovas-4241090933?position=27&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=QQuKh0jKKMGQdUSjvZpvng%3D%3D,LinkedIn,"['Gcp', 'Sql']",3 years,"Tecovas was founded with the simple goal of making the world's best western boots, apparel and leather goods - and selling them at a fair price. We are a brand revolutionizing a category and welcoming first-time boot buyers and western enthusiasts alike.Tecovasis looking for an Associate Data Engineer to play a vital role in building and maintaining the data infrastructure that powers our business decisions.Reporting directly to the Director of Data Engineering, you will be responsible for designing, building, and optimizing our data pipelines, transforming raw data into valuable insights, and ensuring the reliability and performance of our data warehouse. You will have the opportunity to work with cutting-edge cloud technologies and directly contribute to the continued success of Tecovas.This role is required to be based in Austin, TX. Candidates must either be currently located in or willing to relocate to Austin, TX.What you'll do:Design, develop, and maintain scalable and efficient data pipelines using Google Cloud Platform (GCP) services.Orchestrate and schedule complex data workflows using Apache Airflow.Build and optimize data models and schemas within Google BigQuery (BQ).Develop and maintain insightful data visualizations and dashboards using Tableau.Collaborate closely with data analysts, business stakeholders, and other teams to understand their data needs and deliver effective solutions.Implement robust data quality checks and monitoring to ensure data accuracy and reliability.Troubleshoot and resolve data pipeline and data warehouse issues efficiently.Document data pipelines, data models, and ETL/ELT processes clearly.Stay up-to-date with the latest advancements in data engineering and cloud technologies.Experience we're looking for:Bachelor's degree in Computer Science, Engineering, Mathematics, or a related field.3+ years of experience as a Data Engineer or in a similar role.Strong proficiency in Google Cloud Platform (GCP) services, including BigQuery, Cloud Storage, Cloud Functions, Dataflow, and Terraform.Hands-on experience with Apache Airflow for workflow orchestration and scheduling.Solid understanding of data warehousing principles and practical experience with Google BigQuery.Experience developing interactive and informative dashboards using Tableau.Strong SQL skills and experience working with various dataWhat you bring to the table:You possess excellent interpersonal and communication skills.You are highly organized and a self-starter.You feel confident working in a fast-paced environment.You are able to quickly learn new systems and implement new procedures.You can easily collaborate with cross-functional partners.You have a positive attitude and are motivated by a challenge.Full Time Benefits & Perks:We offer insurance plans that pay 79-90% of your health premium coverage and 100% of your dental & vision insurance coverage for your family/dependents401(k) matchPaid Parental LeaveFlexible PTO policyCorporate wellness programCompetitive salary: The base salary for this position is between $100,000-125,000/annually. Exact compensation may vary depending on experience and other qualifications.Eligibility to participate in Corporate Bonus ProgramGenerous employee discounts!About Us:Based in Austin, TX, Tecovas brings the spirit of the West to the modern consumer. Handcrafting the best Western footwear, workwear, apparel, and accessories, Tecovas has grown rapidly since its founding as the first digitally native Western brand in 2015, serving customers through www.Tecovas.com, Tecovas Stores from coast to coast, and select wholesale partners. We're certainly growing- and hiring passionate, humble, positive, and talented people determined to help us continue to grow!Important note:We strive to hire values-aligned people because we believe it takes each and all of us to be successful, and lead with grit, speed and a clear vision of where we're headed. In a remote setting, interviewing at Tecovas may include phone interviews, virtual ""on-site"" interviews, and on-the-job mock cases. We are committed to run a thorough process for candidates with whom we identify a potential match, and we will do our best to follow-up with each and every applicant! If you're on the fence, just give it a try!Hiring process and disclaimer: Should you receive an offer from us after going through the interview process, a background check will be conducted prior to onboarding. The results of a background check are evaluated as part of the hiring process, but this does not mean that you will not be considered for the job based upon the results. We are an equal opportunity employer and we encourage everyone to apply!Show moreShow less",
Staff Data Engineer,CNN,"New York, NY",2025-07-01,https://www.linkedin.com/jobs/view/staff-data-engineer-at-cnn-4247579438?position=28&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=sUJr4FEpxWgektWaFqjRWQ%3D%3D,LinkedIn,"['Machine Learning', 'Python', 'Aws', 'Java', 'Spark', 'Scala', 'Sql', 'Kafka']",8 years,"Welcome to Warner Bros. Discovery… the stuff dreams are made of.Who We Are…When we say, “the stuff dreams are made of,” we’re not just referring to the world of wizards, dragons and superheroes, or even to the wonders of Planet Earth. Behind WBD’s vast portfolio of iconic content and beloved brands, are thestorytellersbringing our characters to life, thecreatorsbringing them to your living rooms and thedreamerscreating what’s next…From brilliant creatives, to technology trailblazers, across the globe, WBD offers career defining opportunities, thoughtfully curated benefits, and the tools to explore and grow into your best selves. Here you are supported, here you are celebrated, here you can thrive.Staff Data Engineer – Atlanta, GA.About Warner Bros. DiscoveryWarner Bros. Discovery, a premier global media and entertainment company, offers audiences the world's most differentiated and complete portfolio of content, brands and franchises across television, film, streaming and gaming. The new company combines Warner Media’s premium entertainment, sports and news assets with Discovery's leading non-fiction and international entertainment and sports businesses.For more information, please visit www.wbd.com.Meet Our TeamThe Data & Analytics organization is at the forefront of developing and maintaining frameworks, tools, and data products vital to WBD, including flagship streaming product Max and non-streaming products such as Films Group, Sports, News and overall WBD eco-system. Our mission is to foster unified analytics and drive data-driven use cases by leveraging a robust multi-tenant platform and semantic layer. We are committed to delivering innovative solutions that empower teams across the company to catalyze subscriber growth, amplify engagement, and execute timely, informed decisions, ensuring our continued success in an ever-evolving digital landscape.Roles & ResponsibilitiesWe are looking for a highly motivated Staff Data Engineer to build a state-of-the-art data platform to solve various data-driven use cases across the organization. This platform will host various data products such as but not limited to, Subscription, Content, and Product Analytics, Personalization and Recommendation, Marketing & Ad-Sales enablement. You will be charged with building a new core data platform in the cloud - handles both streaming and batch data processing, capable of solving any big data initiatives in scope now or evolve in future as well. You will be helping data engineers, analysts, and scientists perform their functions by building highly scalable capabilities across the platform.This individual will bring in his/her expertise in a wide variety of big data processing frameworks (both open source and proprietary), large scale database systems (OLAP and OLTP), stream data processing, API Development, Machine learning operationalization, and cloud automation to build and support all the data needs across our platform.Take lead role in translating various business requirements in to engineering architecture.Build software across our entire cutting-edge data platform, including event driven data processing, storage, and serving through scalable and highly available APIs, with cutting-edge technologies.Change how we think, act, and utilize our data by performing exploratory and quantitative analytics, data mining, and discovery.Think of new ways to help make our data platform more scalable, resilient and reliable and then work across our team to put your ideas into action.Work closely with data analysts and business stake holders to make data easily accessible and understandable to them.Ensure data quality by implementing re-usable data quality frameworks.Develop and enforce data engineering, security, data quality standards through automation.Participate in supporting the platform 24X7.Be passionate about growing team - hire and mentor engineers and analysts.Be responsible for cloud cost and improving efficiency.What To BringBachelor’s degree in computer science or similar discipline8+ years of experience in software engineering and/or data engineeringAbility and willingness to learn any new technologies and apply them at work in order to stay ahead of the curve.Expertise in at least few programming languages - Java, Scala, Python or similar.Expertise in building and managing large volume data processing (both streaming and batch) platform is a must.Expertise in stream processing systems such as Kafka, Kinesis, Pulsar or SimilarExpertise in distributed data processing frameworks such as Apache Spark, Flink or similar.Expertise in SQL and No-SQL – Apache Cassandra, DynamoDB, MySQLExpertise in OLAP databases such as Snowflake or Redshift.Experience in operationalizing and scaling machine models is a huge plus.Experience with variety of data Tools & frameworks (example: Apache Airflow, Druid) will be a huge plus.Experience with Analytics Tools such as Looker, Tableau is preferred.Cloud (AWS) experience is preferredDirect to consumer digital business experience is preferredStrong interpersonal, communication and presentation skills.How We Get Things Done…This last bit is probably the most important! Here at WBD, our guiding principles are the core values by which we operate and are central to how we get things done. You can find them at www.wbd.com/guiding-principles/ along with some insights from the team on what they mean and how they show up in their day to day. We hope they resonate with you and look forward to discussing them during your interview.Championing Inclusion at WBDWarner Bros. Discovery embraces the opportunity to build a workforce that reflects a wide array of perspectives, backgrounds and experiences. Being an equal opportunity employer means that we take seriously our responsibility to consider qualified candidates on the basis of merit, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.If you’re a qualified candidate with a disability and you require adjustments or accommodations during the job application and/or recruitment process, please visit our accessibility page for instructions to submit your request.In compliance with local law, we are disclosing the compensation, or a range thereof, for roles in locations where legally required. Actual salaries will vary based on several factors, including but not limited to external market data, internal equity, location, skill set, experience, and/or performance. Base pay is just one component of Warner Bros. Discovery’s total compensation package for employees. Pay Range: $105,000.00 - $195,000.00 salary per year. Other rewards may include annual bonuses, short- and long-term incentives, and program-specific awards. In addition, Warner Bros. Discovery provides a variety of benefits to employees, including health insurance coverage, an employee wellness program, life and disability insurance, a retirement savings plan, paid holidays and sick time and vacation.Show moreShow less",
Data Engineer,Leader Bank,United States,2025-07-01,https://www.linkedin.com/jobs/view/data-engineer-at-leader-bank-4233100791?position=29&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=h9URJRmnkTvLwjxGWcTVkw%3D%3D,LinkedIn,"['Machine Learning', 'Sql']",Seniority levelEntry level,"DescriptionLeader Bank is looking for exceptionally dedicated team members to join one of the region’s fastest growing community banks and mortgage lenders. At the time of its founding in 2002, Leader Bank had one branch office, $6.5 million in assets and 7 team members. Since then, the Bank has become one of the most successful banks in Massachusetts with $4.5 billion in assets, more than 400 team members, 7 branch offices, and annual mortgage originations of over $2 billion.Exemplary products and an innovative spirit have driven Leader Bank’s rapid growth over the years, and our team members embrace these values. Our mission is to obsess over our clients, make them feel valued, and maintain long-term relationships with them by constantly enhancing our products and processes to always improve our client experience. For our team members, Leader Bank prioritizes competitive compensation and benefits, a healthy work-life balance, and an environment that fosters diversity and inclusion.ResponsibilitiesWrite and maintain efficient data pipelines and ETL processesAssemble, prepare, and analyze large data setsImplement methods to improve data reliability, quality, and securityIdentify opportunities for data enrichmentDevelop analytic tools, programs, and reportsBuild and enhance data models that align with business outcomesMaintain our data stackQualificationsBachelor of Science in Data Engineering or related area of studyExperience in designing databases, warehouses, and analytical systemExperience with Snowflake, Matillion, Retool is a plusExperience deploying machine learning models is a plusAdvanced SQL skills, proficient in PythonPassion for problem solvingLeader Bank offers an excellent compensation and benefits package including 401k plan with corporate match, medical and dental insurance, and the opportunity to work for a fast growing, local organization.Leader Bank, N.A. is an Equal Opportunity and Affirmative Action employer and does not discriminate on the basis of race, color, religion, age, gender, marital status, sexual orientation, national origin, disability, military status, veteran status, or any other protected class.Leader Bank is an E-Verify® participant.* For more information, click on the links below:E-Verify® is a registered trademark of U.S. Department of Homeland SecurityShow moreShow less",
Data Engineer,Rivian,"Palo Alto, CA",2025-07-04,https://www.linkedin.com/jobs/view/data-engineer-at-rivian-4218982138?position=30&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=vYbMupT35IqAxcDqmowiTg%3D%3D,LinkedIn,"['Docker', 'Python', 'Kubernetes', 'Aws', 'Sql', 'Agile']",1-3 years,"About RivianRivian is on a mission to keep the world adventurous forever. This goes for the emissions-free Electric Adventure Vehicles we build, and the curious, courageous souls we seek to attract.As a company, we constantly challenge what’s possible, never simply accepting what has always been done. We reframe old problems, seek new solutions and operate comfortably in areas that are unknown. Our backgrounds are diverse, but our team shares a love of the outdoors and a desire to protect it for future generations.Role SummaryRivian's Commercial Technology Data Team is responsible for the data ingestion, proliferation and reporting needs across web, mobile, and operational applications. We are building a world-class team responsible for data warehousing, reporting and artificial intelligence technologies.We are seeking a Data Engineer to build the ingestion, aggregation, warehousing and reporting that will enhance internal visibility as well as uplift the customer journey.As a Data Engineer, you will develop data models and capabilities for our commercial technology organization. Your work will be instrumental in enabling and empowering data driven decision making at Rivian. In particular, the capabilities and artifacts you produce will be used in driving key commercial decisions.ResponsibilitiesUse dbt and Databricks to develop and maintain comprehensive data catalogs for our factory, and shared services technology domains. Build tooling to support these catalogs with high levels of automation, including dbt tests and jobs.Write clean, readable, and well-documented code. Adhere to coding standards and best practices.Stay up-to-date on industry trends and advancements in data engineering and analytics, in particular dbt, dbt Labs and topics related to warehouse-as-code.Collaborate within a team of data engineers, scientists, and analysts to advance our data catalog and visualization capabilities in Databricks and Hex.Collaborate both in and across teams in the commercial organizationWilling to support on-call rotation as needed for Production systems.QualificationsBachelor's degree in CS or any STEM discipline.1-3 years of professional software development experience, ideally with some exposure to Data Engineering or Analytics Engineering.Proficiency in Python and SQLProficiency with cloud infrastructures (AWS preferred)Familiarity working with AWS Lambdas, Docker and Kubernetes or other serverless and container-based architecturesSolid understanding of databases (SQL and NoSQL.)Understanding of caching, security and privacy considerationsUnit testing and test-driven development experienceSystematic troubleshooting and root cause analysis experienceKnowledge of Agile Development of Accessible Software ToolsPay DisclosureSalary Range/Hourly Rate for California Based Applicants: $105,100-$131,400 (actual compensation will be determined based on experience, location, and other factors permitted by law).Benefits Summary:Rivian provides robust medical/Rx, dental and vision insurance packages for full-time employees, their spouse or domestic partner, and children up to age 26. Coverage is effective on the first day of employment, and Rivian covers most of the premiums.Equal OpportunityRivian is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, sex, sexual orientation, gender, gender expression, gender identity, genetic information or characteristics, physical or mental disability, marital/domestic partner status, age, military/veteran status, medical condition, or any other characteristic protected by law.Rivian is committed to ensuring that our hiring process is accessible for persons with disabilities. If you have a disability or limitation, such as those covered by the Americans with Disabilities Act, that requires accommodations to assist you in the search and application process, please email us at candidateaccommodations@rivian.com.Candidate Data PrivacyRivian may collect, use and disclose your personal information or personal data (within the meaning of the applicable data protection laws) when you apply for employment and/or participate in our recruitment processes (“Candidate Personal Data”). This data includes contact, demographic, communications, educational, professional, employment, social media/website, network/device, recruiting system usage/interaction, security and preference information. Rivian may use your Candidate Personal Data for the purposes of (i) tracking interactions with our recruiting system; (ii) carrying out, analyzing and improving our application and recruitment process, including assessing you and your application and conducting employment, background and reference checks; (iii) establishing an employment relationship or entering into an employment contract with you; (iv) complying with our legal, regulatory and corporate governance obligations; (v) recordkeeping; (vi) ensuring network and information security and preventing fraud; and (vii) as otherwise required or permitted by applicable law.Rivian may share your Candidate Personal Data with (i) internal personnel who have a need to know such information in order to perform their duties, including individuals on our People Team, Finance, Legal, and the team(s) with the position(s) for which you are applying; (ii) Rivian affiliates; and (iii) Rivian’s service providers, including providers of background checks, staffing services, and cloud services.Rivian may transfer or store internationally your Candidate Personal Data, including to or in the United States, Canada, the United Kingdom, and the European Union and in the cloud, and this data may be subject to the laws and accessible to the courts, law enforcement and national security authorities of such jurisdictions.Please note that we are currently not accepting applications from third party application services.Show moreShow less",
Data Engineer,SRM,"Idaho, United States",2025-06-17,https://www.linkedin.com/jobs/view/data-engineer-at-srm-4252879019?position=31&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=%2BR5Qrhc9Xct21GzrVxbtJA%3D%3D,LinkedIn,"['Azure', 'Sql', 'Aws']",0-2 years,"DATA ENGINEERSUMMARY:In this position, you will be responsible for designing, developing, implementing, deploying, documenting, managing, and supporting enterprise cloud ETL processes and environments.PRIMARY FUNCTIONS AND ESSENTIAL RESPONSIBILITIES:Support ENGIE Insight’s computer systems and ETL (“Extract, Transform, Load”) data processes.Translate business requirements into data models that drive data lake or mart or warehouse design and configuration.Actively participate in estimating the level of effort and documenting the work breakdown.Actively participate in team ceremonies (daily stand-up, backlog refinement, review, retrospectives, etc.)Work with cross-functional teams to gather, document, and approve business requirements for data analysis and reporting projects.Actively participate in translating business requirements into a system design specification to manage Unstructured, Transactional, Hierarchical, Master and metadata.Actively participate in designing, developing, and implementing data integration (ETL) processes to transform unstructured and disparate source data into target data stores, lakes, marts, or warehouses.Actively participate in designing, developing, and implementing test automation processes.Review implementation and maintenance of ETL processes that support feature development and testing.Monitor data lake, mart, or warehouse ETL processes and implement tuning to address scalability, recoverability, and performance impediments.Provide on-call support to Business Intelligence environments for internal and external users.Plan and conduct development work on data integration projects necessitating the origination and application of new and unique approaches.QUALIFICATIONS AND REQUIREMENTS:Education/ Certifications/ ExperienceBachelor's Degree required in one of the following fields: Computer Science, Computer Engineering, Electrical & Computer Engineering, Information Systems, or a closely related field0-2 years of data engineering or software engineering experience preferredAzure or AWS (Amazon Web Services) data engineer certification a plusMay consider a combination of relevant experience with educational and other skills and abilities in lieu of educational requirementsCompetencies/ Skills/ AbilitiesStrong verbal, written and interpersonal skills.Experience with the Microsoft Office suite of products requiredFamiliarity with systems: SQL Server, Snowflake, or other database technologyBasic understanding of data modeling preferredBasic understanding of C# or any OOP programming language preferredBasic understanding of SQL query language requiredBasic understanding of OOP concepts preferredBasic understanding of various data formats (XML, JSON, parquet, etc..) requiredAzure Data Factory, Informatica, or another cloud ETL tool experience preferredFamiliarity or work experience with Azure Data Factory a plusOtherPassion to drive Engie Impact’s mission and valuesA t ENGIE, our Goal is to promote, and thrive on diversity, equity, and inclusion. We do so for the benefit of our employees our goal is to support, customers, products and services, and community. ENGIE is proud to be an equal opportunity workplace, and we are firmly committed to creating an equitable and inclusive environment for all employees.We are committed to providing employees with a work environment free of discrimination and harassment. All employment decisions at ENGIE are based on business needs, job requirements, and individual qualifications. ENGIE is committed to providing equal employment opportunities regardless of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship, age, sex or gender (including pregnancy, childbirth, and related medical conditions), gender identity, or gender expression (including transgender status), sexual orientation, marital status, civil union, or domestic partnership status, military service or veteran status, physical or mental disability, protected medical condition, genetic information, or any other legally protected category (referred to as “protected characteristics”) as defined by applicable federal, state or local law in the locations where we operate.The pay range for this role is: $71,400- 120,750Pay range is based on several factors and may vary in addition to a full range of medical, financial, and/or other benefits. Final salary and offer will be determined by the applicant’s background, experience, skills, internal equity, and alignment with geographical market data. This position is eligible for our comprehensive and competitive benefits package including medical, dental, vision, and basic life insurance. Additional ENGIE benefits include a 401k plan, paid time off and annual bonus . ENGIE complies with all federal, state, and local minimum wage lawsWORK ENVIRONMENT:Work schedules are determined by business need and manager discretion; full time employment is considered 40 hours per weekRemote work is permitted from a residence within commuting distance of ENGIE’s Spokane worksite at manager discretionIncumbent must be available for on-call responsibilities during scheduled timeOff-site travel to local events may be required up to 10 days per year.Health & Safety Working RequirementsAdequate working surface (can fit two monitors, a keyboard, mouse, and docking station)Adjustable ergonomic chairProper LightingHeating, air conditioning and ventilation to create a comfortable environmentAppropriate internet and bandwidth to conduct businessIncumbent may be exposed to frequent noise caused by telephones, office machines, and nearby oral communications among fellow employeesAs a global organization, attending meetings and events during early mornings and evenings may be requiredPerforming duties and attending events during the evening and on the weekend occurs occasionally and may be requiredBusiness travel may be required up to 10% of the timeREQUIRED PHYSICAL ACTIVITIES:Manual and physical dexterity needed to operate a computer keyboard and handle paper documentsAdequate hearing and verbal abilities to communicate effectively in person, by telephone, and by video callSufficient near vision acuity to read information appearing on computer display screen, in handwritten forms, and printed on paperEntering text or data into a computer or other machine with a traditional keyboard. Traditional Keyboard refers to a panel of keys used as the primary input device on a computer, typographic machine, or 10-Key numeric keypad.Clarity of vision at approximately 20 inches or less (i.e., working with small objects or reading small print), including use of computers.Business Unit: GBU Energy SolutionsDivision: SRMLegal Entity: ENGIE INSIGHT SERVICES INC.Professional Experience: Skilled ( >3 experience <15 years)Education Level: Bachelor's DegreeShow moreShow less",
Jr. Data Engineer,Agility Partners,"Columbus, Ohio Metropolitan Area",2025-07-07,https://www.linkedin.com/jobs/view/jr-data-engineer-at-agility-partners-4263601894?position=32&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=c9hnjz2sJYiV6B2a0UvRYQ%3D%3D,LinkedIn,"['Git', 'Python', 'Aws', 'Azure', 'Sql']",Seniority levelAssociate,"We are seeking a skilled Data Engineer for an exciting opportunity in Columbus, OH. The ideal candidate will have strong experience in data engineering with a focus on Python, Databricks, SQL, and version control systems.Key Responsibilities:- Design, implement, and optimize data pipelines and workflows using Databricks- Develop and maintain efficient data models and SQL queries for data extraction, transformation, and loading (ETL)- Collaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs- Utilize version control systems to manage and track changes in code and data processes- Ensure data quality and integrity through rigorous testing and validation processesRequired Skills:- Proficiency in Python for data processing and analysis- Extensive experience with Databricks for data engineering tasks- Strong SQL skills for database management and data manipulation- Experience with version control systems (e.g., Git) for collaborative development- Excellent problem-solving skills and attention to detailPreferred Qualifications:- Experience with cloud platforms (e.g., AWS, Azure) for data storage and processing- Familiarity with enterprise data lake technologies and frameworks- Strong communication skills and ability to work independentlyShow moreShow less",
"Senior Software Engineer, Data Engineering",Roku,"Austin, TX",2025-07-11,https://www.linkedin.com/jobs/view/senior-software-engineer-data-engineering-at-roku-4265244869?position=33&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=h7UPDVisH5%2BchZFm%2FCQI0g%3D%3D,LinkedIn,"['Python', 'Aws', 'Spark', 'Gcp', 'Sql', 'Kafka']",Seniority levelMid-Senior level,"Teamwork makes the stream work.Roku is changing how the world watches TVRoku is the #1 TV streaming platform in the U.S., Canada, and Mexico, and we've set our sights on powering every television in the world. Roku pioneered streaming to the TV. Our mission is to be the TV streaming platform that connects the entire TV ecosystem. We connect consumers to the content they love, enable content publishers to build and monetize large audiences, and provide advertisers unique capabilities to engage consumers.From your first day at Roku, you'll make a valuable - and valued - contribution. We're a fast-growing public company where no one is a bystander. We offer you the opportunity to delight millions of TV streamers around the world while gaining meaningful experience across a variety of disciplines.About the teamThe Roku Data Engineering team is dedicated to building a world-class big data platform that allows both internal and external stakeholders to use data for business growth. Working closely with business units and engineering teams, we gather and analyze key metrics that inform critical decisions across new and existing initiatives. As a Senior Data Engineer, you will be responsible for designing data models and creating scalable data pipelines to collect business metrics across all Roku devices.About the roleRoku pioneered streaming to the TV. We connect users to the streaming content they love, enable content publishers to build and monetize large audiences, and provide advertisers with unique capabilities to engage consumers. Roku streaming players and Roku TV™ models are available worldwide through direct retail sales and licensing agreements with TV brands and pay-TV operators. With tens of millions of players sold across many countries, thousands of streaming channels, and billions of hours watched on our platform, building scalable, highly available, fault-tolerant big data platforms is essential for our success.What you’ll be doingBuild highly scalable, available, and fault-tolerant distributed data processing systems for both batch and streaming, processing tens of terabytes of data ingested daily, as well as managing a petabyte-sized data warehouseDevelop robust data solutions and streamline diverse datasets into simplified models to promote self-serviceDevelop data pipelines that ensure high quality and are resilient to low-quality data sourcesTake responsibility for data mapping, business logic, transformations, and data qualityDebugging low-level systems, measuring performance, and optimizing large production clustersEngage in architectural discussions, influence the product roadmap, and take ownership of new projectsSupport and maintain existing platforms while transitioning to newer technology stacks and architecturesWe’re excited if you haveA Bachelor’s or Master’s of Science in Computer Science is preferred5+ years of professional experience as a data or software engineer3+ years of hands-on experience with SQL is requiredProficiency in at least one scripting language, Python, is requiredExperience with big data technologies such as HDFS, YARN, MapReduce, Hive, Kafka, Spark, Airflow, and PrestoProficiency in data modeling, including the design, implementation, and optimization of conceptual, logical, and physical data models to support scalable and efficient data architecturesExperience with cloud AWS/GCP and Looker is a plusCollaborate with cross-functional teams such as developers, analysts, and operations to execute deliverablesBenefitsRoku is committed to offering a diverse range of benefits as part of our compensation package to support our employees and their families. Our comprehensive benefits include global access to mental health and financial wellness support and resources. Local benefits include statutory and voluntary benefits which may include healthcare (medical, dental, and vision), life, accident, disability, commuter, and retirement options (401(k)/pension). Our employees can take time off work for vacation and other personal reasons to balance their evolving work and life needs. It's important to note that not every benefit is available in all locations or for every role. For details specific to your location, please consult with your recruiter.The Roku CultureRoku is a great place for people who want to work in a fast-paced environment where everyone is focused on the company's success rather than their own. We try to surround ourselves with people who are great at their jobs, who are easy to work with, and who keep their egos in check. We appreciate a sense of humor. We believe a fewer number of very talented folks can do more for less cost than a larger number of less talented teams. We're independent thinkers with big ideas who act boldly, move fast and accomplish extraordinary things through collaboration and trust. In short, at Roku you'll be part of a company that's changing how the world watches TV.We have a unique culture that we are proud of. We think of ourselves primarily as problem-solvers, which itself is a two-part idea. We come up with the solution, but the solution isn't real until it is built and delivered to the customer. That penchant for action gives us a pragmatic approach to innovation, one that has served us well since 2002.To learn more about Roku, our global footprint, and how we've grown, visit https://www.weareroku.com/factsheet.By providing your information, you acknowledge that you have read our Applicant Privacy Notice and authorize Roku to process your data subject to those terms.Show moreShow less",
Associate Data Engineer,RxAnte,"Portland, ME",2025-06-07,https://www.linkedin.com/jobs/view/associate-data-engineer-at-rxante-4246611695?position=34&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=WxrcA6Vrqy7d6u%2B4oCnRBg%3D%3D,LinkedIn,"['Ci/Cd', 'Python', 'Sql', 'Aws']",1-2 years,"DescriptionCompany OverviewOver the next ten years, there will be at least 4.6 million hospitalizations from the misuse of prescription drugs in people 65 or older, resulting in $528 billion in annual avoidable costs. RxAnte is on a mission to improve people’s health by helping them get more from medicines. A rapidly growing, tech-enabled healthcare services company with over 30 million lives under management, RxAnte has become a leading provider of value-based pharmacy care management solutions for health plans.RxAnte launched Mosaic Pharmacy Service in 2019, a wholly owned subsidiary designed to offer pharmacy and chronic care management services for our clients’ most medically complex and vulnerable members. Using data, advanced analytics, specialized software and pharmacy automation, Mosaic is transforming the pharmacy experience for medically complex seniors while also helping payers achieve their quality improvement and cost savings objectives.Job ProfileThe Associate Data Engineer reports directly to the VP, Data Strategy and Innovation and is responsible for taking part in the building of systems required to deliver Mosaic and RxAnte's analytic products in a scalable manner. Experience with ETL/ELT, Data analysis, or Data Visualization is required. We are seeking someone who loves to learn new technology, knows data tells a story, is self-driven, and enjoys working on different types of projects. This is a remote work position.Specific Responsibilities IncludeWork with leadership to architect, develop, and maintain processes and programsSupport building out data pipelinesEngage with SQL, Python, SAS, or BI Reporting tools such as Tableau to develop programs for internal stakeholdersCollaborate with internal team to establish and maintain best practicesDocument and QC work with support from senior developers and managerLearn new technologies to support the projects and needs of the teamSuccessfully manage individual projects and raise concerns/roadblocks as they ariseOther activities as neededRequirements1-2 years of relevant/related experience in similar roleBachelor’s degree is required; major in mathematics/statistics, computer science, health services research, health policy, public health, or other relevant fields is preferredExperience with SQL, Python, SAS, or BI Reporting tools such as TableauExperience with cloud environments (ideally AWS) is a plusExperience with administrative healthcare claims data is a plusExperience with CI/CD pipeline is a plusStrong communication, analytical, and data quality skillsAbility to document and work through requirements gatheringExperience working in an environment which utilizes project management tools such as AtlassianShow moreShow less",
Data Engineer II,AppFolio,"Chicago, IL",2025-06-28,https://www.linkedin.com/jobs/view/data-engineer-ii-at-appfolio-4231719557?position=35&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=i0yMmZf6kn5P8MfV3sy3Ow%3D%3D,LinkedIn,"['Docker', 'Data Science', 'Python', 'Kubernetes', 'Aws', 'Sql', 'Kafka', 'Agile']",3 years,"DescriptionWhat we’re looking for:As a member of the Data Platform Engineering team, the Data Engineer II will work collaboratively to develop an infrastructure that ingests data from disparate sources, processes them real-time and routes them to various target storages and applications, while providing access to high quality data to users, ranging from application developers interested in specific events to data analysts keen on business intelligence to data scientists training ML models..At AppFolio, we paddle as one. We ride and make waves together, with a relentless focus on building great products for the way our customers work and live today – and tomorrow. AppFolio is a destination organization where careers are made and accelerated. Here, innovation is a team sport.Your impact:Design, build and operate on next generation data pipeline infrastructure based on Apache Kafka and its ecosystemImprove data architecture, quality, discoverability and access policies to enable and enforce data governanceCollaborate with engineers, data analysts and scientists to ensure that our data infrastructure meets the SLOs of our data-intensive customersDevelop techniques for monitoring the completeness, correctness and reliability of our data setsLeverage agile practices, encourage collaboration, prioritization, and urgency to develop at a rapid paceResearch, share, and recommend new technologies and trendsQualificationsYou have hands-on experience with using Apache Kafka in production and have a passion for building a reliable, scalable and fault-tolerant infrastructure.You have industry experience with working with real time transformation technologies such as Apache FlinkYou have worked with a variety of data sources, including change data capture systems, data streaming and event sourcing in production.You have hands-on experience with data warehouse technology, particularly with Snowflake.You embrace the platform-first approach to build standard solutions and self-serve capabilities for engineering teamsYou want to work with a high degree of autonomy, while at the same time working on initiatives of high importance to the company.You care about work-life balance and want your company to care about it too; you'll put in the extra hour when needed but won't let it become a habit.Must have3+ years of experience with Apache Kafka, Kafka Connect and its ecosystem2+ years of experience in streaming processing technologies, such as Apache Flink2+ years of experience with AWS primitives (IAM, VPC, S3, MSK, EKS, etc.)3+ years of experience working with programming languages like Python or RubyExcellent SQL skills with working knowledge of query optimization2+ years of experience working with Infrastructure as Code, configuration management, and monitoring tools.Bachelors in Computer Science or other quantitative fields.Nice to haveExperience with Debezium connectorsExperience with large scale Data Lakes and Lake Houses, especially with Apache Iceberg is a plusExperience with distributed SQL query engines, such as Trino is a plusExperience with containers and container orchestration tools. Docker and Kubernetes experience is desirable.Data science skills for analyzing data and communicating with ML engineers are a plus.Compensation & BenefitsThe base salary/hourly wage that we reasonably expect to pay for this role is: $104,000 to $130,000.The actual base salary/hourly wage for this role will be determined by a variety of factors, including but not limited to: the candidate’s skills, education, experience, etc.Please note that base pay is one important aspect of a compelling Total Rewards package. The base pay range indicated here does not include any additional benefits or bonuses that you may be eligible for based on your role and/or employment type.Regular full-time employees are eligible for benefits - see here.About AppFolioAppFolio is the technology leader powering the future of the real estate industry. Our innovative platform and trusted partnership enable our customers to connect communities, increase operational efficiency, and grow their business. For more information about AppFolio, visit appfolio.com.Why AppFolioGrow| We enable a culture of high performance, where delivering results is recognized by opportunities for growth and compelling total rewards. Our challenging and meaningful work drive the growth of our business, and ourselves.Learn| We partner with you to realize your potential by investing in you from the start. We're cultivating a team of big thinkers through coaching and mentorship with our best-in-class leaders, and giving you the time and tools to develop your skills.Impact| We are creating a world where living in, investing in, managing, and supporting communities feels magical and effortless, freeing people to thrive. We do this by innovating with purpose while cultivating a culture of impact. We learn as much from each other as we do our customers and our communities.Connect| We excel at hybrid work by fostering an environment that feels flexible, personal and connected, no matter where we are. We create space to fuel innovation and collaboration, and we come together to celebrate, connect, and succeed.Paddle as One.Learn more at appfolio.com/company/careersStatement of Equal OpportunityAt AppFolio, we value diversity in backgrounds and perspectives and depend on it to drive our innovative culture. That’s why we’re a proud Equal Opportunity Employer, and we believe that our products, our teams, and our business are stronger because of it. This means that no matter what race, color, religion, sex, sexual orientation, gender identification, national origin, age, marital status, ancestry, physical or mental disability, or veteran status, you’re always welcome at AppFolio.Show moreShow less",
"Data Engineer II, Amazon Key",Ring,"Seattle, WA",2025-07-07,https://www.linkedin.com/jobs/view/data-engineer-ii-amazon-key-at-ring-4263656824?position=36&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=3Bqen8sGcbknVC3qRCAIaA%3D%3D,LinkedIn,"['Python', 'Bash', 'Aws']",Seniority levelMid-Senior level,"DescriptionWe are actively seeking a motivated and multi-talented individual who is passionate, highly autonomous and have deep expertise in the design, creation, and management of large and complex datasets. You should be an authority at crafting, implementing, and operating stable, scalable solutions to flow huge amounts of data from production systems into the Redshift cluster, and build complex transforms to compute and store new data developed by the software teams.Key job responsibilitiesAs Data Engineer, you develop, review technical design of and enhance data management solutions supporting the reporting Tools. You should excel in the design, creation, management, and business use of large data sets ,deliver data engineering initiatives and build end-to-end analytical solutions that are highly available, scalable, stable, secure, and cost-effective.A day in the lifeDevelop new data engineering patterns that leverage new cloud architecturesBuild and deliver high quality data solutions to support data scientists, engineers and analysts.Interface with software development teams to extract, transform, and load data from a wide variety of data sources.You will work closely with the Business Intelligence Engineers, Product Managers, Software Development teams and other business stakeholders on many non-standard and unique business problems and use creative problem solving to deliver actionable output in data gathering, transformation, ingestion and storage.About The TeamAmazon Key team’s mission is to provide Amazon with 1-click access to every customer's doorstep. We are inventing the next-generation smart delivery operation technologies in IoT (Internet-of-Things). We develop technology-based solutions matching customer needs and delivery capacity with precision and efficiency, and expanding and transforming delivery experience with unprecedented quality, productivity and scale.Basic Qualifications3+ years of data engineering experienceExperience with data modeling, warehousing and building ETL pipelinesExperience with SQLBachelor's degree3+ years of non-internship professional industry experience in software development, data engineering or related field with a track record of manipulating, processing, and extracting value from large datasetsExperience in one or more of: Python, R, Bash, or other scripting languagePreferred QualificationsMaster's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalentExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissionsExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)Knowledge of professional software engineering & best practices for full software development life cycle, including coding standards, software architectures, code reviews, source control management, continuous deployments, testing, and operational excellenceStrong business acumen, proven ability to influence others, strong attention to detail, excellent organization skills, and ability to manage multiple projectsAmazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Company- Amazon.com Services LLCJob ID: A2940781Show moreShow less",
Data Engineer,Genentech,"New York, NY",2025-07-10,https://www.linkedin.com/jobs/view/data-engineer-at-genentech-4252714539?position=37&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=dIUnIToscec%2FkC9kPDxAYQ%3D%3D,LinkedIn,"['Machine Learning', 'Python', 'Sql', 'Aws']",Seniority levelAssociate,"The PositionThe PositionA healthier future. It’s what drives us to innovate. To continuously advance science and ensure everyone has access to the healthcare they need today and for generations to come. Creating a world where we all have more time with the people we love. That’s what makes us Genentech, a member of the Roche Group.The OpportunityAt Genentech and Roche, we're at the forefront of a revolutionary transformation in drug discovery powered by AI and machine learning. Our ""lab in the loop"" strategy processes massive quantities of experimental data to train AI models that accelerate the discovery of new medicines. To enable this vision, we're seeking an exceptional Data Engineer to be part of the team building and maintaining our next-generation Therapeutic Molecule Registration (TMR) platform - a foundational component of our AI-driven drug discovery infrastructure, Lab-in-the-Loop (https://www.youtube.com/watch?v=cN1PxxQWoEc). This platform will serve as the central nervous system for managing and integrating molecular data across our global research organization, handling hundreds of billions of records and enabling unprecedented scale in virtual molecule design and testing. As the volume of AI-generated molecular designs grows exponentially, our TMR platform must evolve to become a high-performance, cloud-native system capable of supporting rapid iteration cycles between computational design and experimental validation. You will be instrumental in consolidating our molecule registration systems into a single, harmonized environment, unlocking the full potential of our data and accelerating the development of life-changing therapies. The ideal candidate will combine data engineering experience with an interest in chemical and biological data management systems.You will work closely with Genentech Computational Sciences (gCS) colleagues, including our machine learning for drug development team, Genentech Research & Early Development (gRED) Drug Discovery teams including the Antibody Engineering division, and other teams across the Roche family of companies to identify, strategize, and productionalize high-impact applications from across the drug discovery and development pipeline. Genentech provides a dynamic and challenging environment for cutting-edge, multidisciplinary research in AI and drug discovery including access to rich sources of data, close links to top academic institutions around the world, as well as internal Genentech and Roche partners and research units.In this role, you will:Implement features of our TMR platformWrite clean, well-tested code following team standardsBuild and maintain data pipelinesFacilitate data migration to TMR and production deploymentParticipate in code reviews and technical discussionsContribute to documentation and testing effortsCollaborate with team members on technical solutionsWho You Are3+ years of data engineering experienceStrong experience with Postgres SQL and OracleSkilled with at least one modern data toolkit (Glue, dbt, Databricks,...)Cloud platform exposure (preferably AWS)Python programming skillsStrong testing practicesBachelor's degree in Computer Science or related field (or equivalent experience)PreferredExperience with data modelling & schema designInvolvement in a database migrationFamiliarity with scientific softwareInterest in chemical/biological data systemsExperience with AWS services#gCS#tech4lifeAIRelocation benefits are available for this job postingThe expected salary range for this position, based on the primary location of New York is $128,300 - 238,300 of hiring range. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law. A discretionary annual bonus may be available based on individual and Company performance. This position also qualifies for the benefits detailed at the link provided below.BenefitsGenentech is an equal opportunity employer. It is our policy and practice to employ, promote, and otherwise treat any and all employees and applicants on the basis of merit, qualifications, and competence. The company's policy prohibits unlawful discrimination, including but not limited to, discrimination on the basis of Protected Veteran status, individuals with disabilities status, and consistent with all federal, state, or local laws.If you have a disability and need an accommodation in relation to the online application process, please contact us by completing this form Accommodations for Applicants.Show moreShow less",
Data Engineer II,Horizon Media,"New York, NY",2025-07-10,https://www.linkedin.com/jobs/view/data-engineer-ii-at-horizon-media-4262546882?position=38&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=qaR9Er1Tp1YWXKqMAReadw%3D%3D,LinkedIn,"['Python', 'Aws', 'Java', 'Postgresql', 'Spark', 'Azure', 'Gcp', 'Sql', 'Mysql']",1-3 years,"Job DescriptionWho We AreHorizon Media, founded in 1989 by Bill Koenigsberg, is recognized as one of the most innovative marketing and advertising firms. We are headquartered in New York City, with offices in Los Angeles and Toronto. A leader in driving business solutions for marketers, Horizon is known for its highly personal approach to client service. Renowned for its incredible culture, Horizon is consistently named to all the prestigious annual Best Places to Work lists published by Fortune, AdAge, Crain’s New York Business and Los Angeles Business Journal. Together we are building a place of belonging.At Horizon, we understand the value that different perspectives can bring to our clients and culture, so we strive for an environment where our employees feel welcomed, safe and empowered. We value YOU and believe that your authentic voice and unique perspective allows us to create a more rewarding culture, and experience, together.Our simple recipe for success? We hire talented people (thinkers, doers, dreamers, makers), challenge them and give them every opportunity to grow.Job SummaryAs a Data Engineer Level 2, you will be support the design and implementation of enterprise-wide analytics solutions that transform raw data into actionable business insights. This role combines deep expertise in data architecture, visualization development, and analytics engineering to create scalable solutions that enable data-driven decision-making across the organization.Key ResponsibilitiesAssist in the development and maintenance of ETL/ELT pipelines using Spark (Java) and Python.Work with relational databases (e.g., PostgreSQL, MySQL, SQL Server) to extract, transform, and load data.Monitor data pipelines and troubleshoot data quality and performance issues.Support integration of media performance data from various sources (e.g., ad servers, DSPs, third-party analytics).Collaborate with stakeholders to understand data needs and support analytics workflows.Contribute to documentation of data sources, data models, and technical processes.QualificationsRequired:Bachelor’s degree in Computer Science, Engineering, Information Systems, or a related field.1-3 years of hands-on experience with:Relational Database Management Systems (RDBMS) and writing efficient SQL queries.Python for data manipulation and scripting.Apache Spark with Java or PySpark (core development and optimization).Familiarity with data modeling, ETL concepts, and big data processing. Strong problem-solving and analytical skills.Attention to detail and a proactive approach to debugging and resolving issues.Effective communication and collaboration skills.Willingness to learn and adapt in a fast-paced environment.Nice To HaveExperience in the media/advertising industry or handling marketing/media datasets.Familiarity with cloud data platforms (e.g., AWS, GCP, Azure).Exposure to workflow orchestration tools (e.g., Airflow, Dagster).#HMHorizon Media is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.Salary Range$140,000.00 - $170,000.00A successful applicant’s actual base salary may vary based on factors such as individual’s skill sets, experience, training, education, licensure/certifications, and qualifications for the role.As an organization, we take an aptitude and competency-based hiring approach.We provide a competitive total rewards package including a discretionary bonus and a variety of benefits including health insurance coverage, life and disability insurance, retirement savings plans, company paid holidays and unlimited paid time off (PTO), mental health and wellness resources, pet insurance, childcare resources, identity theft insurance, fertility assistance programs, and fitness reimbursement.Show moreShow less",
Data Engineer,Capgemini,"Atlanta, GA",2025-07-10,https://www.linkedin.com/jobs/view/data-engineer-at-capgemini-4265031269?position=39&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=5gFHeV3zrynU1YRpXn%2FmdQ%3D%3D,LinkedIn,"['Hadoop', 'Aws', 'Java', 'Scrum', 'Spark', 'Scala', 'Sql', 'Kafka', 'Agile']",9 years,"Choosing Capgemini means choosing a company where you will be empowered to shape your career in the way you’d like, where you’ll be supported and inspired by a collaborative community of colleagues around the world, and where you’ll be able to reimagine what’s possible. Join us and help the world’s leading organizations unlock the value of technology and build a more sustainable, more inclusive world.Job DescriptionRole Summary/Purpose:Senior Data Engineer with AWS architect/engineering experience to be part of our scrum teams and perform functional & system development for Hadoop applications for our Enterprise Data Lake initiative.This is high visibility fast paced key initiative will integrate data across internal and external sources, provide analytical insights and integrate with our critical systems.Essential Responsibilities:Participate in the agile development processDevelop functional and technical specifications from business requirements for the commercial platformEnsure application quality and adherence to performance requirementsHelp create project estimates and plans. Represent engineering team in project meetings and solution discussionsParticipate code review processWork with team members to achieve business results in a fast paced and quickly changing environmentPair up with data engineers to develop cutting edge Analytic applications leveraging AWS, Big Data technologies: Kafka, PySpark/Spark, Hadoop, Shell Scripting, SQLMentor and influence up and down the chain of commandPerform other duties and/or projects as assignedQualifications/Requirements:Bachelor's degree in a quantitative field (such as Engineering, Computer Science, Statistics, Econometrics) and a minimum of 9 years of experienceMinimum 5 years' experience working with and developing big data solutionsHands-on experience on AWS, writing shell scripts, complex SQL queries, Hadoop commands, Kafka, Spark, and GitExperience with real-time data streaming systemsAbility to write abstracted, reusable code componentsProgramming experience in at least two of the following languages: Scala, Java or PythonDesired Characteristics:Strong business acumenCritical Thinking and CreativityPerformance tuning experienceWillingness to learn new technologies quicklySuperior oral, and written communication skills, as well as the willingness to collaborate across teams of internal and external technical staff, business analysts, software support and operations staff.Skills - AWS Architect/engineering ExperienceLife at CapgeminiCapgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:Flexible workHealthcare including dental, vision, mental health, and well-being programsFinancial well-being programs such as 401(k) and Employee Share Ownership PlanPaid time off and paid holidaysPaid parental leaveFamily building benefits like adoption assistance, surrogacy, and cryopreservationSocial well-being benefits like subsidized back-up child/elder care and tutoringMentoring, coaching and learning programsEmployee Resource GroupsDisaster ReliefDisclaimerCapgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.Click the following link for more information on your rights as an Applicanthttp://www.capgemini.com/resources/equal-employment-opportunity-is-the-lawCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, generative AI, cloud and data, combined with its deep industry expertise and partner ecosystem.Show moreShow less",
Data Engineer,Tradeweb,"New York, NY",2025-07-01,https://www.linkedin.com/jobs/view/data-engineer-at-tradeweb-4260297001?position=40&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=3WFl5qmhHY9FM6%2BWpH3a%2Fw%3D%3D,LinkedIn,"['Tensorflow', 'Machine Learning', 'Python', 'Pytorch', 'Spark', 'Go', 'Sql', 'Kafka']",3 years,"Company DescriptionJOB DESCRIPTIONTradeweb Markets is a world leader in the evolution of electronic trading. A fintech company serving approximately 2,500 clients – including the world’s largest banks, asset managers, hedge funds, insurance companies, wealth managers and retail clients -- in more than 65 countries across the globe. Since our first trade in 1998, we have helped transform and electronify the fixed income markets. Tradeweb is a culture built on innovation, creativity and collaboration. Through a combination of very talented and driven people, innovative products and solutions, cutting-edge technology, market data, and a vast network of clients, we continue to work together to improve the way financial markets trade.Mission: Move first and never stop. Collaborate with clients to create and build solutions that drive efficiency, connectivity, and transparency in electronic trading.Tradeweb Markets LLC (""Tradeweb"") is proud to be an EEO Minorities/Females/Protected Veterans/Disabled/Affirmative Action Employer.https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfRole OverviewWe're looking for a forward-thinking engineer with a strong foundation in modern technologies, including AI, to help drive the modernization of our Finance systems. This role sits within the Finance Transformation team and reports directly to the Head of Finance Transformation, while partnering closely with Technology, Data and AI teams across the firm.You’ll play a key role in implementing and customizing a complex SaaS billing platform, integrating Finance systems into a centralized data lake, and scaling Finance AI use cases from concept to deployment.Job ResponsibilitiesPartner with Finance, Product, Technology, and Data teams to gather requirements and translate them into scalable solutionsDesign data models and schemas to support billing, revenue, and financial reporting processesBuild core engineering components, including ETL pipelines, mediation logic, and rating algorithms for revenue calculation and invoice generationImplement and customize a third-party SaaS billing platform tailored to enterprise Finance needsDrive data integration initiatives to connect Finance systems with the firmwide data lakeLead development of Finance-focused AI use cases, including prototyping, testing, and scaling applications across workflowsTroubleshoot production issues and own platform support across billing, reporting, and data pipelinesContribute to code reviews, QA tracking, and continuous improvement of engineering practicesWrite rigorous unit tests and uphold high standards for reliability and accuracy in financial data processingBuild connectors to move trade and financial data between trade repositories, billing systems, and financial platformsQualificationsBS or higher in a technical field such as Computer Science, Mathematics, Physics, or a related discipline2–3 years of experience coding in Python, including real-world or production useSkilled at writing and debugging complex SQL queriesHands-on experience with ETL or stream processing tools such as Kafka, Spark, Flink, Airflow, or PrefectFamiliarity with modern AI/ML frameworks (e.g. Scikit-learn, PyTorch, TensorFlow, Hugging Face, MLflow, Kubeflow)Experience building, fine-tuning, or integrating machine learning or generative AI solutionsComfortable using AI-assisted development tools like GitHub Copilot, ChatGPT, or Code InterpreterProactive, curious, and driven — a go-getter who’s excited to take ownership and push projects forwardEnjoys solving messy, complex problems by breaking them into simple, elegant piecesLearns fast, adapts quickly, and thrives in fast-paced, high-growth environmentsPassionate about automation, clean design, and continuous improvementAdditional InformationTradeweb is committed to providing valuable and competitive benefits. In addition to working in our culture of innovation and collaboration, we offer:Health Insurance: Highly competitive medical, dental, and vision programsHybrid Environment: Our employees have the flexibility of working in the office and from home.Health Care and Dependent Care Flexible Spending Accounts: You may elect to set aside pre-tax earnings to pay for eligible health care and dependent day care expenses for you and your eligible family members.Maven Family Building Benefit: Maven offers support for fertility and preconception; pregnancy and post-partum; adoption; surrogacy and pediatrics for children up to age 10. Tradeweb provide a $10,000 lifetime reimbursement towards fertility, egg freezing, adoption and surrogacy expenses.Building Wealth - 401(k) Savings Plan: Employees are immediately eligible for the 401(k) plan. Participants may contribute up to 75% of eligible compensation into a traditional 401(k) and/or Roth 401(k). Tradeweb will match 100% of the first 4% of compensation that you contribute.The current pay range for this role if performed in the city of New York is currently $120,000 to $250,000 per year, based on a regular, full-time schedule. The amount of pay offered will be determined by a number of factors, including but not limited to qualifications, market data, geographic location, and internal guidelines.Other Benefit ProgramsPre-Tax Commuter Benefits ProgramARAG Legal ServicesEmployee Assistance ProgramTuition ReimbursementFinancial Wellness ToolsTravel Assistance BenefitsPet InsuranceCorporate Gym SubsidiesWellness PerksPaid Time Off and Parental LeaveShow moreShow less",
Data Engineer,Lazard Asset Management,"New York, United States",2025-07-02,https://www.linkedin.com/jobs/view/data-engineer-at-lazard-asset-management-4246012425?position=41&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=vgafsX2xxD9Xr3x%2FXpxnLg%3D%3D,LinkedIn,"['Git', 'Data Science', 'Python', 'Linux', 'Azure', 'Sql']",5 years,"AmericasAsset ManagementNew YorkLazard is one of the world’s preeminent financial advisory and asset management firms.Our people and culture make the difference. While global in presence and reach, ours is a close, collaborative community of just over 3,000 professionals. Lazard is a place of continuous knowledge sharing, skill development and relationship building, where professionals grow and succeed together. Our entrepreneurial culture, flat structure and embrace of individual differences, allow creative ideas, original concepts, and unique perspectives to drive our business forward — and for careers to take flight.Many of the world’s leading investors — from individuals to institutions across the globe — have entrusted Lazard Asset Management. We pride ourselves in uncovering the best investment opportunities for our clients. The purpose of our asset management business is to help our clients invest for the future — whether it’s for retirement, to grow and preserve inter-generational wealth, or to benefit the organizations that make our world smarter, healthier and more sustainable.Lazard Asset Management (LAM) is looking for a Data Engineer to join our Enterprise Platform Technology team. This role will be responsible for supporting combined data requirements, dedicated to data acquisition and data ingestion.We’ll trust you to:Support various data science workloads by creating, setting up Azure Linux machines, generating certs for SSH and Git access for automated account management with Ansible across all cloud VMs, etc.Configure, package management and maintain Linux VMs using shell scripting.Data processing and scrip development using Python libraries.Work on a team of data engineers and data architects to acquire data from internal and external sources, ingest into our repositories, and optimize it for data consumptions by end-users.Implement methods to improve data reliability and quality.Combine raw information from different sources to create consistent formats.Develop and test architectures for data extraction and transformation.Build and maintain optimal data pipeline architectures.Assemble large and complex data sets.You’ll need to have:5+ years of experience in writing SQL queries.5+ years of experience in developing ELT workloads on-prem and in the cloud.3+ years of experience in Python.2+ years of experience in working with Linux and shell scripting.1+ years of experience in DevOps: Gitlab CI/ CD pipelines (both YAML and UI based).Experience with SQL based database technologies.Knowledge of best practices for code quality, testing, and performance optimization.What We OfferWe strive to enhance the total health and well-being of our employees through comprehensive, competitive benefits. Our goal is to offer a highly individualized employee experience that enables you to balance your commitments to career, family, and community. When you work for Lazard, you are working for an organization that cares about your unique talents and passions and will continue to invest in the development of your career.We expect the base salary range for this role to be approximately $140,000 – 160,000 USD. Various factors contribute to determining the actual base compensation offered, including but not limited to the applicant’s years of relevant experience, career tenure, qualifications, level of education attained, certifications or other professional licenses held, relevant skills for the role. Base salary is one component of Lazard Asset Management’s compensation package, which also includes comprehensive benefits and may include incentive compensation.Does this sound like you?Apply! We’ll get in touch on the next steps.Inclusion at LazardLazard is an intellectual capital business focused on delivering the best advice and solutions to clients. Achieving these objectives requires us to identify, develop and retain the best talent. A workforce comprised of people with varied backgrounds and experiences creates a rich diversity of thought that empowers us to challenge conventional wisdom, as diverse perspectives lead to better decisions.Our appreciation of diversity’s strength is ingrained in our multicultural heritage. As a global firm that has grown organically from local roots in different countries, we have a deep tradition of respect for individual differences, which has been core to our success for 175 years.The ongoing cultivation of an inclusive culture are essential to our continued growth. We are committed to sustaining an environment in which all employees – regardless of socioeconomic status, race, ethnicity, nationality, religion, gender, gender expression, sexual orientation, physical abilities, veteran or military status – can maximize their individual potential, as well as our collective success.#LAMShow moreShow less",
Data Engineer Graduate (Ads Data Application) - 2025 Start (BS/MS),TikTok,"San Jose, CA",2025-07-06,https://www.linkedin.com/jobs/view/data-engineer-graduate-ads-data-application-2025-start-bs-ms-at-tiktok-4186476591?position=42&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=FzvV13q27iPr8fSfOP2oiA%3D%3D,LinkedIn,"['Hadoop', 'Machine Learning', 'Python', 'Express', 'Data Science', 'Java', 'Spark', 'Go', 'Sql', 'Kafka']",Seniority levelInternship,"ResponsibilitiesTikTok's immersive experience, global presence, and high engagement makes it the ideal marketing destination for business, big and small, to showcase their unique brand identity, connect with their consumers, and build strong lasting relationships over time. The Ads Data Team builds and manages the petabyte scale data infrastructure, batch/realtime pipelines and services to support Tiktok's global Ads business. We are committed to building a robust data foundation and scalable data applications, unblocking the full potential of advertising data to optimize advertiser experience, boost business growth, empower strategy execution.We are looking for talented individuals to join our team in 2025. As a graduate, you will get unparalleled opportunities for you to kickstart your career, pursue bold ideas and explore limitless growth opportunities. Co-create a future driven by your inspiration with TikTok.Successful candidates must be able to commit to an onboarding date by end of year 2025.We will prioritize candidates who are able to commit to the company start dates. Please state your availability and graduation date clearly in your resume.Applications will be reviewed on a rolling basis. We encourage you to apply early.Candidates can apply for a maximum of TWO positions and will be considered for jobs in the order you applied for. The application limit is applicable to TikTok and its affiliates' jobs globally.We are looking for passionate Data Engineers that have strong problem solving skills to join forces with talented cross functional partners (business operation, data science, engineering and product management) to solve some of the most interesting data challenges with efficiency and quality. In this role, you will contribute to the company's core business across innovative advertising products, campaign management and measurement solutions. You will see a direct impact from your day-to-day work to customer satisfaction and company growth.Responsibilities:Work closely with Product Managers, Data Scientists/Analysts, and Software/Machine Learning Engineers and other stakeholders to understand data requirements and deliver data solutions that meet business needs.Evaluate, implement and maintain data infrastructure tools and technologies to support efficient data processing, storage and query.Design, build and optimize scalable data pipelines to ingest, process and transform large volumes of data.Design and implement robust data models and visualization to support complex analytical queries and reporting requirements.Ensure the data integrity, accuracy and consistency of data by implementing data quality checks, validation processes and monitoring mechanisms.Continously optimize data pipelines, queries and processes to improve performance, reduce latency and enhance scalability.Provide rapid response to SLA oncall support to business critical data pipelines.Create and maintain good documentation for data assets and promote best practices for data governance within the data user community.QualificationsMinimum Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field.Experience as a Data Engineer or similar role in supporting data-centric business.Strong knowledge of SQL and experience working with relational and non-relational databases.Proficiency in programming languages such as Python, Java, Go etc.Solid understanding of data modeling and data warehousing concepts, data integration and ETL/ELT techniques.Effective communication skills and ability to collaborate effectively with cross-functional teams.Preferred Qualifications:Experience with big data technologies(e.g. Apache Hadoop, Spark, Kafka, Flink) and working with terabyte to petabyte scale data.Experience with cloud data warehouses(eg. Snowflake, Databricks, BigQuery) and modern business intelligence/data stack.Experience with data governance, data privacy and compliance.Experience in the advertising, e-commerce or gaming industry.Excellent problem-solving skills, attention to detail, and ability to thrive in a fast-paced environment.Job Information【For Pay Transparency】Compensation Description (Annually)The base salary range for this position in the selected city is $112000 - $250000 annually.Compensation may vary outside of this range depending on a number of factors, including a candidate’s qualifications, skills, competencies and experience, and location. Base pay is one part of the Total Package that is provided to compensate and recognize employees for their work, and this role may be eligible for additional discretionary bonuses/incentives, and restricted stock units.Benefits may vary depending on the nature of employment and the country work location. Employees have day one access to medical, dental, and vision insurance, a 401(k) savings plan with company match, paid parental leave, short-term and long-term disability coverage, life insurance, wellbeing benefits, among others. Employees also receive 10 paid holidays per year, 10 paid sick days per year and 17 days of Paid Personal Time (prorated upon hire with increasing accruals by tenure).The Company reserves the right to modify or change these benefits programs at any time, with or without notice.For Los Angeles County (unincorporated) Candidates:Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state, and local laws including the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act. Our company believes that criminal history may have a direct, adverse and negative relationship on the following job duties, potentially resulting in the withdrawal of the conditional offer of employment:Interacting and occasionally having unsupervised contact with internal/external clients and/or colleagues;Appropriately handling and managing confidential information including proprietary and trade secret information and access to information technology systems; andExercising sound judgment.About TikTokTikTok is the leading destination for short-form mobile video. At TikTok, our mission is to inspire creativity and bring joy. TikTok's global headquarters are in Los Angeles and Singapore, and we also have offices in New York City, London, Dublin, Paris, Berlin, Dubai, Jakarta, Seoul, and Tokyo.Why Join UsInspiring creativity is at the core of TikTok's mission. Our innovative product is built to help people authentically express themselves, discover and connect – and our global, diverse teams make that possible. Together, we create value for our communities, inspire creativity and bring joy - a mission we work towards every day.We strive to do great things with great people. We lead with curiosity, humility, and a desire to make impact in a rapidly growing tech company. Every challenge is an opportunity to learn and innovate as one team. We're resilient and embrace challenges as they come. By constantly iterating and fostering an ""Always Day 1"" mindset, we achieve meaningful breakthroughs for ourselves, our company, and our users. When we create and grow together, the possibilities are limitless. Join us.Diversity & InclusionTikTok is committed to creating an inclusive space where employees are valued for their skills, experiences, and unique perspectives. Our platform connects people from across the globe and so does our workplace. At TikTok, our mission is to inspire creativity and bring joy. To achieve that goal, we are committed to celebrating our diverse voices and to creating an environment that reflects the many communities we reach. We are passionate about this and hope you are too.TikTok AccommodationTikTok is committed to providing reasonable accommodations in our recruitment processes for candidates with disabilities, pregnancy, sincerely held religious beliefs or other reasons protected by applicable laws. If you need assistance or a reasonable accommodation, please reach out to us at https://tinyurl.com/RA-requestShow moreShow less",
Data Engineer,Veracity Software Inc,"Charlotte, NC",2025-06-27,https://www.linkedin.com/jobs/view/data-engineer-at-veracity-software-inc-4255467947?position=43&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=RafDUu4DQL9YmRFyEHCAww%3D%3D,LinkedIn,"['Java', 'Python', 'Gcp']",Seniority levelMid-Senior level,"Job Title: Data EngineerDuration: 12 Months (Contract)Locations: Charlotte, NC / Phoenix, AZ / Minneapolis, MNJob DescriptionWe are seeking an experiencedData Engineerto join our team and contribute to the design, development, and optimization of data solutions supporting enterprise-level initiatives. The successful candidate will work on large-scale data processing, transformation, and analytics using cutting-edge cloud and ETL technologies.ResponsibilitiesDesign, build, and maintain scalable data pipelines on GCP (Google Cloud Platform).Develop and optimize queries in BigQuery to support analytics and reporting.Implement data transformation and ETL workflows using Ab Initio.Work with Teradata to integrate and manage large datasets.Ensure data quality, integrity, and security across all solutions.Collaborate with data architects, business analysts, and application teams to meet business requirements.Contribute to performance tuning, error handling, and automation of data processes.Required SkillsStrong hands-on experience with GCP and BigQuery.Proficiency in Ab Initio ETL development.Experience working with Teradata databases.Desired SkillsExperience with Java or Python for data processing and automation.Prior work in Finance or Banking domain is a plus.Show moreShow less",
Data Engineer,OneImaging,United States,2025-06-10,https://www.linkedin.com/jobs/view/data-engineer-at-oneimaging-4248345712?position=44&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=uTlBWOFQoPwq292QpFWk%2Fw%3D%3D,LinkedIn,"['Docker', 'Mongodb', 'Ci/Cd', 'Machine Learning', 'Python', 'Aws', 'Java', 'Kubernetes', 'Postgresql', 'Spark', 'Scala', 'Sql']",Seniority levelAssociate,"We are seeking aData Engineerto join our dynamic team. The ideal candidate is an enthusiastic problem-solver who excels at building scalable data systems and has hands-on experience withDatabricks,Looker,AWS,MongoDB,PostgreSQL, andTerraform. You will work alongside sales, customer success and engineering to design, implement, and maintain the operational data infrastructure that powers our analytics and platform offerings.Key ResponsibilitiesData Pipeline & IntegrationDesign, build, and maintain end-to-end data pipelines using Databricks (SparkSQL, PySpark) for data ingestion, transformation, and processingIntegrate data from various structured and unstructured sources, including medical imaging systems, EMRs, Change-Data-Capture from SQL Databases, and external APIsAnalytics & VisualizationCollaborate with the analytics team to create, optimize, and maintain dashboards in LookerImplement best practices in data modeling and visualization for operational efficiencyCloud InfrastructureDeploy and manage cloud-based solutions on AWS (e.g., S3, EMR, Lambda, EC2) to ensure scalability, availability, and cost-efficiency using IaC tooling (Terraform and Databricks Asset Bundles)Develop and maintain CI/CD pipelines for data-related services and applicationsDatabase ManagementOversee MongoDB and PostgreSQL databases, including schema design, indexing, and performance tuningEnsure data integrity, availability, and optimized querying for both transactional and analytical workloadsSecurity & ComplianceAdhere to healthcare compliance requirements (e.g., HIPAA) and best practices for data privacy and securityImplement error handling, logging, and monitoring frameworks to ensure reliability and transparencyImplement data governance frameworks to maintain data integrity and confidentialityCollaboration & DocumentationWork cross-functionally with data scientists, product managers, and other engineering teams to gather requirements and define data workflowsDocument data pipelines, system architecture, and processes for internal and external stakeholdersRequirementsEducation & ExperienceBachelor's or Master's degree in Computer Science, Engineering, or a related field3+ years of professional experience in data engineering or a similar roleTechnical SkillsDatabricks (Spark): Proven expertise in building large-scale data pipelinesLooker: Experience in creating dashboards, data models, and self-service analytics solutionsAWS: Proficient with core services like S3, EMR, Lambda, IAM, EC2, etcMongoDB & PostgreSQL: Demonstrated ability to design schemas, optimize queries, and manage high-volume databasesSQL & Scripting: Strong SQL skills, plus familiarity with Python, Scala, or Java for data-related tasksSoft SkillsExcellent communication and team collaboration abilitiesStrong problem-solving aptitude and analytical thinkingDetail-oriented, with a focus on delivering reliable, high-quality solutionsPreferredExperience in healthcare or imaging (e.g., DICOM, HL7/FHIR)Familiarity with DevOps tools (Docker, Kubernetes, Terraform) and CI/CD pipelinesKnowledge of machine learning workflows and MLOps practicesBenefitsHealth Care Plan (Medical, Dental & Vision)Retirement Plan (401k, IRA)Paid Time Off (Vacation, Sick & Public Holidays)Training & DevelopmentWork From HomeShow moreShow less",
Jr Data Engineer,Insight Global,"Plano, TX",2025-06-19,https://www.linkedin.com/jobs/view/jr-data-engineer-at-insight-global-4253529290?position=45&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=%2BSNRVUquekfF3qpOSlpaIA%3D%3D,LinkedIn,"['Gcp', 'Kafka', 'Agile', 'Aws']",Seniority levelAssociate,"Key ResponsibilitiesBuild and maintain data pipelines usingSnowflake,DBT,Kafka, and other tools.Perform data transformations, error handling, and sensitive data scrubbing.Support business-facing data needs (e.g., DL4/reporting).Collaborate with cross-functional teams acrossPlano,San Antonio, and international locations.Contribute to monitoring and reliability efforts (Grafana, Datadog, Fortuna exposure is a plus).Work closely with FTEs and 3rd-party team members in a fast-paced, agile environment.Must-Have:SnowflakeDBTKafka (receiving side)Cloud services (preferably AWS or GCP)Nice-to-Have:DataStage, TalendGrafana, Datadog, FortunaFamiliarity with Google SRE principlesExposure to fraud models or machine learningShow moreShow less",
Data Engineer,Ascendion,"Overland Park, KS",2025-07-08,https://www.linkedin.com/jobs/view/data-engineer-at-ascendion-4263138080?position=46&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=OuUGjxh86pmpWZ4g9diU4Q%3D%3D,LinkedIn,['Sql'],2-4 years,"About AscendionAscendion is a fullservice digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.Ascendion | Engineering to elevate lifeWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:Build the coolest tech for the world’s leading brandsSolve complex problems – and learn new skillsExperience the power of transforming digital engineering for Fortune 500 clientsMaster your craft with leading training programs and hands-on experienceExperience a community of change makers!Join a culture of high performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.About the roleJob Title : Data EngineerLocation - Onsite in Overland Park, KansasKey Responsibilities:Use SQL programming skills to write complex stored procedures, views, queries, user defined functions and common table expressions.Create normalized Fact and Dimension Tables from multiple disparate data sources.Use SSIS to acquire data from multiple sources (text files, databases, etc), process SQL queries and normalize data (ETL).Use source control to manage iterations of code.Complete ad hoc projects to support operations and end user analysis.Stay updated with advancements and best practices in design and development.Will be a part of an On-Call rotation with some after hours and weekend work required.Required Qualifications:50% Database development using SQL-Transact30% SQL Server Integration Services (SSIS), Extract Transform Load (ETL)20% Identify bugs, perform root cause analysis and implement solutionsAzure and snowflakeKQL is preferred and top priority, if not then regular SQL. Bachelor’s degree in Computer Science or relevant fieldMinimum of 2-4 years’ experience developing in a time-critical environmentStrong SQL programming skills including complex stored procedures, complex SQL queries, and user defined functions, common table expressions and transaction management using best practicesIdeally Team Foundation Server and Visual Studio experience requiredSSIS, SSRS, SSAS and ETL experience requiredSalary Range:The salary for this position is between $85,000 to $95,000 USD annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.Benefits:The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance][shortterm disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [1015 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]This position is eligible for commissions in accordance with the terms of the Company’s plan. Commissions for this position are estimated to be based on individual performance. Additionally, this role is also eligible for a bonus based on the achievement of mutually agreed KRAs.Want to change the world? Let us knowTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!Show moreShow less",
Data Engineer,Flock Freight,"Encinitas, CA",2025-06-14,https://www.linkedin.com/jobs/view/data-engineer-at-flock-freight-4261113232?position=47&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=h7g11NeRYQxMcQ9AjsM3Dw%3D%3D,LinkedIn,"['Machine Learning', 'Python', 'Data Science', 'Sql', 'Kafka', 'Agile']",1-3 years,"Flock Freight is a FreightTech company that uses patented technology to move freight more efficiently, reliably, and sustainably. The company’s guaranteed terminal-free truckload service, FlockDirect, finds the best options to pool freight among billions of possible combinations, optimizing routes while finding and filling trucks’ empty spaces. Shipments stay safe in a single truck, driven by a single driver, all the way to their destination allowing shippers to only pay for the space they need and enabling carriers to earn more from every linear foot of capacity — all while slashing carbon emissions by up to 40% compared to traditional shipping methods.Flock Freight has been a Certified B Corp since 2020, meeting high social, environmental, transparency, and accountability standards to all of our stakeholders.About The RoleFlock Freight is looking for an exceptional engineer who is passionate about building the data infrastructure that enables the development and maintenance of complex analytical and data science processes. The data engineering team empowers the rest of the organization to access and act on data as effectively as possible to make the freight world a better place. A successful engineer will collaborate closely with data consumers to build a data platform and tooling to help scale the business.ResponsibilitiesBuild and maintain scalable, maintainable, data transformation pipelines using Python and dbt to power the whole company's data science and analytics needsStand up new low-latency data processing infrastructure to provide clean data to machine learning modelsApply software engineering best practices to the development, testing, and deployment of data systems and toolingIntroduce and improve workflow orchestration and automation processesIdentify, synthesize, and prioritize stakeholder requirements and opportunities for improvements to existing systemsDefine and demonstrate patterns for writing, documenting, and distributing data transformation codeParticipate in incident response for production data systems and monitoring automationHelp maintain secure data governance and access policiesWrite and maintain detailed but clear technical documentationQualifications1 to 3 years of experience working as a data engineer or in dedicated data engineering courseworkExperience working on complex data pipelines using SQL (preferably Snowflake) and PythonExperience using popular data transformation frameworks like dbt or SQLMeshUnderstanding of data modeling strategies for both transactional and data warehousing workloadsA solid conceptual grasp (and preferably hands-on experience with) orchestration frameworks like Airflow, Dagster, Prefect, or similarFamiliarity with cloud data architecture such as asynchronous messaging (Kafka, Pub/Sub, or similar), HTTP(S), REST, S3, and data storage formats such as Avro, Parquet, and IcebergFamiliarity with common statistical, data visualization, and machine learning use cases, and preferably experience with popular cloud BI tools such as Mode, Sigma, or LookerExceptional problem solving, analysis, decomposition, and communication skills applied within an agile development environmentExcellent communication, collaborative demeanor and ability to work in distributed, multi-functional teams with the ability to articulate a point of view.Important Note: We require verification of an individual’s legal right and eligibility to work in the United States. Please note, this position is based in Encinitas, CA, and is not open to remote work at this time.The expected on-target earnings range for this position is $120,000.00 - $135,000 per year. This range reflects typical earning potential in this role. When determining an offer, we take into account a variety of factors that are important in making compensation decisions including, but not limited, to: skill sets; experience and tenure; education and certifications; and other business and organizational needs. The disclosed range estimate is based on market data. Salary bands may, in some circumstances, be adjusted to a different geographic area depending on the candidate's position and location.In addition to salary, full-time employees are also eligible for an equity package and our competitive benefits that support you and your family as part of your total rewards package at Flock Freight. Our policy is that we target candidates local to one of Flock Freight’s headquarters location (Encinitas, CA)Life & Benefits Of The FlockHybrid Work Model: As an organization we value in-office collaboration, working cross-functionally, and winning together. Through scheduled in-office and work from home days, we are able to work more efficiently and collaboratively.401(k) Employer Match: We know Flock may just be a stepping stone in your epic journey, but we want to be sure you’re setting your future self up for success! For that reason, we provide a 401(k) plan that offers a 4% employer match.Medical, Dental & Vision: Generous coverage for employees and dependents. Both HMO and PPO options provided.Our Promise To YouFlock Freight is committed to creating an environment that’s fair and inclusive. We fill our open positions based on qualifications, merit, and business needs. We value the skills of people from all backgrounds and are proud to hire, promote, and retain talent from a diverse candidate pool. Diverse perspectives are central to innovation at Flock Freight and make our team better. We're interested in your inherent abilities, not just the skills you bring from your last role - if you think you have what it takes to succeed in the role but don't check every box, please still get in touch. We'd love to start a conversation with you.Flock Freight has zero tolerance for behavior that negatively impacts marginalized groups, including women, people of color, veterans, immigrants, people with disabilities, and members of the LGBTQIA+ community. We invite people of all identities to join the Flock!Show moreShow less",
Data Engineer,Yellow Bloom Solutions,"Princeton, NJ",2025-07-03,https://www.linkedin.com/jobs/view/data-engineer-at-yellow-bloom-solutions-4260172644?position=48&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=qDR6%2BxG8Egh7Qu5YGPUuJw%3D%3D,LinkedIn,"['Azure', 'Scrum', 'Sql', 'Agile']",Seniority levelMid-Senior level,"Job Title:Data EngineerLocation:Princeton, NJ (Hybrid – 3 Days Onsite)Employment Type: Contract / Full-Time(NOT C2C)Position SummaryWe are looking for a highly skilledData Engineerto design and develop scalable data solutions using Microsoft Azure technologies. The ideal candidate will have hands-on experience in building modern data pipelines, implementing BI solutions, and working with structured and unstructured data. This position requires a strong background in data engineering, Azure cloud services, and a collaborative mindset for working in cross-functional Agile teams.Key ResponsibilitiesDevelop and maintain end-to-end data pipelines using Azure Data Factory, Azure Databricks, and PySpark.Design and implement robust data models supporting data staging (bronze), refined (silver), and curated (gold) layers.Perform ETL/ELT processing of data from multiple sources including flat files, relational databases, and cloud storage.Build and deploy tabular models using Azure Analysis Services (SSAS) and support reporting systems with optimized queries.Leverage Azure Data Lake Analytics and Azure Data Lake Store to manage and process large-scale data sets.Create and automate workflows using Azure Integration Runtime, Event Hubs, and Stream Analytics.Collaborate with analysts, data scientists, and business stakeholders to understand requirements and deliver effective data solutions.Write efficient and maintainable code in notebooks for data movement and transformation in Databricks.Implement data quality, validation, and governance best practices throughout the pipeline lifecycle.Required Technical SkillsCloud & Data Engineering:Azure Data Factory (ADF)Azure DatabricksAzure Data Lake (ADLS, ADLA)Azure Analysis Services (SSAS)Azure Integration RuntimeAzure Event HubsAzure Stream AnalyticsDBT (Data Build Tool)Database & ProgrammingAzure SQLMongoDBPySparkETL & Reporting ToolsSQL Server Integration Services (SSIS)SQL Server Reporting Services (SSRS)Preferred Domain ExperienceFamiliarity with data from actuarial or insurance domains, including:Client hierarchies, treaty structures, renewal processesExposure and experience data, pricing models, and program structuresExperience building pipelines that support analytics, pricing tools, and reportingTeam & Collaboration SkillsStrong written and verbal communication skills with the ability to explain technical details to non-technical stakeholders.Experience working in Agile/Scrum development environments.Demonstrated ability to work both independently and in a collaborative team environment.Strong decision-making skills and the ability to manage priorities in high-pressure scenarios.What You’ll GainOpportunity to work on modern data architecture using the latest Azure technologiesA dynamic and collaborative work culture focused on innovation and problem-solvingFlexible hybrid work schedule with a base in Princeton, NJSkills: sql server intergration services,azure analysis services,azure data lake,server reporting,azure integration runtime,sql server integration services,microsoft bi/azure bi solutons,azure data factory,azure sql,dbt,pyspark,azure analysis,azure stream analytics,azure databricks,sql server reporting services,azure event hubs,mongodbShow moreShow less",
Data Engineer II - Ad Intelligence,ESPN,"Glendale, CA",2025-06-04,https://www.linkedin.com/jobs/view/data-engineer-ii-ad-intelligence-at-espn-4244565856?position=49&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=Kljs9awRz4hnFw0mM3TGEQ%3D%3D,LinkedIn,"['Ci/Cd', 'Microservices', 'Hadoop', 'Machine Learning', 'Python', 'Data Science', 'Java', 'Scala', 'Sql', 'Agile']",Seniority levelMid-Senior level,"On any given day at Disney Entertainment & ESPN Product & Technology, we’re reimagining ways to create magical viewing experiences for the world’s most beloved stories while also transforming Disney’s media business for the future. Whether that’s evolving our streaming and digital products in new and immersive ways, powering worldwide advertising and distribution to maximize flexibility and efficiency, or delivering Disney’s unmatched entertainment and sports content, every day is a moment to make a difference to partners and to hundreds of millions of people around the world.A few reasons why we think you’d love working for Disney Entertainment & ESPN Product & Technology:Building the future of Disney’s media: Our Technologists are designing and building the products and platforms that will power our media, advertising, and distribution businesses for years to come.Reach, Scale & Impact: More than ever, Disney’s technology and products serve as a signature doorway for fans' connections with the company’s brands and stories. Disney+. Hulu. ESPN. ABC. ABC News…and many more. These products and brands – and the unmatched stories, storytellers, and events they carry – matter to millions of people globally.Innovation: We develop and implement groundbreaking products and techniques that shape industry norms, and solve complex and distinctive technical problems.Ad Platforms organization within Disney Entertainment and ESPN Product & Technology is fully responsible for building, enhancing and maintaining the high-performance, distributed, microservice-based Advertising Platform across all of Disney online properties, including Hulu and ESPN+. We build and maintain proprietary technology, ranging from ad serving and ad delivery, campaign management, reporting as well as all the integrations internal and external that come with evolving and maintaining a best-in-class video advertising business.The Ad Intelligence team is under Ad Platforms and its mission is to transform advertising and Disney's Ad platform with data and AI across TV and streaming video. We build solutions to measure and optimize every aspect of the advertising life cycle. Our tenant is a strong cross-domain team to deliver E2E solutions covering tech areas ranging from machine learning, big data, microservices to data visualization. Our team is seeking a senior software engineer who will be a core team member for our advertising data platform engineering group. This engineering group focuses on big data infrastructure, operational data, audience solution, inventory forecasting and full funnel measurements as a foundation layer for Disney’s addressable Ad Platforms. The right person for this role should have extensive experience on bigdata as well as solid expertise on backend services or full stackResponsibilitiesBuild components of large-scale data platform for real-time and batch processing, and own features of big data applications to fit evolving business needsBuild next-gen cloud based big data infrastructure for batch and streaming data applications, and continuously improve performance, scalability and availabilityContribute to the best engineering practices, including the use of design patterns, CI/CD, code review and automated testAs a key member of the team, contribute to all aspects of the software lifecycle: design, experimentation, implementation and testing.Collaborate with program managers, product managers, SDET, and researchers in an open and innovative environmentDay-to-Day technical operations of the Data & BI channelEnsure monitoring and alerting on production services are in place and be available on-call for production incidents.Triage and troubleshoot operational issues, run point on escalations, and provide Tier 2/3 supportDeliver on channel-specific key performance metrics (KPM) to track channel health and growthDevelop strong partnerships with ad sales, business intelligence, engineering, product management, and analytics leaders to drive focus on strategic and tactical channel objectivesAnalyze and communicate the impact of key strategy, market, or product changesBasic Qualifications:3+ years of relevant data science, Business Intelligence, or data analytics experience working with large data pipelinesExperience delivering products in an agile software development environment including sprint work in JiraStrong SQL experience including writing/optimizing complex queries against distributed data stores such as Aurora, Hadoop, Hive, Oracle, and SnowflakeExperience with data pipeline and workflow management tools: Apache NiFi or Apache AirflowExpert level knowledge in at least one programming language such as Python, R, Scala, Java.Demonstrated experience working with business intelligence tools such as MicroStrategy, Tableau, Business Objects, Looker, etc.Demonstrated ability to effectively communicate complex concepts to technical and unacquainted audiencesStrong presentation and documentation skills including experience working with Confluence, Power Point, and LucidChartStrong analytical and critical thinking skillsExcellent interpersonal skills with the ability to collaborate and partner across multiple teams and organizationsPreferred Qualifications:Experience in ad technology, operations, or support, preferably with focus in video advertising technology or digital advertising systemsDemonstrated ability with cloud infrastructure technologies, including Terraform, K8S, Spinnaker, IAM, ALB, and etc.Experience with ClickHouse, Druid, Snowflake, Impala, Presto, Kinesis, etc.Experience with network topologies, systems architecture overviews, and entity relationship diagrams (ERD)Experience using data analytics to triage and troubleshoot advertising workflowsRequired Education:Bachelor’s degree in Computer Science, Data Analytics or Science, Applied Mathematics, Statistics, related technical fields, or equivalent experienceAdditional Information#DISNEYTECHThe hiring range for this position in Glendale, California is $104,600 to $140,200 per year, in Seattle, Washington is $109,500 to $146,800 per year, and in San Francisco, California is $114,500 to $153,500 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.Show moreShow less",
Senior Data Engineer,Included Health,United States,2025-07-10,https://www.linkedin.com/jobs/view/senior-data-engineer-at-included-health-4264612684?position=50&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=oL2UTZR6k3j82cMQztUZAw%3D%3D,LinkedIn,"['Machine Learning', 'Python', 'Aws', 'Azure', 'Gcp', 'Sql']",Seniority levelMid-Senior level,"Job SummaryThe Senior Data Engineer will help build our data infrastructure. You'll work with product, engineering, and analytics teams to ensure data is reliable, accessible, and ready for powering analytics, reporting, and machine learning use cases., reporting directly to the Engineering Manager.ResponsibilitiesDesign scalable data pipelines (batch and streaming)Build our data warehouse and data lake solutionsCollaborate with data scientists, analysts, and software engineers to improve data availability and qualityImplement data quality, observability, and governance best practicesSupport ETL/ELT processes, data transformations, and data model designContribute to the selection and implementation of modern data tools and frameworksParticipate in on-call rotations for critical data infrastructureQualifications5+ years of hands-on experience in data engineeringProficiency in SQL and one or more programming languages (Python)Hands-on experience with modern data platforms such as BigQuery, Snowflake, Databricks, or RedshiftExperience with data pipeline orchestration tools (Airflow, dbt, Prefect)Experience with data modeling, data warehousing concepts, and ETL/ELT processesFamiliarity with cloud platforms (GCP, AWS, or Azure)Experience with clean, maintainable, and well-documented codeStrong collaboration skillsPhysical/Cognitive RequirementsCapability to remain seated in a stationary position for prolonged periods.Eye-hand coordination and manual dexterity to operate keyboard, computer and other office-related equipment.PayThe United States new hire base salary target ranges for this full-time position are:$138,380 - $254,111+ equity + benefitsThis range reflects the minimum and maximum target for new hire salaries for candidates based on their respective Zone. Below is additional information on Included Health's commitment to maintaining transparent and equitable compensation practices across our distinct geographic zones.Starting base salary for you will depend on several job-related factors, unique to each candidate, which may include education; training; skills; years and depth of experience; certifications and licensure; our needs; internal peer equity; organizational considerations; and understanding of geographic and market data. Compensation structures and ranges are tailored to each zone's unique market conditions to ensure that all employees receive fair and great compensation package based on their roles and locations. Your Recruiter can share your geographic zone upon inquiry.Benefits & PerksIn addition to receiving a great compensation package, the compensation package may include, depending on the role, the following and more:Remote-first culture401(k) savings plan through FidelityComprehensive medical, vision, and dental coverage through multiple medical plan options (including disability insurance)Paid Time Off (""PTO"") and Discretionary Time Off (""DTO"")12 weeks of 100% Paid Parental leaveFamily Building & Compassionate Leave: Fertility coverage, $25,000 for surrogacy/adoption, and paid leave for failed treatments, adoption or pregnancies.Work-From-Home reimbursement to support team collaboration home office workYour recruiter will share more about the salary range and benefits package for your role during the hiring process.This is a remote position.About Included HealthIncluded Health is a new kind of healthcare company, delivering integrated virtual care and navigation. We’re on a mission to raise the standard of healthcare for everyone. We break down barriers to provide high-quality care for every person in every community — no matter where they are in their health journey or what type of care they need, from acute to chronic, behavioral to physical. We offer our members care guidance, advocacy, and access to personalized virtual and in-person care for everyday and urgent care, primary care, behavioral health, and specialty care. It’s all included. Learn more atincludedhealth.com.Included Health is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Included Health considers all qualified applicants with arrest or conviction records in accordance with the San Francisco Fair Chance Ordinance, the Los Angeles County Fair Chance Ordinance, and California law.Show moreShow less",
"Data Engineer, GO-AI Technology & Development Team",Amazon,"North Reading, MA",2025-07-07,https://www.linkedin.com/jobs/view/data-engineer-go-ai-technology-development-team-at-amazon-4263655959?position=51&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=NjZDllNC%2ByHnmnrY1OCqlw%3D%3D,LinkedIn,"['Hadoop', 'Python', 'Spark', 'Go', 'Scala', 'Sql']",Seniority levelNot Applicable,"DescriptionAmazon Robotics develops state-of-the art robotics for Amazon’s Fulfillment Centers, which handle more individual items than any company in the world. We are combining computer vision, mobile robots, advanced end-of-arm tooling and high-degree of freedom movement to solve real-world problems at huge scale. Our team partners with a variety of customers across Amazon worldwide to conceive, develop, prototype and deploy a wide range of robotic systems. Within Amazon Robotics, Global Operations - Artificial Intelligence (GO-AI) enables Amazon to accelerate and scale our robotic and automation solutions via human supervisory control. GO-AI’s Technology & Development Team is a cross-functional team of Software Developers (frontend and backend), Data Engineers, Technical Program Managers, and Business Intelligence Engineers who are responsible for the technical strategy and delivery for GO-AI.Key job responsibilitiesDesign, implement, and deploy GO-AI data solutions that are high quality (e.g., secure, testable, maintainable, low-defects, efficient, well documented etc.) and incorporate best practicesFocus on operational excellence, constructively identifying problems and proposing solutions, taking on projects that improve GO-AI’s data architecture, making it better and easier to maintainA day in the lifeAmazon offers a full range of benefits for you and eligible family members, including domestic partners and their children. Benefits can vary by location, the number of regularly scheduled hours you work, length of employment, and job status such as seasonal or temporary employment. The benefits that generally apply to regular, full-time employees include:Medical, Dental, and Vision CoverageMaternity and Parental Leave OptionsPaid Time Off (PTO)401(k) PlanIf you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply!Basic Qualifications1+ years of data engineering experienceExperience with data modeling, warehousing and building ETL pipelinesExperience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)Experience with one or more scripting language (e.g., Python, KornShell)Preferred QualificationsExperience with big data technologies such as: Hadoop, Hive, Spark, EMRExperience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.Amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.Company- Amazon.com Services LLCJob ID: A3026872Show moreShow less",
Senior Data Engineer,the LEGO Group,"Boston, MA",2025-07-10,https://www.linkedin.com/jobs/view/senior-data-engineer-at-the-lego-group-4262586028?position=52&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=8MRRIwAzidihyvSt3vilCA%3D%3D,LinkedIn,"['Ci/Cd', 'Data Science', 'Python', 'Sql', 'Agile']",Seniority levelNot Applicable,"Job DescriptionAre you interested in being a key contributor in enabling our LEGO® Retail organization to understand its shoppers deeply and expand data transparency to drive data driven decisions that always put the shopper first?Bring your understanding of data platforms together with your retail/ecommerce knowledge and natural curiosity into play in this role to be part of a pioneering data and analytics team bringing digital transformation to life in our commercial areas!Core ResponsibilitiesPartner with LEGO® Brand Retail (Stores) digital product teams to ensure high quality data is collected and published to LEGO Nexus (databricks) to a standard fit for purpose for downstream delivery of data products.Transform, cleanse and enrich data to create self-serve data sources for use across LEGO® Brand Retail, Branded Channels, the broader A&I and LEGO® Retail teams & other data consumers (ie personalization, retail excellence).Develop self-serve dashboards regarding LEGO® Brand Retail metrics, and establish the use of these tools with LEGO Retail and broader Commercial colleagues to drive data driven decision making.Collaborate with colleagues, specifically Retail Excellence, Data Science and Commercial Analytics to continually iterate and enrich data product & overall data availability.Enable LEGO® Retail specific data understanding and champion data literacy via guidelines, training, drop-in sessions, documentation, and knowledge sharing.Partner with the Data Office to ensure platforms, tools & processes meet business needs; acting as inputters, collaborators, testers and/or ambassadors.Consistently champion best practices in data product development within the team, across LEGO® Retail and with the broader analytics community, helping ensure data integrity, -quality, and -scalability of overall LEGO® Retail data products on the LEGO Data Platform.Play your part in our team succeedingThe department’s key focus is to enable self-service data products for LEGO® Retail data consumers and decision makers, helping shape data-driven actions both for operational optimization purposes and tactical and strategical decision recommendations. The key partners of this role will initially be heavily focused in LEGO® Brand Retail but have potential to expand to be multifunctional in areas such as Marketing, Finance, Operations etc.This role is essential as we further invest in our digitization agenda, and our new data strategy, including scaling the LEGO® Data Platform to unlock the value from data across the LEGO® Group. In order to become truly data-driven, our first party retail and direct to consumer activity is the cornerstone to everything we do. This role will help both enrich our current data landscape and bring new data variables to life, but also enable important data development projects around the business to use LEGO® Retail data that will both increase accuracy in the models and speed up insights to action.Do you have what it takes?Data Specialist, Data Solutions and/or Data Engineering experience.Expert skills in SQL and/or Python or similar experience in manipulating large structured and unstructured datasets.Experience working with complex ETL processes and knowledge of data transformation best practices.Expert skills in data visualization – Power BI or similar.Experience in the Retail industry.Can provide technical context and direction, translating technical constraints / trade-offs to people within non-technical roles, including senior leadership.Thorough and detail oriented, strong adaptability and focus to deliver results.Dedicated, with ability to take ownership and accountability of large initiatives.Strong collaborative personality with a #OneTeam approach.Positive can-do attitude, with natural curiosity and bravery to challenge constructively.Willing to travel 10 days a year.Fluent English skills both verbal and written.Nice to haveExperience with CI/CD pipelinesCross-culture understanding of working with global colleagues and partners across multiple time zoneFamiliarity with AGILE ways of workingUniversity degree or equivalent experience or similar qualification in applicable subject [Mathematics, Sciences, BI, Analytics, etc.] OR practical experience with track record in relevant tools.Additional Details On This PositionOur workplace enables our LEGO® colleagues to be and do their best at work. Introducing a flexible way of working through a hybrid working model is a great example of how we live up to our ambition. This 3 day in the office hybrid working model will exemplify our People Promise by embracing the different life situations of our colleagues.No relocation assistance is offered for this position.CompensationThe salary for this position has a range of $100,906.00 - $151,360.00 based on anticipated responsibilities, market benchmarks, and organizational constraints. The LEGO Group carefully considers a wide range of compensation factors, including but not limited to prior experience, skills, expertise, location, internal equity, and other considerations permitted by law. The comprehensive remuneration details, inclusive of benefits, will be communicated upon finalisation of the employment offer.Applications are reviewed on an ongoing basis. However, please note we do amend or withdraw our jobs and reserve the right to do so at any time, including prior to any advertised closing date. So, if you're interested in this role we encourage you to apply as soon as possible.What’s in it for you?Here are some of what to expectFamilyCareLeave –We offer enhanced paid leave options for those important times.Insurances –All colleagues are covered by our life and disability insurance which provides protection and peace of mind.Wellbeing– We want you to be your best self, so you’ll have access to the Headspace App and lots of wellbeing initiatives and programs run by local teams where you are basedColleagueDiscount– We know you'll love to build so from day 1 you will qualify for our generous colleague discount.Bonus– We do our best work to succeed together. When goals are reached and if eligible, you'll be rewarded through our bonus schemeYour workplace –When you join the team you'll be assigned a primary workplace location i.e. one of our Offices, stores or factories. Our hybrid work policy means an average of 3 days per week in the office. The hiring team will discuss the policy and role eligibility with you during the recruitment process.We strive to create a diverse, dynamic and inclusive culture of play at the LEGO Group, where everyone feels safe, valued and they belong.The LEGO Group is highly committed to equal employment opportunity and equal pay and seeks to encourage applicants from all backgrounds (eg. sex, gender identity or expression, race/ethnicity, national origin, sexual orientation, disability, age, religion and Veteran status) to apply for roles in our team.We support our employees in being there for the moments that matter in life and celebrate families of all kinds, the loved ones that make us who we are. Being part of the LEGO Group also means taking part in our annual Play Day, playing a part in building a sustainable future and continuing our mission to “inspire and develop the builders of tomorrow.”The LEGO Group is fully committed to Children’s Rights and Child Wellbeing across the globe. Candidates offered positions with high engagement with children are required to take part in Child Safeguarding Background Screening, as a condition of the offer.Thank you for sharing our global commitment to Children’s Rights.We conduct drug screening as a part of our drug free workplace policy and in support of our commitment to the health and safety of our employees.Online Application Accessibility Statement; which is intended for people with disabilities - LEGO systems endeavors to make www.LEGO.com/jobs accessible to any and all users. If you would like to contact us regarding the accessibility of our web site or need assistance completing the application process, please contact the HR Service Desk at 1.860-763-7777, option #3. Please note, these communication channels should be used for those having difficulty accessing our on-line channels, not to inquire about job opportunities.Just imagine building your dream career.Then make it real.Join the LEGO® team today.Show moreShow less",
Junior Data Engineer,Medpace,"Cincinnati, OH",2025-06-30,https://www.linkedin.com/jobs/view/junior-data-engineer-at-medpace-4217695016?position=53&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=buRpqGyFh9rYhguID4wbow%3D%3D,LinkedIn,"['Data Science', 'Python', 'Azure', 'Sql', 'Rest Api']",Seniority levelNot Applicable,"Our corporate activities are growing rapidly, and we are currently seeking a full-time, office-based Junior Data Engineer to join our Information Technology team. This position will work on a team to accomplish tasks and projects that are instrumental to the company’s success. If you want an exciting career where you use your previous expertise and can develop and grow your career even further, then this is the opportunity for you.ResponsibilitiesUtilize skills in development areas including data warehousing, business intelligence, and databases (Snowflake, ANSI SQL, SQL Server, T-SQL);Support programming/software development using Extract, Transform, and Load (ETL) and Extract, Load and Transform (ELT) tools, (dbt, Azure Data Factory, SSIS);Design, develop, enhance and support business intelligence systems primarily using Microsoft Power BI;Collect, analyze and document user requirements;Participate in software validation process through development, review, and/or execution of test plan/cases/scripts;Create software applications by following software development lifecycle process, which includes requirements gathering, design, development, testing, release, and maintenance;Communicate with team members regarding projects, development, tools, and procedures; andProvide end-user support including setup, installation, and maintenance for applicationsQualificationsBachelor's Degree in Computer Science, Data Science, or a related field;Internship experience in Data or Software Engineering;Knowledge of developing dimensional data models and awareness of the advantages and limitations of Star Schema and Snowflake schema designs;Solid ETL development, reporting knowledge based off intricate understanding of business process and measures;Knowledge of Snowflake cloud data warehouse, Fivetran data integration and dbt transformations is preferred;Knowledge of Python is preferred;Knowledge of REST API;Basic knowledge of SQL Server databases is required;Knowledge of C#, Azure development is a bonus; andExcellent analytical, written and oral communication skills.Medpace OverviewMedpace is a full-service clinical contract research organization (CRO). We provide Phase I-IV clinical development services to the biotechnology, pharmaceutical and medical device industries. Our mission is to accelerate the global development of safe and effective medical therapeutics through its scientific and disciplined approach. We leverage local regulatory and therapeutic expertise across all major areas including oncology, cardiology, metabolic disease, endocrinology, central nervous system, anti-viral and anti-infective. Headquartered in Cincinnati, Ohio, employing more than 5,000 people across 40+ countries.Why Medpace?People. Purpose. Passion. Make a Difference Tomorrow. Join Us Today.The work we’ve done over the past 30+ years has positively impacted the lives of countless patients and families who face hundreds of diseases across all key therapeutic areas. The work we do today will improve the lives of people living with illness and disease in the future.Cincinnati PerksCincinnati Campus OverviewFlexible work environmentCompetitive PTO packages, starting at 20+ daysCompetitive compensation and benefits packageCompany-sponsored employee appreciation eventsEmployee health and wellness initiativesCommunity involvement with local nonprofit organizationsDiscounts on local sports games, fitness gyms and attractionsModern, ecofriendly campus with an on-site fitness centerStructured career paths with opportunities for professional growthDiscounted tuition for UC online programsAwardsNamed a Top Workplace in 2024 by The Cincinnati EnquirerRecognized by Forbes as one of America's Most Successful Midsize Companies in 2021, 2022, 2023 and 2024Continually recognized with CRO Leadership Awards from Life Science Leader magazine based on expertise, quality, capabilities, reliability, and compatibilityWhat To Expect NextA Medpace team member will review your qualifications and, if interested, you will be contacted with details for next steps.Show moreShow less",
Data Engineer Intern,Wurl,United States,2025-07-05,https://www.linkedin.com/jobs/view/data-engineer-intern-at-wurl-4233512147?position=54&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=7PjKc5uflfy4Zn2tY2J3Iw%3D%3D,LinkedIn,"['Data Science', 'Python', 'Sql', 'Aws']",Seniority levelInternship,"About Wurl, LLC.At Wurl, our mission is to create innovative technologies and services that help accelerate the shift to streaming. At our core, we are innovators focused on driving the streaming industry forward with technologies that help connect viewers to the right content and ads. Our technology and data-driven solutions help companies reach new audiences, engage high-value viewers, and ultimately increase their revenue.In 2022, Wurl was acquired by AppLovin (Nasdaq: APP), a leading marketing platform, giving our employees the best of both worlds: the dynamic environment of a 150+ person startup as well as the stability of a high-growth public tech company. We invest in providing a culture that fosters passion, drives excellence, and encourages collaboration to drive innovation. Wurl is a fully-remote company that has been globally recognized for our people and technology, having been named a Great Place to Work by Fortune in 2022, 2023, and 2024; an Ad Tech Company of the Year finalist by the UK Business Tech Awards in 2023; a BIG Innovation Awards winner in 2024 and 2025; and the “Innovation in Advertising” winner by the StreamTV Awards in 2024.Data Engineer Intern (Remote - US)Wurl connects the world’s video content to global audiences. Our data platform powers advanced analytics, real-time decision-making, and secure operations across the OTT video streaming ecosystem.We’re seeking aData Engineering Internto support our performance marketing initiatives on Connected TV (CTV). This role offers hands-on experience in building data-driven solutions that enhance ad targeting, attribution, and campaign optimization in the evolving landscape of streaming TV advertising.What You’ll DoCollaborate with the Data Engineering team to develop and maintain scalable data pipelines that support performance marketing efforts on CTV.Assist in implementing attribution models that tie CTV viewership to campaigns.Work with cross-functional teams to integrate AI-driven targeting into our data infrastructure.Analyze viewer engagement data to identify opportunities for optimizing return on ad spend (ROAS).Contribute to the development of tools and dashboards that provide insights into campaign performance and audience behavior.What You BringPursuing or recently completed a Bachelor’s or Master’s degree in Computer Science, Data Science, or a related technical field.Proficiency in Python and SQL.Familiarity with data engineering concepts and tools (e.g., ETL processes, data warehousing).Interest in digital advertising, performance marketing, and streaming media technologies.Strong analytical and problem-solving skills.Excellent communication and collaboration abilities.Nice to HaveExperience with cloud platforms (e.g., AWS) and big data technologies.Understanding of ad tech ecosystems and metrics (e.g., CPM, CTR, ROAS).Exposure to AI/ML concepts, particularly in the context of ad targeting and personalization. DataBricks experienceFamiliarity with interactive ad formats and their implementation.What We OfferPaid internship ($30/hr) with flexible scheduling.Remote-first work culture and support.Mentorship from experienced engineers and product experts.AppLovin is proud to be an equal opportunity employer that is committed to inclusion and diversity. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status, or other legally protected characteristics. Learn more about EEO rights as an applicant here.If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send us a request at accommodations@applovin.com.AppLovin will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law. If you’re applying for a position in California, learn more here.Please read our Global Applicant Privacy Notice to learn more about how AppLovin processes your personal information.Show moreShow less",
Data Engineer,"The Dignify Solutions, LLC","Bellevue, WA",2025-06-15,https://www.linkedin.com/jobs/view/data-engineer-at-the-dignify-solutions-llc-4261611366?position=55&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=TZwNb%2FQMj%2BzsaHD57h1TKA%3D%3D,LinkedIn,"['Azure', 'Sql', 'Ci/Cd', 'Spark']",Seniority levelMid-Senior level,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.Show moreShow less",
Data Engineer (L5) - Content,Netflix,United States,2025-07-12,https://www.linkedin.com/jobs/view/data-engineer-l5-content-at-netflix-4252417307?position=56&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=V964llTHEAsP5kyIveZo9w%3D%3D,LinkedIn,"['Data Science', 'Python', 'Sql', 'Spark']",Seniority levelNot Applicable,"Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.Netflix aspires to “entertain the world” by finding amazing stories and by sharing those stories through a personalized product experience that pleases our members and generates value for the business.The Content Data engineering team focuses on member interaction with those stories, seeking to improve our content strategy through scaled data products that drive a shared understanding of content metadata and performance throughout the company.You will partner with Data Science + Analytics teams who consume your data as inputs to analyses and models which predict performance for launched + unlaunched content. You will partner with Finance + Strategy teams who consume your data for reporting and to evaluate prospective content deals.What You’ll DoAnalyze + understand the businessBecome a subject matter expert in the streaming space, understanding:Content deal structures + performance patternsThe impact of acquisition + churn on financial outcomesHow data can be leveraged as a competitive asset in the broader streaming environmentScale that understanding through robust data products to drive informed decision-makingBuild data models that reflect your domain expertise, meet current business needs, and will remain flexible as strategy evolvesTreat the company as your customer, analyzing stakeholder consumption of your data and iterating on design to improve their experienceTeam focus areas range across: managing robust content metadata, scaling measurement of the business value and member satisfaction driven by our content, innovating on data models + metrics for new business areas (e.g. live, ads, etc)What You HaveBig Picture StrengthsSkill, appreciation, and patience for building detailed + performant data models and pipelinesStakeholder empathy and curiosity, with a willingness to learn the nuances of your partners’ domain (e.g. amortization logic from accounting, ML model foundations from data science, quarterly earnings from FP&A)Strong communication skills, with ability to calibrate the level of detail in your presentations to your audience while maintaining accuracy and impactWillingness to work in a fast-paced, demanding, and nebulous environment. Love freedom and hate being micromanagedTechnical StrengthsFluency in SQL, Python, distributed processing (e.g., Spark, Flink), and orchestration frameworksExperience sourcing and modeling data from application APIsComfort with MPP/Cloud data warehouse solutions (Snowflake, Redshift, BigQuery, Vertica, etc)Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.Inclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.We are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.Job is open for no less than 7 days and will be removed when the position is filled.Show moreShow less",
Senior Staff IT Data Engineer,Palo Alto Networks,"Santa Clara, CA",2025-07-11,https://www.linkedin.com/jobs/view/senior-staff-it-data-engineer-at-palo-alto-networks-4265245686?position=57&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=m1LLOTbTYFWNvgMexrlj9w%3D%3D,LinkedIn,"['Python', 'Spark', 'Scala', 'Gcp', 'Sql', 'Kafka']",5 years,"Our MissionAt Palo Alto Networks® everything starts and ends with our mission:Being the cybersecurity partner of choice, protecting our digital way of life.Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.Who We AreWe take our mission of protecting the digital way of life seriously. We are relentless in protecting our customers and we believe that the unique ideas of every member of our team contributes to our collective success. Our values were crowdsourced by employees and are brought to life through each of us everyday - from disruptive innovation and collaboration, to execution. From showing up for each other with integrity to creating an environment where we all feel included.As a member of our team, you will be shaping the future of cybersecurity. We work fast, value ongoing learning, and we respect each employee as a unique individual. Knowing we all have different needs, our development and personal wellbeing programs are designed to give you choice in how you are supported. This includes our FLEXBenefits wellbeing spending account with over 1,000 eligible items selected by employees, our mental and financial health resources, and our personalized learning opportunities - just to name a few!At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work full time from our office with flexibility offered where needed. This setup fosters casual conversations, problem-solving, and trusted relationships. Our goal is to create an environment where we all win with precision.Job DescriptionYour CareerOur Data & Analytics group is responsible for working with various business owners/stakeholders from Sales, Marketing, People, GCS, Infosec, Operations, and Finance to solve complex business problems which will have a direct impact on the metrics defined to showcase the progress of Palo Alto Networks. We leverage the latest technologies from the Cloud & Big Data ecosystem to improve business outcomes and create through prototyping, Proof-of-Concept projects and application development. We are looking for a Staff IT Data Engineer with extensive experience in Data engineering, SQL, Cloud engineering and business intelligence (BI) tools. The ideal candidate will be responsible for designing, implementing, and maintaining scalable data transformations and analytical solutions that support our business objectives. This role requires a strong understanding of data engineering principles, as well as the ability to collaborate with cross-functional teams to deliver high-quality data solutions.This is an in office role in our HQ (Santa Clara, CA)Your ImpactDesign, develop, and maintain data pipelines to extract, transform, and load (ETL) data from various sources into our data warehouse or data lake environmentAptitude for proactively identifying and implementing GenAI-driven solutions to achieve measurable improvements in the reliability and performance of data pipelines or to optimize key processes like data quality validation and root cause analysis for data issues, is a nice-to-haveCollaborate with stakeholders to gather requirements and translate business needs into technical solutionsOptimize and tune existing data pipelines for performance, reliability, and scalabilityImplement data quality and governance processes to ensure data accuracy, consistency, and compliance with regulatory standardsWork closely with the BI team to design and develop dashboards, reports, and analytical tools that provide actionable insights to stakeholdersMentor junior members of the team and provide guidance on best practices for data engineering and BI developmentQualificationsYour ExperienceBachelor's degree in Computer Science, Engineering, or a related field or equivalent military experience requiredDemonstrated readiness to leverage GenAI tools to enhance efficiency within the typical stages of the data engineering lifecycle, for example by generating complex SQL queries, creating initial Python/Spark script structures, or auto-generating pipeline documentation, is a nice-to-have.5+ years of experience in data engineering, with a focus on building and maintaining data pipelines and analytical solutionsExpertise in SQL programming and database management systemsHands-on experience with ETL tools and technologies (e.g. Apache Spark, Apache Airflow).Experience with cloud platforms such as Google Cloud Platform (GCP), and experience with relevant services (e.g. GCP Dataflow, GCP DataProc, Biq Query, Procedures, Cloud Composer etc).Experience with Big data tools like Spark, Kafka, etcExperience with object-oriented/object function scripting languages: Python/Scala, etcExperience with BI tools and visualization platforms (e.g. Tableau) is a plusExperience with SAP HANA, SAP BW, SAP ECC, or other SAP modules is a plusStrong analytical and problem-solving skills, with the ability to analyze complex data sets and derive actionable insightsExcellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teamsAdditional InformationThe TeamWorking at a high-tech cybersecurity company within Information Technology is a once-in-a-lifetime opportunity. You’ll join the brightest minds in technology, creating, building, and supporting tools and enabling our global teams on the front line of defense against cyberattacks.We’re connected by one mission but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team of people who feel excited by the prospect of a challenge and feel a thrill at resolving technical gaps that inhibit productivity.Compensation DisclosureThe compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $122000/YR- $197000/YR. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here.Our CommitmentWe’re problem solvers that take risks and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.All your information will be kept confidential according to EEO guidelines.Show moreShow less",
Data Engineer,Paces,"Brooklyn, NY",2025-05-22,https://www.linkedin.com/jobs/view/data-engineer-at-paces-4235550250?position=58&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=UEQZQjz9yh5fJRx7j8QfmA%3D%3D,LinkedIn,"['Python', 'Aws', 'Spark', 'Azure', 'Gcp', 'Sql', 'Rust']",Seniority levelEntry level,"In the next 30 years, the world will transform every part of the built environment to be climate positive green infrastructure. Knowing what, where, and how to build infrastructure like solar farms and data centers is one of the great opportunities of our time. However, there are problems!The Problem80% of clean energy projects that developers start never actually get built because most projects are started without deep due diligence on zoning and interconnection due to the cost of collecting that data. This means $17B worth of canceled projects per year.Our SolutionPaces is software for green infrastructure developers to identify the best places to build and manage their projects. First we collect environmental, permitting, zoning, energy grid data from various different sources; then we analyze the data and use AI to identify the best places for developers to build their next projects.Our TeamWe are building a team where people can proudly say their time at Paces is themost impactful, meaningful work of their career.Our amazing team in Brooklyn, New York includes incredible team members from tech companies like Meta AI, DeepMind, and Tesla; various high growth startups; as well as from industry leaders like Nexamp, Total Energies, and GE.Paces is growing rapidly and looking for exceptional people to join who want to have a massive positive climate impact while building a great culture!We are looking for exceptional talents to join our growing Engineering team!🏆 What You’ll AchieveDesign, implement, and maintain scalable ETL data pipelines from hundreds of data sourcesOptimize our storage and retrieval systems for performance and reliabilityEnsure data quality, consistency, and security across the platformCollaborate closely with our CTO, Data Infra Lead, Product Lead, and team to directly impact product roadmap📈Requirements1 yr+ hands on professional experience working on data infraSolid understanding of data engineering concepts, with a strong grasp of data structures, algorithms, and system designStrong coding skills with demonstrated proficiency in relevant programming languages, such as Python, Rust, ScalaAdvanced SQL expertise, including experience with complex queries, query optimization, and working with various database systemsHands-on experience with big data tools (e.g. Spark) and data pipeline orchestration tools (e.g. Dagster, Airflow, Prefect)Proven experience in building robust, scalable and performant data pipelines on the cloud (AWS / GCP / Azure)✨ About YouYou will thrive in our culture if you:Have a strong bias towards action and prioritize executionShare our passion to build something that fights climate changeEasily handle the unstructured environment of fast moving startupsHave the hunger to grow together with Paces as we scale up🚀 Bonus PointsPrevious experience at a high-growth, fast-paced startupPrevious experience working with (geo)spatial datasets and libraries (e.g. GEOS, GDAL)Hands-on experience with novel data tools and frameworks such Apache Arrow, DuckDB, DeltaLake, Apache Iceberg💰 Compensation And Benefits120K - 170K annual compensationCompetitive equity compensation401(k) matchingHealth, Dental and Vision insurancePaid company holidays & PTOHybrid work in the office in Williamsburg, Brooklyn ~3x per week☀️About PacesPaces is software for green infrastructure developers to identify the best places to build and manage their projects.🛠️ If you’re a builder who wants to actively construct a more climate positive future, join us!🗺️ We are enabling theclimate optimum use for all the world’s landand are thought leaders in the evolving data center and renewables markets.🐶 We have a dog calendar and have dogs running around in the office.🌞 We have world class customers and are growing!📢 We've raised +$13M led byNavitas CapitalandResolute Venturesand Y Combinator.Show moreShow less",
"Data Engineer, Associate",Pacific Gas and Electric Company,"Oakland, CA",2025-07-10,https://www.linkedin.com/jobs/view/data-engineer-associate-at-pacific-gas-and-electric-company-4264371020?position=59&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=epqYA1rHEAlwe9Ve8I8cPA%3D%3D,LinkedIn,[],1 years,"Requisition ID # 166293Job Category: Information TechnologyJob Level: Individual ContributorBusiness Unit: Information TechnologyWork Type: HybridJob Location: OaklandDepartment OverviewInformation Systems Technology Services is a unified organization comprised of various departments which collaborate effectively in order to deliver high quality technology solutions.Position SummaryDesigns, develops, modifies, configures, debugs and evaluates jobs for extracting data from various sources, implements transformation logic, and stores data in various formats fit for use by stakeholders. Collects metadata about jobs including data lineage and transformation logic. Works with teams, clients, data owners and leadership throughout the development cycle practicing continuous improvement.Job ResponsibilitiesCompletes data analysis and reporting by creating and running database queries and reports.Develops, compiles, and provides technical data.Understands business requirements and applies them to complex diagnostics and analysis of system performance issues.Communicates (oral and written) with peers inside and outside of the department, providing information or exchanging data.Partners with various departments to understand and incorporate standards information and requirements into work procedures.Identifies, analyzes and improves existing business processes within a department to meet new goals and objectives.Troubleshoots and assist in database resolution.Reviews design and technical specification documents for alignment with business solutions.Uses established Company procedures for all work and within all required standards.QualificationsMinimum:Bachelors Degree in Computer Science or job-related discipline or equivalent experienceDesired:Experience with data engineering/ETL ecosystems (e.g., Informatica, SAP BODS, OBIEE), 1 yrExperience with data engineering/transformationExperience with SAP S/4HanaShow moreShow less",
Data Engineer,Kikoff,"San Francisco, CA",2025-07-05,https://www.linkedin.com/jobs/view/data-engineer-at-kikoff-4248989938?position=60&pageNum=0&refId=qm%2F9zBj%2FVqcigtr%2BmuhsSA%3D%3D&trackingId=LGXhISRuAw%2BZbyNNCP3wAw%3D%3D,LinkedIn,['Sql'],Seniority levelEntry level,"ResponsibilitiesWe are looking for a Data Engineer or Analytics Engineer to join our Data team. You will collaborate with the data scientist and engineers to design, build, and scale high-leverage data models, foundational datasets and scalable infrastructure that enables analytics, modeling, and experimentation. Your responsibilities includes:Build and optimize high-quality ergonomic foundational datasets and the relevant data pipelinesEstablish data quality management processes at scaleBuild analytical dashboardsQualificationsHands-on experience in building robust data models and data pipelinesA self-starter with a bias for action and excellent communication skills, with ability to explain complex technical concepts in easy-to-understand waysExpert knowledge of SQL, and popular data engineering tools such as snowflake, dbt, Dagster etcExperience working with DS teamA humble collaborative can-do attitude and natural curiosityKikoff: A FinTech Unicorn Powering Financial Progress with AIAt Kikoff, our mission is to provide radically affordable financial tools to help consumers achieve financial security. We're a profitable, high growth FinTech unicorn serving millions of people, many of whom are building credit or navigating life paycheck to paycheck. With innovative technology and AI, we simplify credit building, reduce debt, and expand access to financial opportunities to those who need them the most. Founded in 2019, Kikoff is headquartered in San Francisco and backed by top-tier VC investors and NBA star Stephen Curry.Why KikoffThis is a consumer fintech startup, and you will be working with serial entrepreneurs who have built strong consumer brands and innovative products. We value extreme ownership, clear communication, a strong sense of craftsmanship, and the desire to create lasting work and work relationships. Yes, you can build an exciting business AND have real-life real-customer impact.Competitive pay based on experience🏥 Medical, dental, and vision coverage - Kikoff covers the full cost of health insurance for the employee!📈 Stock Options🏝 Flexible vacation policy to help you recharge💰 US salary range for this full-time position consists of base + equity + benefitsLocation: San Francisco, CAVisa sponsorship available: Kikoff is willing to provide sponsorship for H1-B visas and U.S. green cards for exceptional talent.Equal Employment Opportunity StatementKikoff Inc. is an equal opportunity employer. We are committed to complying with all federal, state, and local laws providing equal employment opportunities and considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.Please Reference The Following Information For More Informationhttps://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdfhttps://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdfShow moreShow less",
Modern Data Engineer,N/A,Hybrid - Bengaluru,1 day ago,https://www.naukri.com/job-listings-modern-data-engineer-fidelity-international-bengaluru-3-to-5-years-110725024403,Naukri,[],N/A,,N/A
Data Engineer | PAN India,N/A,Bengaluru,1 week ago,https://www.naukri.com/job-listings-data-engineer-pan-india-capgemini-technology-services-india-limited-bengaluru-3-to-6-years-020725927454,Naukri,[],N/A,,N/A
Senior - Data Engineering Professional,N/A,Bengaluru,3+ weeks ago,https://www.naukri.com/job-listings-senior-data-engineering-professional-kpmg-india-bengaluru-2-to-7-years-130625501762,Naukri,[],N/A,,N/A
IN_Senior Associate_Azure Data Engineer,N/A,Bengaluru,1 week ago,https://www.naukri.com/job-listings-in-senior-associate-azure-data-engineer-pricewaterhouse-coopers-service-delivery-center-kolkata-bengaluru-2-to-6-years-050725502109,Naukri,[],N/A,,N/A
Data Engineer,N/A,"Hyderabad, Bengaluru",3 days ago,https://www.naukri.com/job-listings-data-engineer-google-india-private-limited-hyderabad-bengaluru-0-to-0-years-090725503768,Naukri,[],N/A,,N/A
"Data Engineer, FinTech International Tax Compliance",N/A,Hyderabad,3+ weeks ago,https://www.naukri.com/job-listings-data-engineer-fintech-international-tax-compliance-amazon-development-centre-india-pvt-ltd-hyderabad-1-to-8-years-050225502166,Naukri,[],N/A,,N/A
Data Engineer - ML,N/A,"Hybrid - Kolkata, Hyderabad, Bengaluru",2 days ago,https://www.naukri.com/job-listings-data-engineer-ml-genpact-kolkata-hyderabad-bengaluru-7-to-12-years-100725036103,Naukri,[],N/A,,N/A
Databricks Data Engineer,N/A,Bengaluru,1 week ago,https://www.naukri.com/job-listings-databricks-data-engineer-infosys-limited-bengaluru-5-to-9-years-030725923997,Naukri,[],N/A,,N/A
Lead Data Engineer ( Preferred Candidates Banking and NBFC ),N/A,Bengaluru,Today,https://www.naukri.com/job-listings-lead-data-engineer-preferred-candidates-banking-and-nbfc-muthoot-finance-bengaluru-4-to-6-years-120725011861,Naukri,[],N/A,,N/A
Staff Data Engineer/Architect,N/A,Hyderabad,3+ weeks ago,https://www.naukri.com/job-listings-staff-data-engineer-architect-synopsys-india-private-limited-hyderabad-5-to-10-years-050525502320,Naukri,[],N/A,,N/A
Data Engineer - (Hadoop/Hive/python/Spark/Scala),N/A,Bengaluru,3+ weeks ago,https://www.naukri.com/job-listings-data-engineer-hadoop-hive-python-spark-scala-capco-technologies-pvt-ltd-bengaluru-5-to-8-years-210225500136,Naukri,[],N/A,,N/A
